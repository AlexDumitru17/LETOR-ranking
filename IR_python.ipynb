{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjpEGjawsulPhSiKZPNcYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexDumitru17/LETOR-ranking/blob/main/IR_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allrank\n",
        "!unzip MQ2007.zip\n",
        "!unzip MQ2008.zip"
      ],
      "metadata": {
        "id": "FD-QEKCBuOSG",
        "outputId": "22f97b3c-24bb-4d99-9b77-7d571e461528",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.9/dist-packages (from -r allRank/requirements.txt (line 2)) (7.2.2)\n",
            "Requirement already satisfied: flake8>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from -r allRank/requirements.txt (line 5)) (6.0.0)\n",
            "Requirement already satisfied: flake8-blind-except==0.1.1 in /usr/local/lib/python3.9/dist-packages (from -r allRank/requirements.txt (line 6)) (0.1.1)\n",
            "Requirement already satisfied: flake8-debugger==1.4.0 in /usr/local/lib/python3.9/dist-packages (from -r allRank/requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: flake8-print==2.0.2 in /usr/local/lib/python3.9/dist-packages (from -r allRank/requirements.txt (line 8)) (2.0.2)\n",
            "Requirement already satisfied: flake8-tuple==0.2.13 in /usr/local/lib/python3.9/dist-packages (from -r allRank/requirements.txt (line 9)) (0.2.13)\n",
            "Requirement already satisfied: mypy==0.790 in /usr/local/lib/python3.9/dist-packages (from -r allRank/requirements.txt (line 10)) (0.790)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from flake8-blind-except==0.1.1->-r allRank/requirements.txt (line 6)) (67.6.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from flake8-tuple==0.2.13->-r allRank/requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.9/dist-packages (from mypy==0.790->-r allRank/requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: mypy-extensions<0.5.0,>=0.4.3 in /usr/local/lib/python3.9/dist-packages (from mypy==0.790->-r allRank/requirements.txt (line 10)) (0.4.4)\n",
            "Requirement already satisfied: typed-ast<1.5.0,>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from mypy==0.790->-r allRank/requirements.txt (line 10)) (1.4.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from pytest->-r allRank/requirements.txt (line 2)) (23.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest->-r allRank/requirements.txt (line 2)) (2.0.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest->-r allRank/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest->-r allRank/requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from pytest->-r allRank/requirements.txt (line 2)) (22.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest->-r allRank/requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from flake8>=3.3.0->-r allRank/requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pycodestyle<2.11.0,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from flake8>=3.3.0->-r allRank/requirements.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: pyflakes<3.1.0,>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from flake8>=3.3.0->-r allRank/requirements.txt (line 5)) (3.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allrank\n",
            "  Downloading allRank-1.4.3-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision<=0.9.0,>=0.7.0\n",
            "  Downloading torchvision-0.9.0-cp39-cp39-manylinux1_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatten-dict>=0.3.0\n",
            "  Downloading flatten_dict-0.4.2-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from allrank) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from allrank) (1.10.1)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.9/dist-packages (from allrank) (22.2.0)\n",
            "Collecting tensorboardX>=2.1.0\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.9/dist-packages (from allrank) (1.4.4)\n",
            "Collecting google-auth==1.27.1\n",
            "  Downloading google_auth-1.27.1-py2.py3-none-any.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch<=1.8.0,>=1.6.0\n",
            "  Downloading torch-1.8.0-cp39-cp39-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from allrank) (1.22.4)\n",
            "Collecting gcsfs==0.6.2\n",
            "  Downloading gcsfs-0.6.2-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.6.2->allrank) (2023.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.6.2->allrank) (4.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.6.2->allrank) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.9/dist-packages (from gcsfs==0.6.2->allrank) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.9/dist-packages (from google-auth==1.27.1->allrank) (67.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth==1.27.1->allrank) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth==1.27.1->allrank) (1.16.0)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth==1.27.1->allrank) (0.2.8)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->allrank) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0.5->allrank) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.23.0->allrank) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.23.0->allrank) (1.1.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX>=2.1.0->allrank) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX>=2.1.0->allrank) (23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch<=1.8.0,>=1.6.0->allrank) (4.5.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from torchvision<=0.9.0,>=0.7.0->allrank) (8.4.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth==1.27.1->allrank) (0.4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib->gcsfs==0.6.2->allrank) (1.3.1)\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-0.8.0-py2.py3-none-any.whl (19 kB)\n",
            "  Downloading google_auth_oauthlib-0.7.1-py2.py3-none-any.whl (19 kB)\n",
            "  Downloading google_auth_oauthlib-0.7.0-py2.py3-none-any.whl (19 kB)\n",
            "  Downloading google_auth_oauthlib-0.5.3-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.6.2->allrank) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.6.2->allrank) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.6.2->allrank) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gcsfs==0.6.2->allrank) (2022.12.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs==0.6.2->allrank) (3.2.2)\n",
            "Installing collected packages: torch, tensorboardX, flatten-dict, cachetools, torchvision, google-auth, google-auth-oauthlib, gcsfs, allrank\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.3.0\n",
            "    Uninstalling cachetools-5.3.0:\n",
            "      Successfully uninstalled cachetools-5.3.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.1\n",
            "    Uninstalling google-auth-2.17.1:\n",
            "      Successfully uninstalled google-auth-2.17.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.0.0\n",
            "    Uninstalling google-auth-oauthlib-1.0.0:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.8.0 which is incompatible.\n",
            "google-api-core 2.11.0 requires google-auth<3.0dev,>=2.14.1, but you have google-auth 1.27.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed allrank-1.4.3 cachetools-4.2.4 flatten-dict-0.4.2 gcsfs-0.6.2 google-auth-1.27.1 google-auth-oauthlib-0.5.3 tensorboardX-2.6 torch-1.8.0 torchvision-0.9.0\n",
            "usage: allRank\n",
            "       [-h]\n",
            "       --job-dir\n",
            "       JOB_DIR\n",
            "       --run-id\n",
            "       RUN_ID\n",
            "       --config-file-name\n",
            "       CONFIG_FILE_NAME\n",
            "allRank: error: the following arguments are required: --job-dir, --run-id, --config-file-name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "datasets = ['MQ2007', 'MQ2008', 'MSLR-WEB10K', 'Yahoo', 'istella-s-letor']\n",
        "losses = ['approxNDCGLoss', 'lambdaLoss', 'listMLE', 'neuralNDCG', 'ordinal', 'pointwise_rmse']\n",
        "n_folds = 5\n",
        "\n",
        "with open('config_template.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "for dataset in datasets:\n",
        "    for fold in range(1, n_folds + 1):\n",
        "        for loss in losses:\n",
        "            data['data']['path'] = f'{dataset}/Fold{fold}/'\n",
        "            data['loss']['name'] = loss\n",
        "            with open('config.json', 'w') as f:\n",
        "                json.dump(data, f)\n",
        "            command = f'allRank --config-file-name config.json --run-id {dataset}-Fold{fold}-{loss} --job-dir output'\n",
        "            "
      ],
      "metadata": {
        "id": "b_Ukma9Iod50",
        "outputId": "87736be7-c765-4eb4-e6c5-9dc9f162cdc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:36:20 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold1-listMLE', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold1-listMLE', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:36:20 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold1/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='listMLE', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:36:20 - will execute cp config.json output/results/MQ2008-Fold1-listMLE/used_config.json\n",
            "[INFO] 2023-04-07 13:36:20 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:36:20 - will load train data from MQ2008/Fold1/train.txt\n",
            "[INFO] 2023-04-07 13:36:20 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/train.txt'> and got x shape (7903, 46), y shape (7903,) and query_ids shape (7903,)\n",
            "[INFO] 2023-04-07 13:36:20 - loaded dataset with 339 queries\n",
            "[INFO] 2023-04-07 13:36:20 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:36:20 - train DS shape: [339, 121, 46]\n",
            "[INFO] 2023-04-07 13:36:20 - will load vali data from MQ2008/Fold1/vali.txt\n",
            "[INFO] 2023-04-07 13:36:20 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/vali.txt'> and got x shape (2104, 46), y shape (2104,) and query_ids shape (2104,)\n",
            "[INFO] 2023-04-07 13:36:20 - loaded dataset with 120 queries\n",
            "[INFO] 2023-04-07 13:36:20 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:36:20 - vali DS shape: [120, 118, 46]\n",
            "[INFO] 2023-04-07 13:36:20 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:36:20 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:36:20 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:36:20 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:36:20 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:23 - Epoch : 0 Train loss: 13.038742892510069 Val loss: 41.1324701944987 Train ndcg_10 0.5711188912391663 Val ndcg_10 0.44447940587997437\n",
            "[INFO] 2023-04-07 13:36:23 - Current:0.44447940587997437 Best:0.0\n",
            "[INFO] 2023-04-07 13:36:23 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:27 - Epoch : 1 Train loss: 13.112635255211574 Val loss: 41.196180470784505 Train ndcg_10 0.5628255009651184 Val ndcg_10 0.4522930383682251\n",
            "[INFO] 2023-04-07 13:36:27 - Current:0.4522930383682251 Best:0.44447940587997437\n",
            "[INFO] 2023-04-07 13:36:27 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:30 - Epoch : 2 Train loss: 13.002461630334544 Val loss: 41.115603892008465 Train ndcg_10 0.5633061528205872 Val ndcg_10 0.4601215720176697\n",
            "[INFO] 2023-04-07 13:36:30 - Current:0.4601215720176697 Best:0.4522930383682251\n",
            "[INFO] 2023-04-07 13:36:30 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:32 - Epoch : 3 Train loss: 12.984246428385596 Val loss: 41.007381439208984 Train ndcg_10 0.5861731171607971 Val ndcg_10 0.47122079133987427\n",
            "[INFO] 2023-04-07 13:36:32 - Current:0.47122079133987427 Best:0.4601215720176697\n",
            "[INFO] 2023-04-07 13:36:32 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:35 - Epoch : 4 Train loss: 12.972728824896798 Val loss: 41.08446578979492 Train ndcg_10 0.5963090062141418 Val ndcg_10 0.477103590965271\n",
            "[INFO] 2023-04-07 13:36:35 - Current:0.477103590965271 Best:0.47122079133987427\n",
            "[INFO] 2023-04-07 13:36:35 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:38 - Epoch : 5 Train loss: 12.957146022875401 Val loss: 40.91653060913086 Train ndcg_10 0.6062127351760864 Val ndcg_10 0.4801822900772095\n",
            "[INFO] 2023-04-07 13:36:38 - Current:0.4801822900772095 Best:0.477103590965271\n",
            "[INFO] 2023-04-07 13:36:38 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:42 - Epoch : 6 Train loss: 12.976700231388959 Val loss: 40.97853418986003 Train ndcg_10 0.5972958207130432 Val ndcg_10 0.4874677062034607\n",
            "[INFO] 2023-04-07 13:36:42 - Current:0.4874677062034607 Best:0.4801822900772095\n",
            "[INFO] 2023-04-07 13:36:42 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:45 - Epoch : 7 Train loss: 12.981812617771745 Val loss: 40.95873463948568 Train ndcg_10 0.5955712199211121 Val ndcg_10 0.48788943886756897\n",
            "[INFO] 2023-04-07 13:36:45 - Current:0.48788943886756897 Best:0.4874677062034607\n",
            "[INFO] 2023-04-07 13:36:45 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:47 - Epoch : 8 Train loss: 12.909583595298384 Val loss: 40.91706771850586 Train ndcg_10 0.6058445572853088 Val ndcg_10 0.49471479654312134\n",
            "[INFO] 2023-04-07 13:36:47 - Current:0.49471479654312134 Best:0.48788943886756897\n",
            "[INFO] 2023-04-07 13:36:47 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:50 - Epoch : 9 Train loss: 12.932801437940569 Val loss: 40.87209676106771 Train ndcg_10 0.6068403124809265 Val ndcg_10 0.5032044053077698\n",
            "[INFO] 2023-04-07 13:36:50 - Current:0.5032044053077698 Best:0.49471479654312134\n",
            "[INFO] 2023-04-07 13:36:50 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:53 - Epoch : 10 Train loss: 12.88845134273743 Val loss: 40.92848307291667 Train ndcg_10 0.6145600080490112 Val ndcg_10 0.5130866765975952\n",
            "[INFO] 2023-04-07 13:36:53 - Current:0.5130866765975952 Best:0.5032044053077698\n",
            "[INFO] 2023-04-07 13:36:53 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:57 - Epoch : 11 Train loss: 12.914492719644642 Val loss: 40.93913065592448 Train ndcg_10 0.624613344669342 Val ndcg_10 0.5193436145782471\n",
            "[INFO] 2023-04-07 13:36:57 - Current:0.5193436145782471 Best:0.5130866765975952\n",
            "[INFO] 2023-04-07 13:36:57 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:36:59 - Epoch : 12 Train loss: 12.967400317346804 Val loss: 40.82129414876302 Train ndcg_10 0.6176257729530334 Val ndcg_10 0.528693437576294\n",
            "[INFO] 2023-04-07 13:36:59 - Current:0.528693437576294 Best:0.5193436145782471\n",
            "[INFO] 2023-04-07 13:36:59 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:37:02 - Epoch : 13 Train loss: 12.925507773340275 Val loss: 40.83542048136393 Train ndcg_10 0.6293433904647827 Val ndcg_10 0.5329857468605042\n",
            "[INFO] 2023-04-07 13:37:02 - Current:0.5329857468605042 Best:0.528693437576294\n",
            "[INFO] 2023-04-07 13:37:02 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:37:04 - Epoch : 14 Train loss: 12.908294267007383 Val loss: 40.80780232747396 Train ndcg_10 0.6155888438224792 Val ndcg_10 0.5411765575408936\n",
            "[INFO] 2023-04-07 13:37:04 - Current:0.5411765575408936 Best:0.5329857468605042\n",
            "[INFO] 2023-04-07 13:37:04 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:07 - Epoch : 15 Train loss: 12.874197746096812 Val loss: 40.79299647013347 Train ndcg_10 0.6124593615531921 Val ndcg_10 0.5405256152153015\n",
            "[INFO] 2023-04-07 13:37:07 - Current:0.5405256152153015 Best:0.5411765575408936\n",
            "[INFO] 2023-04-07 13:37:07 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:11 - Epoch : 16 Train loss: 12.920975046523553 Val loss: 40.80061569213867 Train ndcg_10 0.634320080280304 Val ndcg_10 0.5427030920982361\n",
            "[INFO] 2023-04-07 13:37:11 - Current:0.5427030920982361 Best:0.5411765575408936\n",
            "[INFO] 2023-04-07 13:37:11 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:14 - Epoch : 17 Train loss: 12.893313945218877 Val loss: 40.8309575398763 Train ndcg_10 0.6397711038589478 Val ndcg_10 0.5447614789009094\n",
            "[INFO] 2023-04-07 13:37:14 - Current:0.5447614789009094 Best:0.5427030920982361\n",
            "[INFO] 2023-04-07 13:37:14 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:16 - Epoch : 18 Train loss: 12.875501432953332 Val loss: 40.79857864379883 Train ndcg_10 0.6200866103172302 Val ndcg_10 0.5450605154037476\n",
            "[INFO] 2023-04-07 13:37:16 - Current:0.5450605154037476 Best:0.5447614789009094\n",
            "[INFO] 2023-04-07 13:37:16 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:19 - Epoch : 19 Train loss: 12.893008181479125 Val loss: 40.86951726277669 Train ndcg_10 0.6290003061294556 Val ndcg_10 0.5468230843544006\n",
            "[INFO] 2023-04-07 13:37:19 - Current:0.5468230843544006 Best:0.5450605154037476\n",
            "[INFO] 2023-04-07 13:37:19 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:22 - Epoch : 20 Train loss: 12.860522228004658 Val loss: 40.832191721598306 Train ndcg_10 0.6197737455368042 Val ndcg_10 0.5479246973991394\n",
            "[INFO] 2023-04-07 13:37:22 - Current:0.5479246973991394 Best:0.5468230843544006\n",
            "[INFO] 2023-04-07 13:37:22 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:26 - Epoch : 21 Train loss: 12.876259066713946 Val loss: 40.7998908996582 Train ndcg_10 0.6208502650260925 Val ndcg_10 0.5480080842971802\n",
            "[INFO] 2023-04-07 13:37:26 - Current:0.5480080842971802 Best:0.5479246973991394\n",
            "[INFO] 2023-04-07 13:37:26 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:29 - Epoch : 22 Train loss: 12.897853243667468 Val loss: 40.90544102986654 Train ndcg_10 0.6278268694877625 Val ndcg_10 0.550475001335144\n",
            "[INFO] 2023-04-07 13:37:29 - Current:0.550475001335144 Best:0.5480080842971802\n",
            "[INFO] 2023-04-07 13:37:29 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:32 - Epoch : 23 Train loss: 12.8878327653823 Val loss: 40.83937479654948 Train ndcg_10 0.6222164630889893 Val ndcg_10 0.5546227097511292\n",
            "[INFO] 2023-04-07 13:37:32 - Current:0.5546227097511292 Best:0.550475001335144\n",
            "[INFO] 2023-04-07 13:37:32 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:34 - Epoch : 24 Train loss: 12.878820557158253 Val loss: 40.89409866333008 Train ndcg_10 0.6187023520469666 Val ndcg_10 0.5554242730140686\n",
            "[INFO] 2023-04-07 13:37:34 - Current:0.5554242730140686 Best:0.5546227097511292\n",
            "[INFO] 2023-04-07 13:37:34 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:37 - Epoch : 25 Train loss: 12.830378189199442 Val loss: 40.88165079752604 Train ndcg_10 0.6282040476799011 Val ndcg_10 0.5560786724090576\n",
            "[INFO] 2023-04-07 13:37:37 - Current:0.5560786724090576 Best:0.5554242730140686\n",
            "[INFO] 2023-04-07 13:37:37 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:41 - Epoch : 26 Train loss: 12.921031285176236 Val loss: 40.82695337931315 Train ndcg_10 0.6204840540885925 Val ndcg_10 0.5573222637176514\n",
            "[INFO] 2023-04-07 13:37:41 - Current:0.5573222637176514 Best:0.5560786724090576\n",
            "[INFO] 2023-04-07 13:37:41 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:44 - Epoch : 27 Train loss: 12.881342685328121 Val loss: 40.83969395955403 Train ndcg_10 0.6189035177230835 Val ndcg_10 0.558160126209259\n",
            "[INFO] 2023-04-07 13:37:44 - Current:0.558160126209259 Best:0.5573222637176514\n",
            "[INFO] 2023-04-07 13:37:44 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:46 - Epoch : 28 Train loss: 12.868380915098838 Val loss: 40.861837768554686 Train ndcg_10 0.643701434135437 Val ndcg_10 0.5587642192840576\n",
            "[INFO] 2023-04-07 13:37:46 - Current:0.5587642192840576 Best:0.558160126209259\n",
            "[INFO] 2023-04-07 13:37:46 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:37:49 - Epoch : 29 Train loss: 12.88451741930306 Val loss: 40.86313044230143 Train ndcg_10 0.6357827186584473 Val ndcg_10 0.5612306594848633\n",
            "[INFO] 2023-04-07 13:37:49 - Current:0.5612306594848633 Best:0.5587642192840576\n",
            "[INFO] 2023-04-07 13:37:49 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:37:53 - Epoch : 30 Train loss: 12.869912921151581 Val loss: 40.84326197306315 Train ndcg_10 0.6182562708854675 Val ndcg_10 0.5612306594848633\n",
            "[INFO] 2023-04-07 13:37:53 - Current:0.5612306594848633 Best:0.5612306594848633\n",
            "[INFO] 2023-04-07 13:37:53 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:37:56 - Epoch : 31 Train loss: 12.86125784286004 Val loss: 40.8186902364095 Train ndcg_10 0.6353819370269775 Val ndcg_10 0.5619656443595886\n",
            "[INFO] 2023-04-07 13:37:56 - Current:0.5619656443595886 Best:0.5612306594848633\n",
            "[INFO] 2023-04-07 13:37:56 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:37:58 - Epoch : 32 Train loss: 12.864992664978567 Val loss: 40.8521001180013 Train ndcg_10 0.6235811114311218 Val ndcg_10 0.5618167519569397\n",
            "[INFO] 2023-04-07 13:37:58 - Current:0.5618167519569397 Best:0.5619656443595886\n",
            "[INFO] 2023-04-07 13:37:58 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:01 - Epoch : 33 Train loss: 12.908641331315392 Val loss: 40.789528147379556 Train ndcg_10 0.6365581154823303 Val ndcg_10 0.5618504881858826\n",
            "[INFO] 2023-04-07 13:38:01 - Current:0.5618504881858826 Best:0.5619656443595886\n",
            "[INFO] 2023-04-07 13:38:01 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:04 - Epoch : 34 Train loss: 12.876697680943131 Val loss: 40.83667933146159 Train ndcg_10 0.62669438123703 Val ndcg_10 0.5618482828140259\n",
            "[INFO] 2023-04-07 13:38:04 - Current:0.5618482828140259 Best:0.5619656443595886\n",
            "[INFO] 2023-04-07 13:38:04 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:08 - Epoch : 35 Train loss: 12.904865484322066 Val loss: 40.8417849222819 Train ndcg_10 0.619564950466156 Val ndcg_10 0.5622502565383911\n",
            "[INFO] 2023-04-07 13:38:08 - Current:0.5622502565383911 Best:0.5619656443595886\n",
            "[INFO] 2023-04-07 13:38:08 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:10 - Epoch : 36 Train loss: 12.882299878955942 Val loss: 40.81679712931315 Train ndcg_10 0.6402504444122314 Val ndcg_10 0.5643123388290405\n",
            "[INFO] 2023-04-07 13:38:10 - Current:0.5643123388290405 Best:0.5622502565383911\n",
            "[INFO] 2023-04-07 13:38:10 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:13 - Epoch : 37 Train loss: 12.91100752670153 Val loss: 40.806585693359374 Train ndcg_10 0.6234836578369141 Val ndcg_10 0.564777672290802\n",
            "[INFO] 2023-04-07 13:38:13 - Current:0.564777672290802 Best:0.5643123388290405\n",
            "[INFO] 2023-04-07 13:38:13 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:16 - Epoch : 38 Train loss: 12.871673294224921 Val loss: 40.8307238260905 Train ndcg_10 0.6120390892028809 Val ndcg_10 0.5649736523628235\n",
            "[INFO] 2023-04-07 13:38:16 - Current:0.5649736523628235 Best:0.564777672290802\n",
            "[INFO] 2023-04-07 13:38:16 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:19 - Epoch : 39 Train loss: 12.855061092207917 Val loss: 40.82079188028971 Train ndcg_10 0.6161270141601562 Val ndcg_10 0.5650609731674194\n",
            "[INFO] 2023-04-07 13:38:19 - Current:0.5650609731674194 Best:0.5649736523628235\n",
            "[INFO] 2023-04-07 13:38:19 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:22 - Epoch : 40 Train loss: 12.874535417134783 Val loss: 40.8412109375 Train ndcg_10 0.6155614256858826 Val ndcg_10 0.5650521516799927\n",
            "[INFO] 2023-04-07 13:38:22 - Current:0.5650521516799927 Best:0.5650609731674194\n",
            "[INFO] 2023-04-07 13:38:22 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:25 - Epoch : 41 Train loss: 12.873110498298937 Val loss: 40.85570500691732 Train ndcg_10 0.6236159801483154 Val ndcg_10 0.5650811791419983\n",
            "[INFO] 2023-04-07 13:38:25 - Current:0.5650811791419983 Best:0.5650609731674194\n",
            "[INFO] 2023-04-07 13:38:25 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:28 - Epoch : 42 Train loss: 12.83793073558526 Val loss: 40.808852132161455 Train ndcg_10 0.6363022923469543 Val ndcg_10 0.5658689141273499\n",
            "[INFO] 2023-04-07 13:38:28 - Current:0.5658689141273499 Best:0.5650811791419983\n",
            "[INFO] 2023-04-07 13:38:28 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:31 - Epoch : 43 Train loss: 12.87849464247712 Val loss: 40.821568552653 Train ndcg_10 0.6332880854606628 Val ndcg_10 0.5658689141273499\n",
            "[INFO] 2023-04-07 13:38:31 - Current:0.5658689141273499 Best:0.5658689141273499\n",
            "[INFO] 2023-04-07 13:38:31 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:38:34 - Epoch : 44 Train loss: 12.884917785284442 Val loss: 40.83336588541667 Train ndcg_10 0.6400224566459656 Val ndcg_10 0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:34 - Current:0.5662961602210999 Best:0.5658689141273499\n",
            "[INFO] 2023-04-07 13:38:34 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:38:37 - Epoch : 45 Train loss: 12.876975194542808 Val loss: 40.765586598714194 Train ndcg_10 0.6227433085441589 Val ndcg_10 0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:37 - Current:0.5662961602210999 Best:0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:37 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:38:40 - Epoch : 46 Train loss: 12.882317090104815 Val loss: 40.8287607828776 Train ndcg_10 0.6337335109710693 Val ndcg_10 0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:40 - Current:0.5662961602210999 Best:0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:40 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:38:43 - Epoch : 47 Train loss: 12.870502193417169 Val loss: 40.79083353678386 Train ndcg_10 0.6283231973648071 Val ndcg_10 0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:43 - Current:0.5662961602210999 Best:0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:43 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:38:45 - Epoch : 48 Train loss: 12.872712202831707 Val loss: 40.82168299357097 Train ndcg_10 0.6417721509933472 Val ndcg_10 0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:45 - Current:0.5662961602210999 Best:0.5662961602210999\n",
            "[INFO] 2023-04-07 13:38:45 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:38:49 - Epoch : 49 Train loss: 12.863382603918206 Val loss: 40.79160639444987 Train ndcg_10 0.6417099237442017 Val ndcg_10 0.5666662454605103\n",
            "[INFO] 2023-04-07 13:38:49 - Current:0.5666662454605103 Best:0.5662961602210999\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:38:51 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold1-ordinal', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold1-ordinal', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:38:51 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold1/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='ordinal', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:38:51 - will execute cp config.json output/results/MQ2008-Fold1-ordinal/used_config.json\n",
            "[INFO] 2023-04-07 13:38:51 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:38:51 - will load train data from MQ2008/Fold1/train.txt\n",
            "[INFO] 2023-04-07 13:38:52 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/train.txt'> and got x shape (7903, 46), y shape (7903,) and query_ids shape (7903,)\n",
            "[INFO] 2023-04-07 13:38:52 - loaded dataset with 339 queries\n",
            "[INFO] 2023-04-07 13:38:52 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:38:52 - train DS shape: [339, 121, 46]\n",
            "[INFO] 2023-04-07 13:38:52 - will load vali data from MQ2008/Fold1/vali.txt\n",
            "[INFO] 2023-04-07 13:38:52 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/vali.txt'> and got x shape (2104, 46), y shape (2104,) and query_ids shape (2104,)\n",
            "[INFO] 2023-04-07 13:38:52 - loaded dataset with 120 queries\n",
            "[INFO] 2023-04-07 13:38:52 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:38:52 - vali DS shape: [120, 118, 46]\n",
            "[INFO] 2023-04-07 13:38:52 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:38:52 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:38:52 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:38:52 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:38:52 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: ordinal() missing 1 required positional argument: 'n'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:38:54 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold1-lambdaLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold1-lambdaLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:38:54 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold1/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='lambdaLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:38:54 - will execute cp config.json output/results/MQ2008-Fold1-lambdaLoss/used_config.json\n",
            "[INFO] 2023-04-07 13:38:54 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:38:54 - will load train data from MQ2008/Fold1/train.txt\n",
            "[INFO] 2023-04-07 13:38:54 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/train.txt'> and got x shape (7903, 46), y shape (7903,) and query_ids shape (7903,)\n",
            "[INFO] 2023-04-07 13:38:54 - loaded dataset with 339 queries\n",
            "[INFO] 2023-04-07 13:38:54 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:38:54 - train DS shape: [339, 121, 46]\n",
            "[INFO] 2023-04-07 13:38:54 - will load vali data from MQ2008/Fold1/vali.txt\n",
            "[INFO] 2023-04-07 13:38:54 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/vali.txt'> and got x shape (2104, 46), y shape (2104,) and query_ids shape (2104,)\n",
            "[INFO] 2023-04-07 13:38:54 - loaded dataset with 120 queries\n",
            "[INFO] 2023-04-07 13:38:54 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:38:54 - vali DS shape: [120, 118, 46]\n",
            "[INFO] 2023-04-07 13:38:54 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:38:54 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:38:54 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:38:54 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:38:54 - Current learning rate: 0.001\n",
            "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in Log2Backward. Traceback of forward call that caused the error:\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/models/losses/lambdaLoss.py\", line 70, in lambdaLoss\n",
            "losses = torch.log2(weighted_probas)\n",
            "(Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
            "Variable._execution_engine.run_backward(\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 23, in loss_batch\n",
            "loss.backward()\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/tensor.py\", line 245, in backward\n",
            "torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 145, in backward\n",
            "Variable._execution_engine.run_backward(\n",
            "RuntimeError: Function 'Log2Backward' returned nan values in its 0th output.\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:38:56 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold1-approxNDCGLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:38:56 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold1/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='approxNDCGLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:38:56 - will execute cp config.json output/results/MQ2008-Fold1-approxNDCGLoss/used_config.json\n",
            "[INFO] 2023-04-07 13:38:56 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:38:56 - will load train data from MQ2008/Fold1/train.txt\n",
            "[INFO] 2023-04-07 13:38:57 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/train.txt'> and got x shape (7903, 46), y shape (7903,) and query_ids shape (7903,)\n",
            "[INFO] 2023-04-07 13:38:57 - loaded dataset with 339 queries\n",
            "[INFO] 2023-04-07 13:38:57 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:38:57 - train DS shape: [339, 121, 46]\n",
            "[INFO] 2023-04-07 13:38:57 - will load vali data from MQ2008/Fold1/vali.txt\n",
            "[INFO] 2023-04-07 13:38:57 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/vali.txt'> and got x shape (2104, 46), y shape (2104,) and query_ids shape (2104,)\n",
            "[INFO] 2023-04-07 13:38:57 - loaded dataset with 120 queries\n",
            "[INFO] 2023-04-07 13:38:57 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:38:57 - vali DS shape: [120, 118, 46]\n",
            "[INFO] 2023-04-07 13:38:57 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:38:57 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:38:57 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:38:57 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:38:57 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:00 - Epoch : 0 Train loss: -0.48712033938869265 Val loss: -0.4704930861790975 Train ndcg_10 0.5752156376838684 Val ndcg_10 0.44151610136032104\n",
            "[INFO] 2023-04-07 13:39:00 - Current:0.44151610136032104 Best:0.0\n",
            "[INFO] 2023-04-07 13:39:00 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:03 - Epoch : 1 Train loss: -0.49194980476458167 Val loss: -0.4705169975757599 Train ndcg_10 0.5604270696640015 Val ndcg_10 0.442576140165329\n",
            "[INFO] 2023-04-07 13:39:03 - Current:0.442576140165329 Best:0.44151610136032104\n",
            "[INFO] 2023-04-07 13:39:03 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:06 - Epoch : 2 Train loss: -0.4875364580513102 Val loss: -0.47054282228151956 Train ndcg_10 0.5622959136962891 Val ndcg_10 0.44295060634613037\n",
            "[INFO] 2023-04-07 13:39:06 - Current:0.44295060634613037 Best:0.442576140165329\n",
            "[INFO] 2023-04-07 13:39:06 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:09 - Epoch : 3 Train loss: -0.4886800144801801 Val loss: -0.47056651711463926 Train ndcg_10 0.5784173011779785 Val ndcg_10 0.4432494342327118\n",
            "[INFO] 2023-04-07 13:39:09 - Current:0.4432494342327118 Best:0.44295060634613037\n",
            "[INFO] 2023-04-07 13:39:09 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:11 - Epoch : 4 Train loss: -0.4869111034370805 Val loss: -0.4705915987491608 Train ndcg_10 0.5663822293281555 Val ndcg_10 0.4431935250759125\n",
            "[INFO] 2023-04-07 13:39:11 - Current:0.4431935250759125 Best:0.4432494342327118\n",
            "[INFO] 2023-04-07 13:39:11 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:15 - Epoch : 5 Train loss: -0.49390713057335145 Val loss: -0.4706150472164154 Train ndcg_10 0.568215548992157 Val ndcg_10 0.4431935250759125\n",
            "[INFO] 2023-04-07 13:39:15 - Current:0.4431935250759125 Best:0.4432494342327118\n",
            "[INFO] 2023-04-07 13:39:15 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:18 - Epoch : 6 Train loss: -0.48898646384917177 Val loss: -0.47063920497894285 Train ndcg_10 0.5690746903419495 Val ndcg_10 0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:18 - Current:0.4432927072048187 Best:0.4432494342327118\n",
            "[INFO] 2023-04-07 13:39:18 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:21 - Epoch : 7 Train loss: -0.49018932083363376 Val loss: -0.47066524227460227 Train ndcg_10 0.5746546983718872 Val ndcg_10 0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:21 - Current:0.4432927072048187 Best:0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:21 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:24 - Epoch : 8 Train loss: -0.48778975616514153 Val loss: -0.47068982720375063 Train ndcg_10 0.5725600719451904 Val ndcg_10 0.44303736090660095\n",
            "[INFO] 2023-04-07 13:39:24 - Current:0.44303736090660095 Best:0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:24 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:26 - Epoch : 9 Train loss: -0.4914699137035021 Val loss: -0.47071529428164166 Train ndcg_10 0.5703185796737671 Val ndcg_10 0.4431542158126831\n",
            "[INFO] 2023-04-07 13:39:26 - Current:0.4431542158126831 Best:0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:26 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:30 - Epoch : 10 Train loss: -0.4854590182459108 Val loss: -0.4707392950852712 Train ndcg_10 0.5512831211090088 Val ndcg_10 0.4430355131626129\n",
            "[INFO] 2023-04-07 13:39:30 - Current:0.4430355131626129 Best:0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:30 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:33 - Epoch : 11 Train loss: -0.48897436533705674 Val loss: -0.47076276342074075 Train ndcg_10 0.5803717374801636 Val ndcg_10 0.44323137402534485\n",
            "[INFO] 2023-04-07 13:39:33 - Current:0.44323137402534485 Best:0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:33 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:36 - Epoch : 12 Train loss: -0.4875788217448907 Val loss: -0.4707862695058187 Train ndcg_10 0.5765207409858704 Val ndcg_10 0.446334570646286\n",
            "[INFO] 2023-04-07 13:39:36 - Current:0.446334570646286 Best:0.4432927072048187\n",
            "[INFO] 2023-04-07 13:39:36 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:38 - Epoch : 13 Train loss: -0.485264226046987 Val loss: -0.47081247170766194 Train ndcg_10 0.5741033554077148 Val ndcg_10 0.44668737053871155\n",
            "[INFO] 2023-04-07 13:39:38 - Current:0.44668737053871155 Best:0.446334570646286\n",
            "[INFO] 2023-04-07 13:39:38 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:39:42 - Epoch : 14 Train loss: -0.4882846523878497 Val loss: -0.47083736062049864 Train ndcg_10 0.5871761441230774 Val ndcg_10 0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:42 - Current:0.44710180163383484 Best:0.44668737053871155\n",
            "[INFO] 2023-04-07 13:39:42 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:39:45 - Epoch : 15 Train loss: -0.48895946874731055 Val loss: -0.47084204157193504 Train ndcg_10 0.5739142298698425 Val ndcg_10 0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:45 - Current:0.44710180163383484 Best:0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:45 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:39:48 - Epoch : 16 Train loss: -0.4875029760651884 Val loss: -0.4708472788333893 Train ndcg_10 0.5751806497573853 Val ndcg_10 0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:48 - Current:0.44710180163383484 Best:0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:48 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:39:51 - Epoch : 17 Train loss: -0.49223005982626855 Val loss: -0.47085246245066326 Train ndcg_10 0.5790776014328003 Val ndcg_10 0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:51 - Current:0.44710180163383484 Best:0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:51 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:39:53 - Epoch : 18 Train loss: -0.4899026195911531 Val loss: -0.4708573261896769 Train ndcg_10 0.560749888420105 Val ndcg_10 0.44735100865364075\n",
            "[INFO] 2023-04-07 13:39:53 - Current:0.44735100865364075 Best:0.44710180163383484\n",
            "[INFO] 2023-04-07 13:39:53 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:39:57 - Epoch : 19 Train loss: -0.4864959639433914 Val loss: -0.4708621839682261 Train ndcg_10 0.5743314623832703 Val ndcg_10 0.44731566309928894\n",
            "[INFO] 2023-04-07 13:39:57 - Current:0.44731566309928894 Best:0.44735100865364075\n",
            "[INFO] 2023-04-07 13:39:57 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:00 - Epoch : 20 Train loss: -0.4861994516708858 Val loss: -0.4708671828111013 Train ndcg_10 0.5776834487915039 Val ndcg_10 0.44731566309928894\n",
            "[INFO] 2023-04-07 13:40:00 - Current:0.44731566309928894 Best:0.44735100865364075\n",
            "[INFO] 2023-04-07 13:40:00 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:03 - Epoch : 21 Train loss: -0.4873625515660705 Val loss: -0.47087290287017824 Train ndcg_10 0.5828338265419006 Val ndcg_10 0.44731566309928894\n",
            "[INFO] 2023-04-07 13:40:03 - Current:0.44731566309928894 Best:0.44735100865364075\n",
            "[INFO] 2023-04-07 13:40:03 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:06 - Epoch : 22 Train loss: -0.48572923651838723 Val loss: -0.47087799112002054 Train ndcg_10 0.5734174251556396 Val ndcg_10 0.4473343789577484\n",
            "[INFO] 2023-04-07 13:40:06 - Current:0.4473343789577484 Best:0.44735100865364075\n",
            "[INFO] 2023-04-07 13:40:06 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:09 - Epoch : 23 Train loss: -0.48399667050634515 Val loss: -0.47088295022646587 Train ndcg_10 0.5743269324302673 Val ndcg_10 0.4473528265953064\n",
            "[INFO] 2023-04-07 13:40:09 - Current:0.4473528265953064 Best:0.44735100865364075\n",
            "[INFO] 2023-04-07 13:40:09 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:13 - Epoch : 24 Train loss: -0.4913001696972017 Val loss: -0.4708883782227834 Train ndcg_10 0.5677060484886169 Val ndcg_10 0.4473528265953064\n",
            "[INFO] 2023-04-07 13:40:13 - Current:0.4473528265953064 Best:0.4473528265953064\n",
            "[INFO] 2023-04-07 13:40:13 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:15 - Epoch : 25 Train loss: -0.48977849001729734 Val loss: -0.4708932856718699 Train ndcg_10 0.5600112676620483 Val ndcg_10 0.44736310839653015\n",
            "[INFO] 2023-04-07 13:40:15 - Current:0.44736310839653015 Best:0.4473528265953064\n",
            "[INFO] 2023-04-07 13:40:15 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:18 - Epoch : 26 Train loss: -0.48962657456904385 Val loss: -0.47089792291323346 Train ndcg_10 0.57069331407547 Val ndcg_10 0.44917160272598267\n",
            "[INFO] 2023-04-07 13:40:18 - Current:0.44917160272598267 Best:0.44736310839653015\n",
            "[INFO] 2023-04-07 13:40:18 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:21 - Epoch : 27 Train loss: -0.4912179077269405 Val loss: -0.47090306878089905 Train ndcg_10 0.5672690868377686 Val ndcg_10 0.44917160272598267\n",
            "[INFO] 2023-04-07 13:40:21 - Current:0.44917160272598267 Best:0.44917160272598267\n",
            "[INFO] 2023-04-07 13:40:21 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:25 - Epoch : 28 Train loss: -0.487791250149409 Val loss: -0.470907864967982 Train ndcg_10 0.5936533808708191 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:25 - Current:0.44936221837997437 Best:0.44917160272598267\n",
            "[INFO] 2023-04-07 13:40:25 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:40:27 - Epoch : 29 Train loss: -0.49110232790311176 Val loss: -0.4709128439426422 Train ndcg_10 0.5715979933738708 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:27 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:27 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:30 - Epoch : 30 Train loss: -0.48786863605533026 Val loss: -0.4709138015906016 Train ndcg_10 0.571119487285614 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:30 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:30 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:33 - Epoch : 31 Train loss: -0.48830823376115445 Val loss: -0.47091472943623863 Train ndcg_10 0.5734768509864807 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:33 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:33 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:36 - Epoch : 32 Train loss: -0.48622914278401735 Val loss: -0.47091577649116517 Train ndcg_10 0.5773968696594238 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:36 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:36 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:40 - Epoch : 33 Train loss: -0.4875075476886952 Val loss: -0.47091677188873293 Train ndcg_10 0.5849618315696716 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:40 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:40 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:43 - Epoch : 34 Train loss: -0.4883271204335148 Val loss: -0.4709177315235138 Train ndcg_10 0.5725116729736328 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:43 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:43 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:45 - Epoch : 35 Train loss: -0.49050321058537755 Val loss: -0.47091870109240214 Train ndcg_10 0.5704692006111145 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:45 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:45 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:48 - Epoch : 36 Train loss: -0.48725660272755805 Val loss: -0.4709196408589681 Train ndcg_10 0.5758236646652222 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:48 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:48 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:52 - Epoch : 37 Train loss: -0.487008157205441 Val loss: -0.47092060844103495 Train ndcg_10 0.5649701952934265 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:52 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:52 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:55 - Epoch : 38 Train loss: -0.4870860069374771 Val loss: -0.4709216018517812 Train ndcg_10 0.5761957168579102 Val ndcg_10 0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:55 - Current:0.44936221837997437 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:55 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:40:57 - Epoch : 39 Train loss: -0.48544131716092426 Val loss: -0.47092270652453105 Train ndcg_10 0.5794538855552673 Val ndcg_10 0.4494258463382721\n",
            "[INFO] 2023-04-07 13:40:57 - Current:0.4494258463382721 Best:0.44936221837997437\n",
            "[INFO] 2023-04-07 13:40:57 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:41:00 - Epoch : 40 Train loss: -0.48441857987448883 Val loss: -0.4709236959616343 Train ndcg_10 0.5565239787101746 Val ndcg_10 0.4494258463382721\n",
            "[INFO] 2023-04-07 13:41:00 - Current:0.4494258463382721 Best:0.4494258463382721\n",
            "[INFO] 2023-04-07 13:41:00 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:41:03 - Epoch : 41 Train loss: -0.4898402719019437 Val loss: -0.47092469334602355 Train ndcg_10 0.5714901685714722 Val ndcg_10 0.4494258463382721\n",
            "[INFO] 2023-04-07 13:41:03 - Current:0.4494258463382721 Best:0.4494258463382721\n",
            "[INFO] 2023-04-07 13:41:03 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:41:07 - Epoch : 42 Train loss: -0.4883025255175115 Val loss: -0.4709256152311961 Train ndcg_10 0.5697141289710999 Val ndcg_10 0.4494258463382721\n",
            "[INFO] 2023-04-07 13:41:07 - Current:0.4494258463382721 Best:0.4494258463382721\n",
            "[INFO] 2023-04-07 13:41:07 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:41:10 - Epoch : 43 Train loss: -0.4879269189363384 Val loss: -0.4709265927473704 Train ndcg_10 0.5684431791305542 Val ndcg_10 0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:10 - Current:0.4494650959968567 Best:0.4494258463382721\n",
            "[INFO] 2023-04-07 13:41:10 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:41:12 - Epoch : 44 Train loss: -0.4901608975939343 Val loss: -0.4709275464216868 Train ndcg_10 0.5928457975387573 Val ndcg_10 0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:12 - Current:0.4494650959968567 Best:0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:12 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:41:15 - Epoch : 45 Train loss: -0.4888461984715982 Val loss: -0.4709277411301931 Train ndcg_10 0.5613447427749634 Val ndcg_10 0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:15 - Current:0.4494650959968567 Best:0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:15 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:41:19 - Epoch : 46 Train loss: -0.487832725399715 Val loss: -0.47092792789141336 Train ndcg_10 0.5771650671958923 Val ndcg_10 0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:19 - Current:0.4494650959968567 Best:0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:19 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:41:22 - Epoch : 47 Train loss: -0.49115009961929995 Val loss: -0.47092808882395426 Train ndcg_10 0.5791332721710205 Val ndcg_10 0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:22 - Current:0.4494650959968567 Best:0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:22 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:41:25 - Epoch : 48 Train loss: -0.4887737719710246 Val loss: -0.47092827161153156 Train ndcg_10 0.5798147320747375 Val ndcg_10 0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:25 - Current:0.4494650959968567 Best:0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:25 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:41:27 - Epoch : 49 Train loss: -0.4865046449467144 Val loss: -0.47092846234639485 Train ndcg_10 0.5592293739318848 Val ndcg_10 0.4494650959968567\n",
            "[INFO] 2023-04-07 13:41:27 - Current:0.4494650959968567 Best:0.4494650959968567\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:41:29 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold1-pointwise_rmse', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold1-pointwise_rmse', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:41:29 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold1/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='pointwise_rmse', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:41:29 - will execute cp config.json output/results/MQ2008-Fold1-pointwise_rmse/used_config.json\n",
            "[INFO] 2023-04-07 13:41:29 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:41:29 - will load train data from MQ2008/Fold1/train.txt\n",
            "[INFO] 2023-04-07 13:41:29 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/train.txt'> and got x shape (7903, 46), y shape (7903,) and query_ids shape (7903,)\n",
            "[INFO] 2023-04-07 13:41:29 - loaded dataset with 339 queries\n",
            "[INFO] 2023-04-07 13:41:29 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:41:29 - train DS shape: [339, 121, 46]\n",
            "[INFO] 2023-04-07 13:41:29 - will load vali data from MQ2008/Fold1/vali.txt\n",
            "[INFO] 2023-04-07 13:41:29 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/vali.txt'> and got x shape (2104, 46), y shape (2104,) and query_ids shape (2104,)\n",
            "[INFO] 2023-04-07 13:41:30 - loaded dataset with 120 queries\n",
            "[INFO] 2023-04-07 13:41:30 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:41:30 - vali DS shape: [120, 118, 46]\n",
            "[INFO] 2023-04-07 13:41:30 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:41:30 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:41:30 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:41:30 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:41:30 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: pointwise_rmse() missing 1 required positional argument: 'no_of_levels'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:41:33 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold1-neuralNDCG', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold1-neuralNDCG', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:41:33 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold1/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='neuralNDCG', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:41:33 - will execute cp config.json output/results/MQ2008-Fold1-neuralNDCG/used_config.json\n",
            "[INFO] 2023-04-07 13:41:33 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:41:33 - will load train data from MQ2008/Fold1/train.txt\n",
            "[INFO] 2023-04-07 13:41:33 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/train.txt'> and got x shape (7903, 46), y shape (7903,) and query_ids shape (7903,)\n",
            "[INFO] 2023-04-07 13:41:33 - loaded dataset with 339 queries\n",
            "[INFO] 2023-04-07 13:41:33 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:41:33 - train DS shape: [339, 121, 46]\n",
            "[INFO] 2023-04-07 13:41:33 - will load vali data from MQ2008/Fold1/vali.txt\n",
            "[INFO] 2023-04-07 13:41:33 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold1/vali.txt'> and got x shape (2104, 46), y shape (2104,) and query_ids shape (2104,)\n",
            "[INFO] 2023-04-07 13:41:33 - loaded dataset with 120 queries\n",
            "[INFO] 2023-04-07 13:41:33 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:41:33 - vali DS shape: [120, 118, 46]\n",
            "[INFO] 2023-04-07 13:41:33 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:41:33 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:41:33 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:41:33 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:41:33 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:41:37 - Epoch : 0 Train loss: -0.5777135674229062 Val loss: -0.5585923393567404 Train ndcg_10 0.5771613121032715 Val ndcg_10 0.4432927072048187\n",
            "[INFO] 2023-04-07 13:41:37 - Current:0.4432927072048187 Best:0.0\n",
            "[INFO] 2023-04-07 13:41:37 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:41:40 - Epoch : 1 Train loss: -0.5889302718955859 Val loss: -0.560155455271403 Train ndcg_10 0.5624327659606934 Val ndcg_10 0.44702041149139404\n",
            "[INFO] 2023-04-07 13:41:40 - Current:0.44702041149139404 Best:0.4432927072048187\n",
            "[INFO] 2023-04-07 13:41:40 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:41:43 - Epoch : 2 Train loss: -0.5794893146264518 Val loss: -0.5618151744206746 Train ndcg_10 0.5681752562522888 Val ndcg_10 0.4510231018066406\n",
            "[INFO] 2023-04-07 13:41:43 - Current:0.4510231018066406 Best:0.44702041149139404\n",
            "[INFO] 2023-04-07 13:41:43 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:41:48 - Epoch : 3 Train loss: -0.5927190873826851 Val loss: -0.5633579810460408 Train ndcg_10 0.5830562114715576 Val ndcg_10 0.4536067843437195\n",
            "[INFO] 2023-04-07 13:41:48 - Current:0.4536067843437195 Best:0.4510231018066406\n",
            "[INFO] 2023-04-07 13:41:48 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:41:51 - Epoch : 4 Train loss: -0.5863627446787899 Val loss: -0.5649902184804281 Train ndcg_10 0.573833703994751 Val ndcg_10 0.45124563574790955\n",
            "[INFO] 2023-04-07 13:41:51 - Current:0.45124563574790955 Best:0.4536067843437195\n",
            "[INFO] 2023-04-07 13:41:51 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:41:54 - Epoch : 5 Train loss: -0.5943205446268605 Val loss: -0.5665211200714111 Train ndcg_10 0.5789944529533386 Val ndcg_10 0.45779910683631897\n",
            "[INFO] 2023-04-07 13:41:54 - Current:0.45779910683631897 Best:0.4536067843437195\n",
            "[INFO] 2023-04-07 13:41:54 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:41:58 - Epoch : 6 Train loss: -0.5917994103952143 Val loss: -0.5681176900863647 Train ndcg_10 0.5813954472541809 Val ndcg_10 0.4613427519798279\n",
            "[INFO] 2023-04-07 13:41:58 - Current:0.4613427519798279 Best:0.45779910683631897\n",
            "[INFO] 2023-04-07 13:41:58 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:02 - Epoch : 7 Train loss: -0.597034935754309 Val loss: -0.5698268969853719 Train ndcg_10 0.5868879556655884 Val ndcg_10 0.4672369360923767\n",
            "[INFO] 2023-04-07 13:42:02 - Current:0.4672369360923767 Best:0.4613427519798279\n",
            "[INFO] 2023-04-07 13:42:02 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:06 - Epoch : 8 Train loss: -0.5855105350502824 Val loss: -0.5713839054107666 Train ndcg_10 0.5880736708641052 Val ndcg_10 0.47056037187576294\n",
            "[INFO] 2023-04-07 13:42:06 - Current:0.47056037187576294 Best:0.4672369360923767\n",
            "[INFO] 2023-04-07 13:42:06 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:09 - Epoch : 9 Train loss: -0.5952985313086383 Val loss: -0.5730859716733296 Train ndcg_10 0.5915169715881348 Val ndcg_10 0.472622811794281\n",
            "[INFO] 2023-04-07 13:42:09 - Current:0.472622811794281 Best:0.47056037187576294\n",
            "[INFO] 2023-04-07 13:42:09 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:13 - Epoch : 10 Train loss: -0.5865393662523027 Val loss: -0.5746894001960754 Train ndcg_10 0.5655534863471985 Val ndcg_10 0.4738158881664276\n",
            "[INFO] 2023-04-07 13:42:13 - Current:0.4738158881664276 Best:0.472622811794281\n",
            "[INFO] 2023-04-07 13:42:13 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:17 - Epoch : 11 Train loss: -0.5919619269427296 Val loss: -0.5762191931406657 Train ndcg_10 0.604658842086792 Val ndcg_10 0.47463902831077576\n",
            "[INFO] 2023-04-07 13:42:17 - Current:0.47463902831077576 Best:0.4738158881664276\n",
            "[INFO] 2023-04-07 13:42:17 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:20 - Epoch : 12 Train loss: -0.5975104542256808 Val loss: -0.5777700821558635 Train ndcg_10 0.5989060997962952 Val ndcg_10 0.4762500524520874\n",
            "[INFO] 2023-04-07 13:42:20 - Current:0.4762500524520874 Best:0.47463902831077576\n",
            "[INFO] 2023-04-07 13:42:20 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:24 - Epoch : 13 Train loss: -0.5880094309472053 Val loss: -0.5794535199801127 Train ndcg_10 0.5994648337364197 Val ndcg_10 0.48126220703125\n",
            "[INFO] 2023-04-07 13:42:24 - Current:0.48126220703125 Best:0.4762500524520874\n",
            "[INFO] 2023-04-07 13:42:24 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:42:28 - Epoch : 14 Train loss: -0.5991054539483557 Val loss: -0.5811238606770833 Train ndcg_10 0.6177498698234558 Val ndcg_10 0.48534801602363586\n",
            "[INFO] 2023-04-07 13:42:28 - Current:0.48534801602363586 Best:0.48126220703125\n",
            "[INFO] 2023-04-07 13:42:28 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:31 - Epoch : 15 Train loss: -0.6058771793117917 Val loss: -0.5814302643140157 Train ndcg_10 0.5995543599128723 Val ndcg_10 0.484140008687973\n",
            "[INFO] 2023-04-07 13:42:31 - Current:0.484140008687973 Best:0.48534801602363586\n",
            "[INFO] 2023-04-07 13:42:31 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:35 - Epoch : 16 Train loss: -0.5980531235711765 Val loss: -0.5817759156227111 Train ndcg_10 0.6016133427619934 Val ndcg_10 0.4849257469177246\n",
            "[INFO] 2023-04-07 13:42:35 - Current:0.4849257469177246 Best:0.48534801602363586\n",
            "[INFO] 2023-04-07 13:42:35 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:38 - Epoch : 17 Train loss: -0.6008351392450586 Val loss: -0.5821198105812073 Train ndcg_10 0.5963093638420105 Val ndcg_10 0.4861660897731781\n",
            "[INFO] 2023-04-07 13:42:38 - Current:0.4861660897731781 Best:0.48534801602363586\n",
            "[INFO] 2023-04-07 13:42:38 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:43 - Epoch : 18 Train loss: -0.6023550497747101 Val loss: -0.5824439684549968 Train ndcg_10 0.5864704251289368 Val ndcg_10 0.4865225553512573\n",
            "[INFO] 2023-04-07 13:42:43 - Current:0.4865225553512573 Best:0.4861660897731781\n",
            "[INFO] 2023-04-07 13:42:43 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:46 - Epoch : 19 Train loss: -0.6025183840838857 Val loss: -0.5827457706133524 Train ndcg_10 0.5984709858894348 Val ndcg_10 0.48712003231048584\n",
            "[INFO] 2023-04-07 13:42:46 - Current:0.48712003231048584 Best:0.4865225553512573\n",
            "[INFO] 2023-04-07 13:42:46 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:49 - Epoch : 20 Train loss: -0.595588299904601 Val loss: -0.5830672144889831 Train ndcg_10 0.6061923503875732 Val ndcg_10 0.4875618517398834\n",
            "[INFO] 2023-04-07 13:42:49 - Current:0.4875618517398834 Best:0.48712003231048584\n",
            "[INFO] 2023-04-07 13:42:49 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:53 - Epoch : 21 Train loss: -0.5975138919543376 Val loss: -0.5834346135457357 Train ndcg_10 0.6058555245399475 Val ndcg_10 0.4899342954158783\n",
            "[INFO] 2023-04-07 13:42:53 - Current:0.4899342954158783 Best:0.4875618517398834\n",
            "[INFO] 2023-04-07 13:42:53 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:42:58 - Epoch : 22 Train loss: -0.5996899548533392 Val loss: -0.583777646223704 Train ndcg_10 0.6030208468437195 Val ndcg_10 0.4901081621646881\n",
            "[INFO] 2023-04-07 13:42:58 - Current:0.4901081621646881 Best:0.4899342954158783\n",
            "[INFO] 2023-04-07 13:42:58 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:43:01 - Epoch : 23 Train loss: -0.5914284734599358 Val loss: -0.5840954184532166 Train ndcg_10 0.5969664454460144 Val ndcg_10 0.4902438819408417\n",
            "[INFO] 2023-04-07 13:43:01 - Current:0.4902438819408417 Best:0.4901081621646881\n",
            "[INFO] 2023-04-07 13:43:01 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:43:04 - Epoch : 24 Train loss: -0.6054799927722739 Val loss: -0.5844483256340027 Train ndcg_10 0.5960062742233276 Val ndcg_10 0.4899897873401642\n",
            "[INFO] 2023-04-07 13:43:04 - Current:0.4899897873401642 Best:0.4902438819408417\n",
            "[INFO] 2023-04-07 13:43:04 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:43:08 - Epoch : 25 Train loss: -0.6055238477248358 Val loss: -0.5847701867421468 Train ndcg_10 0.5933095812797546 Val ndcg_10 0.48754873871803284\n",
            "[INFO] 2023-04-07 13:43:08 - Current:0.48754873871803284 Best:0.4902438819408417\n",
            "[INFO] 2023-04-07 13:43:08 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:43:12 - Epoch : 26 Train loss: -0.6108149669866646 Val loss: -0.5850751837094624 Train ndcg_10 0.5993344187736511 Val ndcg_10 0.48825401067733765\n",
            "[INFO] 2023-04-07 13:43:12 - Current:0.48825401067733765 Best:0.4902438819408417\n",
            "[INFO] 2023-04-07 13:43:12 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:43:15 - Epoch : 27 Train loss: -0.6105336476567924 Val loss: -0.5854048927625021 Train ndcg_10 0.5956634283065796 Val ndcg_10 0.48988762497901917\n",
            "[INFO] 2023-04-07 13:43:15 - Current:0.48988762497901917 Best:0.4902438819408417\n",
            "[INFO] 2023-04-07 13:43:15 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:43:19 - Epoch : 28 Train loss: -0.6008740981771531 Val loss: -0.5857182900110881 Train ndcg_10 0.6211689114570618 Val ndcg_10 0.4901352524757385\n",
            "[INFO] 2023-04-07 13:43:19 - Current:0.4901352524757385 Best:0.4902438819408417\n",
            "[INFO] 2023-04-07 13:43:19 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:43:23 - Epoch : 29 Train loss: -0.6088814331015303 Val loss: -0.58604390223821 Train ndcg_10 0.6010246872901917 Val ndcg_10 0.490443617105484\n",
            "[INFO] 2023-04-07 13:43:23 - Current:0.490443617105484 Best:0.4902438819408417\n",
            "[INFO] 2023-04-07 13:43:23 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:27 - Epoch : 30 Train loss: -0.6020750515932178 Val loss: -0.5861052950223287 Train ndcg_10 0.6045094728469849 Val ndcg_10 0.490443617105484\n",
            "[INFO] 2023-04-07 13:43:27 - Current:0.490443617105484 Best:0.490443617105484\n",
            "[INFO] 2023-04-07 13:43:27 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:30 - Epoch : 31 Train loss: -0.6052059800223967 Val loss: -0.5861650904019674 Train ndcg_10 0.6041435599327087 Val ndcg_10 0.490542471408844\n",
            "[INFO] 2023-04-07 13:43:30 - Current:0.490542471408844 Best:0.490443617105484\n",
            "[INFO] 2023-04-07 13:43:30 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:33 - Epoch : 32 Train loss: -0.6014768106747518 Val loss: -0.586232316493988 Train ndcg_10 0.6081370115280151 Val ndcg_10 0.490542471408844\n",
            "[INFO] 2023-04-07 13:43:33 - Current:0.490542471408844 Best:0.490542471408844\n",
            "[INFO] 2023-04-07 13:43:33 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:37 - Epoch : 33 Train loss: -0.6038328375788213 Val loss: -0.586299475034078 Train ndcg_10 0.6178586483001709 Val ndcg_10 0.4919857680797577\n",
            "[INFO] 2023-04-07 13:43:37 - Current:0.4919857680797577 Best:0.490542471408844\n",
            "[INFO] 2023-04-07 13:43:37 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:41 - Epoch : 34 Train loss: -0.603012507819848 Val loss: -0.5863608956336975 Train ndcg_10 0.6042520999908447 Val ndcg_10 0.49208396673202515\n",
            "[INFO] 2023-04-07 13:43:41 - Current:0.49208396673202515 Best:0.4919857680797577\n",
            "[INFO] 2023-04-07 13:43:41 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:45 - Epoch : 35 Train loss: -0.6077752464992107 Val loss: -0.5864226619402567 Train ndcg_10 0.6053385138511658 Val ndcg_10 0.49208396673202515\n",
            "[INFO] 2023-04-07 13:43:45 - Current:0.49208396673202515 Best:0.49208396673202515\n",
            "[INFO] 2023-04-07 13:43:45 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:48 - Epoch : 36 Train loss: -0.6012562875550757 Val loss: -0.5864824811617534 Train ndcg_10 0.6102877855300903 Val ndcg_10 0.49259597063064575\n",
            "[INFO] 2023-04-07 13:43:48 - Current:0.49259597063064575 Best:0.49208396673202515\n",
            "[INFO] 2023-04-07 13:43:48 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:52 - Epoch : 37 Train loss: -0.602116626272511 Val loss: -0.5865460157394409 Train ndcg_10 0.5976817011833191 Val ndcg_10 0.49259597063064575\n",
            "[INFO] 2023-04-07 13:43:52 - Current:0.49259597063064575 Best:0.49259597063064575\n",
            "[INFO] 2023-04-07 13:43:52 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:56 - Epoch : 38 Train loss: -0.6015926617085054 Val loss: -0.5866095900535584 Train ndcg_10 0.6064499616622925 Val ndcg_10 0.49259597063064575\n",
            "[INFO] 2023-04-07 13:43:56 - Current:0.49259597063064575 Best:0.49259597063064575\n",
            "[INFO] 2023-04-07 13:43:56 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:43:59 - Epoch : 39 Train loss: -0.59838060608304 Val loss: -0.5866809209187825 Train ndcg_10 0.6142460107803345 Val ndcg_10 0.49263498187065125\n",
            "[INFO] 2023-04-07 13:43:59 - Current:0.49263498187065125 Best:0.49259597063064575\n",
            "[INFO] 2023-04-07 13:43:59 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:44:02 - Epoch : 40 Train loss: -0.5990527938600838 Val loss: -0.586742643515269 Train ndcg_10 0.5915342569351196 Val ndcg_10 0.49263498187065125\n",
            "[INFO] 2023-04-07 13:44:02 - Current:0.49263498187065125 Best:0.49263498187065125\n",
            "[INFO] 2023-04-07 13:44:02 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:44:07 - Epoch : 41 Train loss: -0.6044338016735066 Val loss: -0.5868077397346496 Train ndcg_10 0.6085091829299927 Val ndcg_10 0.49263498187065125\n",
            "[INFO] 2023-04-07 13:44:07 - Current:0.49263498187065125 Best:0.49263498187065125\n",
            "[INFO] 2023-04-07 13:44:07 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:44:10 - Epoch : 42 Train loss: -0.6077755779291676 Val loss: -0.5868686874707539 Train ndcg_10 0.5960756540298462 Val ndcg_10 0.49263498187065125\n",
            "[INFO] 2023-04-07 13:44:10 - Current:0.49263498187065125 Best:0.49263498187065125\n",
            "[INFO] 2023-04-07 13:44:10 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:44:14 - Epoch : 43 Train loss: -0.5999713290405836 Val loss: -0.5869317173957824 Train ndcg_10 0.6136384010314941 Val ndcg_10 0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:14 - Current:0.49278390407562256 Best:0.49263498187065125\n",
            "[INFO] 2023-04-07 13:44:14 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:44:17 - Epoch : 44 Train loss: -0.6089550165300173 Val loss: -0.5869940320650736 Train ndcg_10 0.6301717758178711 Val ndcg_10 0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:17 - Current:0.49278390407562256 Best:0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:17 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:44:22 - Epoch : 45 Train loss: -0.6048006016244579 Val loss: -0.5870068589846293 Train ndcg_10 0.5937832593917847 Val ndcg_10 0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:22 - Current:0.49278390407562256 Best:0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:22 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:44:25 - Epoch : 46 Train loss: -0.5999656353024958 Val loss: -0.5870203216870625 Train ndcg_10 0.6096402406692505 Val ndcg_10 0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:25 - Current:0.49278390407562256 Best:0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:25 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:44:28 - Epoch : 47 Train loss: -0.6094055237319969 Val loss: -0.5870326042175293 Train ndcg_10 0.6164095997810364 Val ndcg_10 0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:28 - Current:0.49278390407562256 Best:0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:28 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:44:32 - Epoch : 48 Train loss: -0.6056103987679721 Val loss: -0.5870445768038431 Train ndcg_10 0.6168094277381897 Val ndcg_10 0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:32 - Current:0.49278390407562256 Best:0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:32 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:44:36 - Epoch : 49 Train loss: -0.6003807751478347 Val loss: -0.5870571613311768 Train ndcg_10 0.5909969806671143 Val ndcg_10 0.49278390407562256\n",
            "[INFO] 2023-04-07 13:44:36 - Current:0.49278390407562256 Best:0.49278390407562256\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:44:38 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold2-listMLE', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold2-listMLE', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:44:38 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold2/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='listMLE', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:44:38 - will execute cp config.json output/results/MQ2008-Fold2-listMLE/used_config.json\n",
            "[INFO] 2023-04-07 13:44:38 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:44:38 - will load train data from MQ2008/Fold2/train.txt\n",
            "[INFO] 2023-04-07 13:44:38 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/train.txt'> and got x shape (7720, 46), y shape (7720,) and query_ids shape (7720,)\n",
            "[INFO] 2023-04-07 13:44:38 - loaded dataset with 354 queries\n",
            "[INFO] 2023-04-07 13:44:38 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:44:38 - train DS shape: [354, 121, 46]\n",
            "[INFO] 2023-04-07 13:44:38 - will load vali data from MQ2008/Fold2/vali.txt\n",
            "[INFO] 2023-04-07 13:44:38 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/vali.txt'> and got x shape (2095, 46), y shape (2095,) and query_ids shape (2095,)\n",
            "[INFO] 2023-04-07 13:44:38 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:44:38 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:44:38 - vali DS shape: [105, 119, 46]\n",
            "[INFO] 2023-04-07 13:44:38 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 13:44:38 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:44:38 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:44:38 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:44:38 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:44:41 - Epoch : 0 Train loss: 12.99582023943885 Val loss: 51.08830482846215 Train ndcg_10 0.5714678168296814 Val ndcg_10 0.4253655672073364\n",
            "[INFO] 2023-04-07 13:44:41 - Current:0.4253655672073364 Best:0.0\n",
            "[INFO] 2023-04-07 13:44:41 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:44:44 - Epoch : 1 Train loss: 12.983983896546444 Val loss: 51.0422126406715 Train ndcg_10 0.5633636116981506 Val ndcg_10 0.4243069291114807\n",
            "[INFO] 2023-04-07 13:44:44 - Current:0.4243069291114807 Best:0.4253655672073364\n",
            "[INFO] 2023-04-07 13:44:44 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:44:47 - Epoch : 2 Train loss: 12.954501965625138 Val loss: 51.007988775344124 Train ndcg_10 0.5753912925720215 Val ndcg_10 0.43164992332458496\n",
            "[INFO] 2023-04-07 13:44:47 - Current:0.43164992332458496 Best:0.4253655672073364\n",
            "[INFO] 2023-04-07 13:44:47 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:44:50 - Epoch : 3 Train loss: 12.929520256775247 Val loss: 50.942710222516745 Train ndcg_10 0.5774336457252502 Val ndcg_10 0.4413643479347229\n",
            "[INFO] 2023-04-07 13:44:50 - Current:0.4413643479347229 Best:0.43164992332458496\n",
            "[INFO] 2023-04-07 13:44:50 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:44:53 - Epoch : 4 Train loss: 12.937551498413086 Val loss: 50.94905290149507 Train ndcg_10 0.583814263343811 Val ndcg_10 0.4509192407131195\n",
            "[INFO] 2023-04-07 13:44:53 - Current:0.4509192407131195 Best:0.4413643479347229\n",
            "[INFO] 2023-04-07 13:44:53 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:44:55 - Epoch : 5 Train loss: 12.902680682597188 Val loss: 50.921868533179875 Train ndcg_10 0.5919462442398071 Val ndcg_10 0.45932847261428833\n",
            "[INFO] 2023-04-07 13:44:55 - Current:0.45932847261428833 Best:0.4509192407131195\n",
            "[INFO] 2023-04-07 13:44:55 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:44:58 - Epoch : 6 Train loss: 12.931638922395006 Val loss: 50.87447240920294 Train ndcg_10 0.6029221415519714 Val ndcg_10 0.463326096534729\n",
            "[INFO] 2023-04-07 13:44:58 - Current:0.463326096534729 Best:0.45932847261428833\n",
            "[INFO] 2023-04-07 13:44:58 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:02 - Epoch : 7 Train loss: 12.906446500013104 Val loss: 50.84793519519624 Train ndcg_10 0.6040163040161133 Val ndcg_10 0.46837854385375977\n",
            "[INFO] 2023-04-07 13:45:02 - Current:0.46837854385375977 Best:0.463326096534729\n",
            "[INFO] 2023-04-07 13:45:02 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:05 - Epoch : 8 Train loss: 12.884126216004796 Val loss: 50.74952516101656 Train ndcg_10 0.607703685760498 Val ndcg_10 0.4723489582538605\n",
            "[INFO] 2023-04-07 13:45:05 - Current:0.4723489582538605 Best:0.46837854385375977\n",
            "[INFO] 2023-04-07 13:45:05 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:07 - Epoch : 9 Train loss: 12.85113315258996 Val loss: 50.840141841343474 Train ndcg_10 0.6144655346870422 Val ndcg_10 0.4834246039390564\n",
            "[INFO] 2023-04-07 13:45:07 - Current:0.4834246039390564 Best:0.4723489582538605\n",
            "[INFO] 2023-04-07 13:45:07 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:10 - Epoch : 10 Train loss: 12.899586370435811 Val loss: 50.73877908615839 Train ndcg_10 0.6071254014968872 Val ndcg_10 0.4841628968715668\n",
            "[INFO] 2023-04-07 13:45:10 - Current:0.4841628968715668 Best:0.4834246039390564\n",
            "[INFO] 2023-04-07 13:45:10 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:13 - Epoch : 11 Train loss: 12.8945250753629 Val loss: 50.776872943696524 Train ndcg_10 0.6071142554283142 Val ndcg_10 0.48868462443351746\n",
            "[INFO] 2023-04-07 13:45:13 - Current:0.48868462443351746 Best:0.4841628968715668\n",
            "[INFO] 2023-04-07 13:45:13 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:17 - Epoch : 12 Train loss: 12.8295344444318 Val loss: 50.741434333437965 Train ndcg_10 0.6278670430183411 Val ndcg_10 0.49354618787765503\n",
            "[INFO] 2023-04-07 13:45:17 - Current:0.49354618787765503 Best:0.48868462443351746\n",
            "[INFO] 2023-04-07 13:45:17 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:19 - Epoch : 13 Train loss: 12.84158992767334 Val loss: 50.689342280796595 Train ndcg_10 0.6241007447242737 Val ndcg_10 0.5015185475349426\n",
            "[INFO] 2023-04-07 13:45:19 - Current:0.5015185475349426 Best:0.49354618787765503\n",
            "[INFO] 2023-04-07 13:45:19 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:45:22 - Epoch : 14 Train loss: 12.845106361949512 Val loss: 50.6962524777367 Train ndcg_10 0.6341496109962463 Val ndcg_10 0.5067039728164673\n",
            "[INFO] 2023-04-07 13:45:22 - Current:0.5067039728164673 Best:0.5015185475349426\n",
            "[INFO] 2023-04-07 13:45:22 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:25 - Epoch : 15 Train loss: 12.842278345830023 Val loss: 50.73916502453032 Train ndcg_10 0.6328079104423523 Val ndcg_10 0.5078831911087036\n",
            "[INFO] 2023-04-07 13:45:25 - Current:0.5078831911087036 Best:0.5067039728164673\n",
            "[INFO] 2023-04-07 13:45:25 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:28 - Epoch : 16 Train loss: 12.855128207449185 Val loss: 50.68922714960007 Train ndcg_10 0.6319964528083801 Val ndcg_10 0.5082749724388123\n",
            "[INFO] 2023-04-07 13:45:28 - Current:0.5082749724388123 Best:0.5078831911087036\n",
            "[INFO] 2023-04-07 13:45:28 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:31 - Epoch : 17 Train loss: 12.845263130920754 Val loss: 50.687792460123696 Train ndcg_10 0.6531877517700195 Val ndcg_10 0.5111003518104553\n",
            "[INFO] 2023-04-07 13:45:31 - Current:0.5111003518104553 Best:0.5082749724388123\n",
            "[INFO] 2023-04-07 13:45:31 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:34 - Epoch : 18 Train loss: 12.820128403141 Val loss: 50.70707782563709 Train ndcg_10 0.6586374044418335 Val ndcg_10 0.5113277435302734\n",
            "[INFO] 2023-04-07 13:45:34 - Current:0.5113277435302734 Best:0.5111003518104553\n",
            "[INFO] 2023-04-07 13:45:34 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:36 - Epoch : 19 Train loss: 12.838532167639435 Val loss: 50.71785925002325 Train ndcg_10 0.6204202175140381 Val ndcg_10 0.5109466314315796\n",
            "[INFO] 2023-04-07 13:45:36 - Current:0.5109466314315796 Best:0.5113277435302734\n",
            "[INFO] 2023-04-07 13:45:36 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:39 - Epoch : 20 Train loss: 12.82105838107524 Val loss: 50.71045601254418 Train ndcg_10 0.6276373863220215 Val ndcg_10 0.5096129775047302\n",
            "[INFO] 2023-04-07 13:45:39 - Current:0.5096129775047302 Best:0.5113277435302734\n",
            "[INFO] 2023-04-07 13:45:39 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:42 - Epoch : 21 Train loss: 12.809552122644112 Val loss: 50.673357427687876 Train ndcg_10 0.6365411877632141 Val ndcg_10 0.5117676258087158\n",
            "[INFO] 2023-04-07 13:45:42 - Current:0.5117676258087158 Best:0.5113277435302734\n",
            "[INFO] 2023-04-07 13:45:42 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:45 - Epoch : 22 Train loss: 12.834266732641533 Val loss: 50.69042067754836 Train ndcg_10 0.6444331407546997 Val ndcg_10 0.5131129026412964\n",
            "[INFO] 2023-04-07 13:45:45 - Current:0.5131129026412964 Best:0.5117676258087158\n",
            "[INFO] 2023-04-07 13:45:45 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:48 - Epoch : 23 Train loss: 12.809060414632162 Val loss: 50.686888558524 Train ndcg_10 0.6327225565910339 Val ndcg_10 0.5156605839729309\n",
            "[INFO] 2023-04-07 13:45:48 - Current:0.5156605839729309 Best:0.5131129026412964\n",
            "[INFO] 2023-04-07 13:45:48 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:51 - Epoch : 24 Train loss: 12.810607705412611 Val loss: 50.73878359113421 Train ndcg_10 0.6349241733551025 Val ndcg_10 0.5122201442718506\n",
            "[INFO] 2023-04-07 13:45:51 - Current:0.5122201442718506 Best:0.5156605839729309\n",
            "[INFO] 2023-04-07 13:45:51 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:54 - Epoch : 25 Train loss: 12.838542097705906 Val loss: 50.637545485723585 Train ndcg_10 0.6416330933570862 Val ndcg_10 0.5135670304298401\n",
            "[INFO] 2023-04-07 13:45:54 - Current:0.5135670304298401 Best:0.5156605839729309\n",
            "[INFO] 2023-04-07 13:45:54 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:45:58 - Epoch : 26 Train loss: 12.834478593815518 Val loss: 50.674375842866446 Train ndcg_10 0.6324399709701538 Val ndcg_10 0.5162533521652222\n",
            "[INFO] 2023-04-07 13:45:58 - Current:0.5162533521652222 Best:0.5156605839729309\n",
            "[INFO] 2023-04-07 13:45:58 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:46:00 - Epoch : 27 Train loss: 12.792951271358856 Val loss: 50.639696248372395 Train ndcg_10 0.6407267451286316 Val ndcg_10 0.5177546739578247\n",
            "[INFO] 2023-04-07 13:46:00 - Current:0.5177546739578247 Best:0.5162533521652222\n",
            "[INFO] 2023-04-07 13:46:00 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:46:03 - Epoch : 28 Train loss: 12.833539639489125 Val loss: 50.7084224155971 Train ndcg_10 0.6393757462501526 Val ndcg_10 0.5181365013122559\n",
            "[INFO] 2023-04-07 13:46:03 - Current:0.5181365013122559 Best:0.5177546739578247\n",
            "[INFO] 2023-04-07 13:46:03 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:46:06 - Epoch : 29 Train loss: 12.839801421946724 Val loss: 50.6543704078311 Train ndcg_10 0.6386488080024719 Val ndcg_10 0.5197075009346008\n",
            "[INFO] 2023-04-07 13:46:06 - Current:0.5197075009346008 Best:0.5181365013122559\n",
            "[INFO] 2023-04-07 13:46:06 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:09 - Epoch : 30 Train loss: 12.819868518807795 Val loss: 50.705044991629464 Train ndcg_10 0.6359527111053467 Val ndcg_10 0.5197075009346008\n",
            "[INFO] 2023-04-07 13:46:09 - Current:0.5197075009346008 Best:0.5197075009346008\n",
            "[INFO] 2023-04-07 13:46:09 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:12 - Epoch : 31 Train loss: 12.80775713516494 Val loss: 50.66519818987165 Train ndcg_10 0.6286978721618652 Val ndcg_10 0.519972026348114\n",
            "[INFO] 2023-04-07 13:46:12 - Current:0.519972026348114 Best:0.5197075009346008\n",
            "[INFO] 2023-04-07 13:46:12 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:15 - Epoch : 32 Train loss: 12.827582569445594 Val loss: 50.68966878255208 Train ndcg_10 0.6447604298591614 Val ndcg_10 0.519972026348114\n",
            "[INFO] 2023-04-07 13:46:15 - Current:0.519972026348114 Best:0.519972026348114\n",
            "[INFO] 2023-04-07 13:46:15 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:17 - Epoch : 33 Train loss: 12.79898208144021 Val loss: 50.65123581659226 Train ndcg_10 0.6258318424224854 Val ndcg_10 0.519972026348114\n",
            "[INFO] 2023-04-07 13:46:17 - Current:0.519972026348114 Best:0.519972026348114\n",
            "[INFO] 2023-04-07 13:46:17 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:20 - Epoch : 34 Train loss: 12.78021151332532 Val loss: 50.70441236950102 Train ndcg_10 0.641683042049408 Val ndcg_10 0.5200043320655823\n",
            "[INFO] 2023-04-07 13:46:20 - Current:0.5200043320655823 Best:0.519972026348114\n",
            "[INFO] 2023-04-07 13:46:20 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:24 - Epoch : 35 Train loss: 12.85437067753851 Val loss: 50.703927575974234 Train ndcg_10 0.6209866404533386 Val ndcg_10 0.52273029088974\n",
            "[INFO] 2023-04-07 13:46:24 - Current:0.52273029088974 Best:0.5200043320655823\n",
            "[INFO] 2023-04-07 13:46:24 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:26 - Epoch : 36 Train loss: 12.845473488845395 Val loss: 50.69436385745094 Train ndcg_10 0.6321057081222534 Val ndcg_10 0.52273029088974\n",
            "[INFO] 2023-04-07 13:46:26 - Current:0.52273029088974 Best:0.52273029088974\n",
            "[INFO] 2023-04-07 13:46:26 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:29 - Epoch : 37 Train loss: 12.82152260494771 Val loss: 50.685644058954146 Train ndcg_10 0.64444899559021 Val ndcg_10 0.5239918828010559\n",
            "[INFO] 2023-04-07 13:46:29 - Current:0.5239918828010559 Best:0.52273029088974\n",
            "[INFO] 2023-04-07 13:46:29 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:32 - Epoch : 38 Train loss: 12.81319158629509 Val loss: 50.627335720970514 Train ndcg_10 0.6534098386764526 Val ndcg_10 0.5244094729423523\n",
            "[INFO] 2023-04-07 13:46:32 - Current:0.5244094729423523 Best:0.5239918828010559\n",
            "[INFO] 2023-04-07 13:46:32 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:34 - Epoch : 39 Train loss: 12.815957500436213 Val loss: 50.67221265520368 Train ndcg_10 0.6451098322868347 Val ndcg_10 0.5253515243530273\n",
            "[INFO] 2023-04-07 13:46:34 - Current:0.5253515243530273 Best:0.5244094729423523\n",
            "[INFO] 2023-04-07 13:46:34 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:38 - Epoch : 40 Train loss: 12.815722255383507 Val loss: 50.649942270914714 Train ndcg_10 0.6342889070510864 Val ndcg_10 0.526386022567749\n",
            "[INFO] 2023-04-07 13:46:38 - Current:0.526386022567749 Best:0.5253515243530273\n",
            "[INFO] 2023-04-07 13:46:38 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:41 - Epoch : 41 Train loss: 12.846021727653547 Val loss: 50.678656986781526 Train ndcg_10 0.6370697021484375 Val ndcg_10 0.526386022567749\n",
            "[INFO] 2023-04-07 13:46:41 - Current:0.526386022567749 Best:0.526386022567749\n",
            "[INFO] 2023-04-07 13:46:41 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:44 - Epoch : 42 Train loss: 12.835545943955244 Val loss: 50.65262422107515 Train ndcg_10 0.6417794823646545 Val ndcg_10 0.5263121128082275\n",
            "[INFO] 2023-04-07 13:46:44 - Current:0.5263121128082275 Best:0.526386022567749\n",
            "[INFO] 2023-04-07 13:46:44 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:46 - Epoch : 43 Train loss: 12.796856734712245 Val loss: 50.698080589657735 Train ndcg_10 0.6431617736816406 Val ndcg_10 0.5285969972610474\n",
            "[INFO] 2023-04-07 13:46:46 - Current:0.5285969972610474 Best:0.526386022567749\n",
            "[INFO] 2023-04-07 13:46:46 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:46:49 - Epoch : 44 Train loss: 12.842128742886128 Val loss: 50.67575818016415 Train ndcg_10 0.6327599883079529 Val ndcg_10 0.5299033522605896\n",
            "[INFO] 2023-04-07 13:46:49 - Current:0.5299033522605896 Best:0.5285969972610474\n",
            "[INFO] 2023-04-07 13:46:49 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:46:53 - Epoch : 45 Train loss: 12.82171974613168 Val loss: 50.74872767130534 Train ndcg_10 0.6424928903579712 Val ndcg_10 0.5299033522605896\n",
            "[INFO] 2023-04-07 13:46:53 - Current:0.5299033522605896 Best:0.5299033522605896\n",
            "[INFO] 2023-04-07 13:46:53 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:46:55 - Epoch : 46 Train loss: 12.805797754707983 Val loss: 50.684944225492934 Train ndcg_10 0.6499353051185608 Val ndcg_10 0.5299033522605896\n",
            "[INFO] 2023-04-07 13:46:55 - Current:0.5299033522605896 Best:0.5299033522605896\n",
            "[INFO] 2023-04-07 13:46:55 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:46:58 - Epoch : 47 Train loss: 12.804322108037054 Val loss: 50.677565565563384 Train ndcg_10 0.6436086893081665 Val ndcg_10 0.5299033522605896\n",
            "[INFO] 2023-04-07 13:46:58 - Current:0.5299033522605896 Best:0.5299033522605896\n",
            "[INFO] 2023-04-07 13:46:58 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:47:01 - Epoch : 48 Train loss: 12.840180036038328 Val loss: 50.672474270775204 Train ndcg_10 0.6385724544525146 Val ndcg_10 0.5299033522605896\n",
            "[INFO] 2023-04-07 13:47:01 - Current:0.5299033522605896 Best:0.5299033522605896\n",
            "[INFO] 2023-04-07 13:47:01 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:47:04 - Epoch : 49 Train loss: 12.816066370172015 Val loss: 50.713449532645086 Train ndcg_10 0.6326921582221985 Val ndcg_10 0.5299033522605896\n",
            "[INFO] 2023-04-07 13:47:04 - Current:0.5299033522605896 Best:0.5299033522605896\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:47:06 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold2-ordinal', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold2-ordinal', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:47:06 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold2/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='ordinal', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:47:06 - will execute cp config.json output/results/MQ2008-Fold2-ordinal/used_config.json\n",
            "[INFO] 2023-04-07 13:47:06 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:47:06 - will load train data from MQ2008/Fold2/train.txt\n",
            "[INFO] 2023-04-07 13:47:07 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/train.txt'> and got x shape (7720, 46), y shape (7720,) and query_ids shape (7720,)\n",
            "[INFO] 2023-04-07 13:47:07 - loaded dataset with 354 queries\n",
            "[INFO] 2023-04-07 13:47:07 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:47:07 - train DS shape: [354, 121, 46]\n",
            "[INFO] 2023-04-07 13:47:07 - will load vali data from MQ2008/Fold2/vali.txt\n",
            "[INFO] 2023-04-07 13:47:07 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/vali.txt'> and got x shape (2095, 46), y shape (2095,) and query_ids shape (2095,)\n",
            "[INFO] 2023-04-07 13:47:07 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:47:07 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:47:07 - vali DS shape: [105, 119, 46]\n",
            "[INFO] 2023-04-07 13:47:07 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 13:47:07 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:47:07 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:47:07 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:47:07 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: ordinal() missing 1 required positional argument: 'n'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:47:09 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold2-lambdaLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold2-lambdaLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:47:09 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold2/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='lambdaLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:47:09 - will execute cp config.json output/results/MQ2008-Fold2-lambdaLoss/used_config.json\n",
            "[INFO] 2023-04-07 13:47:09 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:47:09 - will load train data from MQ2008/Fold2/train.txt\n",
            "[INFO] 2023-04-07 13:47:09 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/train.txt'> and got x shape (7720, 46), y shape (7720,) and query_ids shape (7720,)\n",
            "[INFO] 2023-04-07 13:47:09 - loaded dataset with 354 queries\n",
            "[INFO] 2023-04-07 13:47:09 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:47:09 - train DS shape: [354, 121, 46]\n",
            "[INFO] 2023-04-07 13:47:09 - will load vali data from MQ2008/Fold2/vali.txt\n",
            "[INFO] 2023-04-07 13:47:09 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/vali.txt'> and got x shape (2095, 46), y shape (2095,) and query_ids shape (2095,)\n",
            "[INFO] 2023-04-07 13:47:09 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:47:09 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:47:09 - vali DS shape: [105, 119, 46]\n",
            "[INFO] 2023-04-07 13:47:09 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 13:47:09 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:47:09 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:47:09 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:47:09 - Current learning rate: 0.001\n",
            "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in Log2Backward. Traceback of forward call that caused the error:\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/models/losses/lambdaLoss.py\", line 70, in lambdaLoss\n",
            "losses = torch.log2(weighted_probas)\n",
            "(Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
            "Variable._execution_engine.run_backward(\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 23, in loss_batch\n",
            "loss.backward()\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/tensor.py\", line 245, in backward\n",
            "torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 145, in backward\n",
            "Variable._execution_engine.run_backward(\n",
            "RuntimeError: Function 'Log2Backward' returned nan values in its 0th output.\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:47:11 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold2-approxNDCGLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:47:11 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold2/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='approxNDCGLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:47:11 - will execute cp config.json output/results/MQ2008-Fold2-approxNDCGLoss/used_config.json\n",
            "[INFO] 2023-04-07 13:47:11 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:47:11 - will load train data from MQ2008/Fold2/train.txt\n",
            "[INFO] 2023-04-07 13:47:12 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/train.txt'> and got x shape (7720, 46), y shape (7720,) and query_ids shape (7720,)\n",
            "[INFO] 2023-04-07 13:47:12 - loaded dataset with 354 queries\n",
            "[INFO] 2023-04-07 13:47:12 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:47:12 - train DS shape: [354, 121, 46]\n",
            "[INFO] 2023-04-07 13:47:12 - will load vali data from MQ2008/Fold2/vali.txt\n",
            "[INFO] 2023-04-07 13:47:12 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/vali.txt'> and got x shape (2095, 46), y shape (2095,) and query_ids shape (2095,)\n",
            "[INFO] 2023-04-07 13:47:12 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:47:12 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:47:12 - vali DS shape: [105, 119, 46]\n",
            "[INFO] 2023-04-07 13:47:12 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 13:47:12 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:47:12 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:47:12 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:47:12 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:14 - Epoch : 0 Train loss: -0.48713944069409776 Val loss: -0.4747605093887874 Train ndcg_10 0.5705915093421936 Val ndcg_10 0.42618799209594727\n",
            "[INFO] 2023-04-07 13:47:14 - Current:0.42618799209594727 Best:0.0\n",
            "[INFO] 2023-04-07 13:47:14 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:17 - Epoch : 1 Train loss: -0.4858679791628304 Val loss: -0.4747882808957781 Train ndcg_10 0.5683239698410034 Val ndcg_10 0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:17 - Current:0.4262009263038635 Best:0.42618799209594727\n",
            "[INFO] 2023-04-07 13:47:17 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:21 - Epoch : 2 Train loss: -0.48989567177443855 Val loss: -0.4748138461794172 Train ndcg_10 0.5709596872329712 Val ndcg_10 0.4231622517108917\n",
            "[INFO] 2023-04-07 13:47:21 - Current:0.4231622517108917 Best:0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:21 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:23 - Epoch : 3 Train loss: -0.49057820961300264 Val loss: -0.47483663388660974 Train ndcg_10 0.582347571849823 Val ndcg_10 0.42317694425582886\n",
            "[INFO] 2023-04-07 13:47:23 - Current:0.42317694425582886 Best:0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:23 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:26 - Epoch : 4 Train loss: -0.48936591411041 Val loss: -0.4748632896514166 Train ndcg_10 0.5687350630760193 Val ndcg_10 0.42382651567459106\n",
            "[INFO] 2023-04-07 13:47:26 - Current:0.42382651567459106 Best:0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:26 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:29 - Epoch : 5 Train loss: -0.4879037906221077 Val loss: -0.474892581928344 Train ndcg_10 0.5745777487754822 Val ndcg_10 0.4244116544723511\n",
            "[INFO] 2023-04-07 13:47:29 - Current:0.4244116544723511 Best:0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:29 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:32 - Epoch : 6 Train loss: -0.48666579305788893 Val loss: -0.4749188641707102 Train ndcg_10 0.5826054215431213 Val ndcg_10 0.4250175952911377\n",
            "[INFO] 2023-04-07 13:47:32 - Current:0.4250175952911377 Best:0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:32 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:35 - Epoch : 7 Train loss: -0.4900496016114445 Val loss: -0.4749443224498204 Train ndcg_10 0.5970407724380493 Val ndcg_10 0.42578673362731934\n",
            "[INFO] 2023-04-07 13:47:35 - Current:0.42578673362731934 Best:0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:35 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:38 - Epoch : 8 Train loss: -0.4909124903086215 Val loss: -0.47497081245694844 Train ndcg_10 0.5688745379447937 Val ndcg_10 0.4262053966522217\n",
            "[INFO] 2023-04-07 13:47:38 - Current:0.4262053966522217 Best:0.4262009263038635\n",
            "[INFO] 2023-04-07 13:47:38 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:41 - Epoch : 9 Train loss: -0.48856315740757744 Val loss: -0.4750000740800585 Train ndcg_10 0.5677343010902405 Val ndcg_10 0.4262053966522217\n",
            "[INFO] 2023-04-07 13:47:41 - Current:0.4262053966522217 Best:0.4262053966522217\n",
            "[INFO] 2023-04-07 13:47:41 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:43 - Epoch : 10 Train loss: -0.4866641540985323 Val loss: -0.47502561608950294 Train ndcg_10 0.5895333290100098 Val ndcg_10 0.42662128806114197\n",
            "[INFO] 2023-04-07 13:47:43 - Current:0.42662128806114197 Best:0.4262053966522217\n",
            "[INFO] 2023-04-07 13:47:43 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:47 - Epoch : 11 Train loss: -0.4886971735684885 Val loss: -0.47505402110871814 Train ndcg_10 0.5781186819076538 Val ndcg_10 0.4270058870315552\n",
            "[INFO] 2023-04-07 13:47:47 - Current:0.4270058870315552 Best:0.42662128806114197\n",
            "[INFO] 2023-04-07 13:47:47 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:50 - Epoch : 12 Train loss: -0.49058383282295054 Val loss: -0.47507414647511076 Train ndcg_10 0.5797063708305359 Val ndcg_10 0.4270058870315552\n",
            "[INFO] 2023-04-07 13:47:50 - Current:0.4270058870315552 Best:0.4270058870315552\n",
            "[INFO] 2023-04-07 13:47:50 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:53 - Epoch : 13 Train loss: -0.48894179305114316 Val loss: -0.4751009478455498 Train ndcg_10 0.5594658255577087 Val ndcg_10 0.42800235748291016\n",
            "[INFO] 2023-04-07 13:47:53 - Current:0.42800235748291016 Best:0.4270058870315552\n",
            "[INFO] 2023-04-07 13:47:53 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:47:55 - Epoch : 14 Train loss: -0.48571022890381893 Val loss: -0.4751290122667948 Train ndcg_10 0.5716014504432678 Val ndcg_10 0.42821383476257324\n",
            "[INFO] 2023-04-07 13:47:55 - Current:0.42821383476257324 Best:0.42800235748291016\n",
            "[INFO] 2023-04-07 13:47:55 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:47:58 - Epoch : 15 Train loss: -0.4849696310900025 Val loss: -0.47513450071925206 Train ndcg_10 0.5725159645080566 Val ndcg_10 0.42821383476257324\n",
            "[INFO] 2023-04-07 13:47:58 - Current:0.42821383476257324 Best:0.42821383476257324\n",
            "[INFO] 2023-04-07 13:47:58 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:02 - Epoch : 16 Train loss: -0.48929392112850467 Val loss: -0.4751396287055243 Train ndcg_10 0.5741199254989624 Val ndcg_10 0.42821383476257324\n",
            "[INFO] 2023-04-07 13:48:02 - Current:0.42821383476257324 Best:0.42821383476257324\n",
            "[INFO] 2023-04-07 13:48:02 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:05 - Epoch : 17 Train loss: -0.4858220438499235 Val loss: -0.4751450274671827 Train ndcg_10 0.5734076499938965 Val ndcg_10 0.42821383476257324\n",
            "[INFO] 2023-04-07 13:48:05 - Current:0.42821383476257324 Best:0.42821383476257324\n",
            "[INFO] 2023-04-07 13:48:05 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:07 - Epoch : 18 Train loss: -0.4868965239848121 Val loss: -0.47515053550402325 Train ndcg_10 0.5649025440216064 Val ndcg_10 0.4283513128757477\n",
            "[INFO] 2023-04-07 13:48:07 - Current:0.4283513128757477 Best:0.42821383476257324\n",
            "[INFO] 2023-04-07 13:48:07 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:10 - Epoch : 19 Train loss: -0.48754160013576014 Val loss: -0.4751562328565688 Train ndcg_10 0.583629310131073 Val ndcg_10 0.4283513128757477\n",
            "[INFO] 2023-04-07 13:48:10 - Current:0.4283513128757477 Best:0.4283513128757477\n",
            "[INFO] 2023-04-07 13:48:10 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:13 - Epoch : 20 Train loss: -0.492301102411949 Val loss: -0.475161695196515 Train ndcg_10 0.5710300207138062 Val ndcg_10 0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:13 - Current:0.42836692929267883 Best:0.4283513128757477\n",
            "[INFO] 2023-04-07 13:48:13 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:16 - Epoch : 21 Train loss: -0.4885145388754074 Val loss: -0.475166582493555 Train ndcg_10 0.5701462030410767 Val ndcg_10 0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:16 - Current:0.42836692929267883 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:16 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:19 - Epoch : 22 Train loss: -0.48753396198574434 Val loss: -0.4751721603529794 Train ndcg_10 0.5761343836784363 Val ndcg_10 0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:19 - Current:0.42836692929267883 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:19 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:22 - Epoch : 23 Train loss: -0.4901745625808414 Val loss: -0.47517794115202766 Train ndcg_10 0.5734423995018005 Val ndcg_10 0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:22 - Current:0.42836692929267883 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:22 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:25 - Epoch : 24 Train loss: -0.48866818516941396 Val loss: -0.47518272059304373 Train ndcg_10 0.5754214525222778 Val ndcg_10 0.4271199405193329\n",
            "[INFO] 2023-04-07 13:48:25 - Current:0.4271199405193329 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:25 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:29 - Epoch : 25 Train loss: -0.48696351590129616 Val loss: -0.4751877886908395 Train ndcg_10 0.5698765516281128 Val ndcg_10 0.4271199405193329\n",
            "[INFO] 2023-04-07 13:48:29 - Current:0.4271199405193329 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:29 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:32 - Epoch : 26 Train loss: -0.4882653177121265 Val loss: -0.4751934494291033 Train ndcg_10 0.5808007121086121 Val ndcg_10 0.4271572232246399\n",
            "[INFO] 2023-04-07 13:48:32 - Current:0.4271572232246399 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:32 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:34 - Epoch : 27 Train loss: -0.4923951750421255 Val loss: -0.4751987650280907 Train ndcg_10 0.5903059840202332 Val ndcg_10 0.4272271394729614\n",
            "[INFO] 2023-04-07 13:48:34 - Current:0.4272271394729614 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:34 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:37 - Epoch : 28 Train loss: -0.4909230861286659 Val loss: -0.4752036594209217 Train ndcg_10 0.5876582264900208 Val ndcg_10 0.42673900723457336\n",
            "[INFO] 2023-04-07 13:48:37 - Current:0.42673900723457336 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:37 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:48:40 - Epoch : 29 Train loss: -0.4883239582433539 Val loss: -0.47520935223216104 Train ndcg_10 0.5744990110397339 Val ndcg_10 0.42673900723457336\n",
            "[INFO] 2023-04-07 13:48:40 - Current:0.42673900723457336 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:40 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:48:44 - Epoch : 30 Train loss: -0.48734587683516034 Val loss: -0.47521035330636163 Train ndcg_10 0.5645127892494202 Val ndcg_10 0.4269949197769165\n",
            "[INFO] 2023-04-07 13:48:44 - Current:0.4269949197769165 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:44 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:48:46 - Epoch : 31 Train loss: -0.48947113449290647 Val loss: -0.4752114332857586 Train ndcg_10 0.5677304267883301 Val ndcg_10 0.4269949197769165\n",
            "[INFO] 2023-04-07 13:48:46 - Current:0.4269949197769165 Best:0.42836692929267883\n",
            "[INFO] 2023-04-07 13:48:46 - early stopping at epoch 31 since ndcg_10 didn't improve from epoch no 20. Best value 0.42836692929267883, current value 0.4269949197769165\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:48:48 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold2-pointwise_rmse', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold2-pointwise_rmse', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:48:48 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold2/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='pointwise_rmse', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:48:48 - will execute cp config.json output/results/MQ2008-Fold2-pointwise_rmse/used_config.json\n",
            "[INFO] 2023-04-07 13:48:48 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:48:48 - will load train data from MQ2008/Fold2/train.txt\n",
            "[INFO] 2023-04-07 13:48:48 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/train.txt'> and got x shape (7720, 46), y shape (7720,) and query_ids shape (7720,)\n",
            "[INFO] 2023-04-07 13:48:48 - loaded dataset with 354 queries\n",
            "[INFO] 2023-04-07 13:48:48 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:48:48 - train DS shape: [354, 121, 46]\n",
            "[INFO] 2023-04-07 13:48:48 - will load vali data from MQ2008/Fold2/vali.txt\n",
            "[INFO] 2023-04-07 13:48:48 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/vali.txt'> and got x shape (2095, 46), y shape (2095,) and query_ids shape (2095,)\n",
            "[INFO] 2023-04-07 13:48:48 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:48:48 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:48:48 - vali DS shape: [105, 119, 46]\n",
            "[INFO] 2023-04-07 13:48:48 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 13:48:48 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:48:48 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:48:48 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:48:48 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: pointwise_rmse() missing 1 required positional argument: 'no_of_levels'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:48:51 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold2-neuralNDCG', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold2-neuralNDCG', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:48:51 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold2/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='neuralNDCG', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:48:51 - will execute cp config.json output/results/MQ2008-Fold2-neuralNDCG/used_config.json\n",
            "[INFO] 2023-04-07 13:48:51 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:48:51 - will load train data from MQ2008/Fold2/train.txt\n",
            "[INFO] 2023-04-07 13:48:51 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/train.txt'> and got x shape (7720, 46), y shape (7720,) and query_ids shape (7720,)\n",
            "[INFO] 2023-04-07 13:48:51 - loaded dataset with 354 queries\n",
            "[INFO] 2023-04-07 13:48:51 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:48:51 - train DS shape: [354, 121, 46]\n",
            "[INFO] 2023-04-07 13:48:51 - will load vali data from MQ2008/Fold2/vali.txt\n",
            "[INFO] 2023-04-07 13:48:51 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold2/vali.txt'> and got x shape (2095, 46), y shape (2095,) and query_ids shape (2095,)\n",
            "[INFO] 2023-04-07 13:48:51 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:48:51 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:48:51 - vali DS shape: [105, 119, 46]\n",
            "[INFO] 2023-04-07 13:48:51 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 13:48:51 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:48:51 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:48:51 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:48:51 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:48:55 - Epoch : 0 Train loss: -0.5818346664730438 Val loss: -0.5602246335574559 Train ndcg_10 0.573148250579834 Val ndcg_10 0.4251003563404083\n",
            "[INFO] 2023-04-07 13:48:55 - Current:0.4251003563404083 Best:0.0\n",
            "[INFO] 2023-04-07 13:48:55 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:48:59 - Epoch : 1 Train loss: -0.5812443440916848 Val loss: -0.5621665761584327 Train ndcg_10 0.5714280009269714 Val ndcg_10 0.42678266763687134\n",
            "[INFO] 2023-04-07 13:48:59 - Current:0.42678266763687134 Best:0.4251003563404083\n",
            "[INFO] 2023-04-07 13:48:59 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:02 - Epoch : 2 Train loss: -0.5913698653043327 Val loss: -0.5639335791269938 Train ndcg_10 0.5757130980491638 Val ndcg_10 0.4270157217979431\n",
            "[INFO] 2023-04-07 13:49:02 - Current:0.4270157217979431 Best:0.42678266763687134\n",
            "[INFO] 2023-04-07 13:49:02 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:05 - Epoch : 3 Train loss: -0.5940502814653903 Val loss: -0.5656576043083554 Train ndcg_10 0.5916863679885864 Val ndcg_10 0.4286506772041321\n",
            "[INFO] 2023-04-07 13:49:05 - Current:0.4286506772041321 Best:0.4270157217979431\n",
            "[INFO] 2023-04-07 13:49:05 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:10 - Epoch : 4 Train loss: -0.5912998678994044 Val loss: -0.5675507800919669 Train ndcg_10 0.5790558457374573 Val ndcg_10 0.43643805384635925\n",
            "[INFO] 2023-04-07 13:49:10 - Current:0.43643805384635925 Best:0.4286506772041321\n",
            "[INFO] 2023-04-07 13:49:10 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:13 - Epoch : 5 Train loss: -0.5904385784251541 Val loss: -0.5695660386766707 Train ndcg_10 0.5873030424118042 Val ndcg_10 0.43987512588500977\n",
            "[INFO] 2023-04-07 13:49:13 - Current:0.43987512588500977 Best:0.43643805384635925\n",
            "[INFO] 2023-04-07 13:49:13 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:16 - Epoch : 6 Train loss: -0.5887490396445754 Val loss: -0.5714350183804829 Train ndcg_10 0.5946769714355469 Val ndcg_10 0.44603484869003296\n",
            "[INFO] 2023-04-07 13:49:16 - Current:0.44603484869003296 Best:0.43987512588500977\n",
            "[INFO] 2023-04-07 13:49:16 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:19 - Epoch : 7 Train loss: -0.5940365555596216 Val loss: -0.5732703543844677 Train ndcg_10 0.611430287361145 Val ndcg_10 0.4482315182685852\n",
            "[INFO] 2023-04-07 13:49:19 - Current:0.4482315182685852 Best:0.44603484869003296\n",
            "[INFO] 2023-04-07 13:49:19 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:24 - Epoch : 8 Train loss: -0.5942930306418467 Val loss: -0.5751595411981855 Train ndcg_10 0.5860929489135742 Val ndcg_10 0.451113760471344\n",
            "[INFO] 2023-04-07 13:49:24 - Current:0.451113760471344 Best:0.4482315182685852\n",
            "[INFO] 2023-04-07 13:49:24 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:27 - Epoch : 9 Train loss: -0.5935229211203796 Val loss: -0.5772365098907835 Train ndcg_10 0.5870879292488098 Val ndcg_10 0.4564846158027649\n",
            "[INFO] 2023-04-07 13:49:27 - Current:0.4564846158027649 Best:0.451113760471344\n",
            "[INFO] 2023-04-07 13:49:27 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:31 - Epoch : 10 Train loss: -0.595290542659113 Val loss: -0.5791504644212269 Train ndcg_10 0.6113853454589844 Val ndcg_10 0.4612170159816742\n",
            "[INFO] 2023-04-07 13:49:31 - Current:0.4612170159816742 Best:0.4564846158027649\n",
            "[INFO] 2023-04-07 13:49:31 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:34 - Epoch : 11 Train loss: -0.5973573151954823 Val loss: -0.5811250936417353 Train ndcg_10 0.5988569855690002 Val ndcg_10 0.4629170298576355\n",
            "[INFO] 2023-04-07 13:49:34 - Current:0.4629170298576355 Best:0.4612170159816742\n",
            "[INFO] 2023-04-07 13:49:34 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:39 - Epoch : 12 Train loss: -0.6041136299822963 Val loss: -0.5825832962989808 Train ndcg_10 0.6027134656906128 Val ndcg_10 0.4711732268333435\n",
            "[INFO] 2023-04-07 13:49:39 - Current:0.4711732268333435 Best:0.4629170298576355\n",
            "[INFO] 2023-04-07 13:49:39 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:42 - Epoch : 13 Train loss: -0.6073570524231863 Val loss: -0.5844068425042289 Train ndcg_10 0.5861441493034363 Val ndcg_10 0.4750818908214569\n",
            "[INFO] 2023-04-07 13:49:42 - Current:0.4750818908214569 Best:0.4711732268333435\n",
            "[INFO] 2023-04-07 13:49:42 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:49:45 - Epoch : 14 Train loss: -0.5995798998296598 Val loss: -0.5862570325533549 Train ndcg_10 0.600692629814148 Val ndcg_10 0.48320460319519043\n",
            "[INFO] 2023-04-07 13:49:45 - Current:0.48320460319519043 Best:0.4750818908214569\n",
            "[INFO] 2023-04-07 13:49:45 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:49:49 - Epoch : 15 Train loss: -0.5982058246930441 Val loss: -0.5866301774978637 Train ndcg_10 0.6031125783920288 Val ndcg_10 0.4831053614616394\n",
            "[INFO] 2023-04-07 13:49:49 - Current:0.4831053614616394 Best:0.48320460319519043\n",
            "[INFO] 2023-04-07 13:49:49 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:49:53 - Epoch : 16 Train loss: -0.6060171174464253 Val loss: -0.5869800953637986 Train ndcg_10 0.5997225642204285 Val ndcg_10 0.4828053414821625\n",
            "[INFO] 2023-04-07 13:49:53 - Current:0.4828053414821625 Best:0.48320460319519043\n",
            "[INFO] 2023-04-07 13:49:53 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:49:56 - Epoch : 17 Train loss: -0.5979343060046266 Val loss: -0.5873500086012341 Train ndcg_10 0.6065988540649414 Val ndcg_10 0.4856199622154236\n",
            "[INFO] 2023-04-07 13:49:56 - Current:0.4856199622154236 Best:0.48320460319519043\n",
            "[INFO] 2023-04-07 13:49:56 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:00 - Epoch : 18 Train loss: -0.6003460170185498 Val loss: -0.5877166872932797 Train ndcg_10 0.5915504693984985 Val ndcg_10 0.49059078097343445\n",
            "[INFO] 2023-04-07 13:50:00 - Current:0.49059078097343445 Best:0.4856199622154236\n",
            "[INFO] 2023-04-07 13:50:00 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:04 - Epoch : 19 Train loss: -0.5992285673227687 Val loss: -0.5880941805385408 Train ndcg_10 0.6232706308364868 Val ndcg_10 0.4913347661495209\n",
            "[INFO] 2023-04-07 13:50:04 - Current:0.4913347661495209 Best:0.49059078097343445\n",
            "[INFO] 2023-04-07 13:50:04 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:08 - Epoch : 20 Train loss: -0.613923307024153 Val loss: -0.5884654408409482 Train ndcg_10 0.6025607585906982 Val ndcg_10 0.4918214976787567\n",
            "[INFO] 2023-04-07 13:50:08 - Current:0.4918214976787567 Best:0.4913347661495209\n",
            "[INFO] 2023-04-07 13:50:08 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:11 - Epoch : 21 Train loss: -0.6040901353130232 Val loss: -0.5887821634610494 Train ndcg_10 0.6083024740219116 Val ndcg_10 0.491772323846817\n",
            "[INFO] 2023-04-07 13:50:11 - Current:0.491772323846817 Best:0.4918214976787567\n",
            "[INFO] 2023-04-07 13:50:11 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:15 - Epoch : 22 Train loss: -0.6035114718695819 Val loss: -0.5891217663174584 Train ndcg_10 0.6056256890296936 Val ndcg_10 0.49239248037338257\n",
            "[INFO] 2023-04-07 13:50:15 - Current:0.49239248037338257 Best:0.4918214976787567\n",
            "[INFO] 2023-04-07 13:50:15 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:19 - Epoch : 23 Train loss: -0.6058651552361957 Val loss: -0.5895089631988889 Train ndcg_10 0.608963668346405 Val ndcg_10 0.49297264218330383\n",
            "[INFO] 2023-04-07 13:50:19 - Current:0.49297264218330383 Best:0.49239248037338257\n",
            "[INFO] 2023-04-07 13:50:19 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:23 - Epoch : 24 Train loss: -0.6061705226278574 Val loss: -0.5898280472982498 Train ndcg_10 0.605228066444397 Val ndcg_10 0.4933597445487976\n",
            "[INFO] 2023-04-07 13:50:23 - Current:0.4933597445487976 Best:0.49297264218330383\n",
            "[INFO] 2023-04-07 13:50:23 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:26 - Epoch : 25 Train loss: -0.5978207689220623 Val loss: -0.5901465529487246 Train ndcg_10 0.6074275374412537 Val ndcg_10 0.4940478503704071\n",
            "[INFO] 2023-04-07 13:50:26 - Current:0.4940478503704071 Best:0.4933597445487976\n",
            "[INFO] 2023-04-07 13:50:26 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:29 - Epoch : 26 Train loss: -0.6008601727458717 Val loss: -0.590519262495495 Train ndcg_10 0.6196036338806152 Val ndcg_10 0.4947494864463806\n",
            "[INFO] 2023-04-07 13:50:29 - Current:0.4947494864463806 Best:0.4940478503704071\n",
            "[INFO] 2023-04-07 13:50:29 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:34 - Epoch : 27 Train loss: -0.6134434874448399 Val loss: -0.5908688942591349 Train ndcg_10 0.6281416416168213 Val ndcg_10 0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:34 - Current:0.49484872817993164 Best:0.4947494864463806\n",
            "[INFO] 2023-04-07 13:50:34 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:37 - Epoch : 28 Train loss: -0.6097076127758134 Val loss: -0.5912083659853254 Train ndcg_10 0.6250869631767273 Val ndcg_10 0.49446621537208557\n",
            "[INFO] 2023-04-07 13:50:37 - Current:0.49446621537208557 Best:0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:37 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:50:41 - Epoch : 29 Train loss: -0.6052708582016034 Val loss: -0.5915833212080456 Train ndcg_10 0.6068151593208313 Val ndcg_10 0.4940493404865265\n",
            "[INFO] 2023-04-07 13:50:41 - Current:0.4940493404865265 Best:0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:41 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:50:44 - Epoch : 30 Train loss: -0.603966519320752 Val loss: -0.5916480087098621 Train ndcg_10 0.6015095114707947 Val ndcg_10 0.4940493404865265\n",
            "[INFO] 2023-04-07 13:50:44 - Current:0.4940493404865265 Best:0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:44 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:50:49 - Epoch : 31 Train loss: -0.6100724120598054 Val loss: -0.5917167731693813 Train ndcg_10 0.5994250774383545 Val ndcg_10 0.4942704439163208\n",
            "[INFO] 2023-04-07 13:50:49 - Current:0.4942704439163208 Best:0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:49 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:50:52 - Epoch : 32 Train loss: -0.6053214386358099 Val loss: -0.5917867319924491 Train ndcg_10 0.6029187440872192 Val ndcg_10 0.4944939613342285\n",
            "[INFO] 2023-04-07 13:50:52 - Current:0.4944939613342285 Best:0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:52 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:50:55 - Epoch : 33 Train loss: -0.6052294343204822 Val loss: -0.5918575644493103 Train ndcg_10 0.616333544254303 Val ndcg_10 0.4944939613342285\n",
            "[INFO] 2023-04-07 13:50:55 - Current:0.4944939613342285 Best:0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:55 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:50:59 - Epoch : 34 Train loss: -0.606383432776241 Val loss: -0.5919282078742981 Train ndcg_10 0.6186040043830872 Val ndcg_10 0.4956156313419342\n",
            "[INFO] 2023-04-07 13:50:59 - Current:0.4956156313419342 Best:0.49484872817993164\n",
            "[INFO] 2023-04-07 13:50:59 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:03 - Epoch : 35 Train loss: -0.6100821825070569 Val loss: -0.5919896557217552 Train ndcg_10 0.6050867438316345 Val ndcg_10 0.4956459105014801\n",
            "[INFO] 2023-04-07 13:51:03 - Current:0.4956459105014801 Best:0.4956156313419342\n",
            "[INFO] 2023-04-07 13:51:03 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:07 - Epoch : 36 Train loss: -0.6027389512223712 Val loss: -0.5920506761187598 Train ndcg_10 0.6044704914093018 Val ndcg_10 0.49571630358695984\n",
            "[INFO] 2023-04-07 13:51:07 - Current:0.49571630358695984 Best:0.4956459105014801\n",
            "[INFO] 2023-04-07 13:51:07 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:10 - Epoch : 37 Train loss: -0.6185438407342986 Val loss: -0.5921129062062218 Train ndcg_10 0.6043604016304016 Val ndcg_10 0.4965241849422455\n",
            "[INFO] 2023-04-07 13:51:10 - Current:0.4965241849422455 Best:0.49571630358695984\n",
            "[INFO] 2023-04-07 13:51:10 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:14 - Epoch : 38 Train loss: -0.6034953365218167 Val loss: -0.5921770129884992 Train ndcg_10 0.6079904437065125 Val ndcg_10 0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:14 - Current:0.4968150854110718 Best:0.4965241849422455\n",
            "[INFO] 2023-04-07 13:51:14 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:18 - Epoch : 39 Train loss: -0.6080515714688489 Val loss: -0.5922469564846584 Train ndcg_10 0.6181498765945435 Val ndcg_10 0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:18 - Current:0.4968150854110718 Best:0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:18 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:21 - Epoch : 40 Train loss: -0.6139416499326458 Val loss: -0.5923142047155471 Train ndcg_10 0.6142013072967529 Val ndcg_10 0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:21 - Current:0.4968150854110718 Best:0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:21 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:25 - Epoch : 41 Train loss: -0.6162766174408002 Val loss: -0.5923812128248669 Train ndcg_10 0.6207306981086731 Val ndcg_10 0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:25 - Current:0.4968150854110718 Best:0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:25 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:29 - Epoch : 42 Train loss: -0.6067587383049357 Val loss: -0.5924512267112731 Train ndcg_10 0.6103001236915588 Val ndcg_10 0.4982546269893646\n",
            "[INFO] 2023-04-07 13:51:29 - Current:0.4982546269893646 Best:0.4968150854110718\n",
            "[INFO] 2023-04-07 13:51:29 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:33 - Epoch : 43 Train loss: -0.6041570676248625 Val loss: -0.5925193201927912 Train ndcg_10 0.6058934926986694 Val ndcg_10 0.49854743480682373\n",
            "[INFO] 2023-04-07 13:51:33 - Current:0.49854743480682373 Best:0.4982546269893646\n",
            "[INFO] 2023-04-07 13:51:33 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:51:36 - Epoch : 44 Train loss: -0.603876493744931 Val loss: -0.5925882929847354 Train ndcg_10 0.6171332597732544 Val ndcg_10 0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:36 - Current:0.498759925365448 Best:0.49854743480682373\n",
            "[INFO] 2023-04-07 13:51:36 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:51:40 - Epoch : 45 Train loss: -0.6008316520917214 Val loss: -0.5926002042634146 Train ndcg_10 0.606983482837677 Val ndcg_10 0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:40 - Current:0.498759925365448 Best:0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:40 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:51:44 - Epoch : 46 Train loss: -0.5927572779062777 Val loss: -0.5926135908989679 Train ndcg_10 0.603877604007721 Val ndcg_10 0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:44 - Current:0.498759925365448 Best:0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:44 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:51:48 - Epoch : 47 Train loss: -0.606428659905148 Val loss: -0.5926261833735875 Train ndcg_10 0.6063171029090881 Val ndcg_10 0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:48 - Current:0.498759925365448 Best:0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:48 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:51:51 - Epoch : 48 Train loss: -0.6149237081829437 Val loss: -0.5926390136991229 Train ndcg_10 0.6213294863700867 Val ndcg_10 0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:51 - Current:0.498759925365448 Best:0.498759925365448\n",
            "[INFO] 2023-04-07 13:51:51 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:51:55 - Epoch : 49 Train loss: -0.6142957085943491 Val loss: -0.5926519263358343 Train ndcg_10 0.5956421494483948 Val ndcg_10 0.4988199472427368\n",
            "[INFO] 2023-04-07 13:51:55 - Current:0.4988199472427368 Best:0.498759925365448\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:51:57 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold3-listMLE', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold3-listMLE', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:51:57 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold3/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='listMLE', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:51:57 - will execute cp config.json output/results/MQ2008-Fold3-listMLE/used_config.json\n",
            "[INFO] 2023-04-07 13:51:57 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:51:57 - will load train data from MQ2008/Fold3/train.txt\n",
            "[INFO] 2023-04-07 13:51:58 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/train.txt'> and got x shape (6821, 46), y shape (6821,) and query_ids shape (6821,)\n",
            "[INFO] 2023-04-07 13:51:58 - loaded dataset with 347 queries\n",
            "[INFO] 2023-04-07 13:51:58 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:51:58 - train DS shape: [347, 119, 46]\n",
            "[INFO] 2023-04-07 13:51:58 - will load vali data from MQ2008/Fold3/vali.txt\n",
            "[INFO] 2023-04-07 13:51:58 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/vali.txt'> and got x shape (2287, 46), y shape (2287,) and query_ids shape (2287,)\n",
            "[INFO] 2023-04-07 13:51:58 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:51:58 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:51:58 - vali DS shape: [105, 118, 46]\n",
            "[INFO] 2023-04-07 13:51:58 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:51:58 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:51:58 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:51:58 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:51:58 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:00 - Epoch : 0 Train loss: 12.881545979282699 Val loss: 57.8965098063151 Train ndcg_10 0.578937292098999 Val ndcg_10 0.40022698044776917\n",
            "[INFO] 2023-04-07 13:52:00 - Current:0.40022698044776917 Best:0.0\n",
            "[INFO] 2023-04-07 13:52:00 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:03 - Epoch : 1 Train loss: 12.845143777149211 Val loss: 57.8142588297526 Train ndcg_10 0.6042861938476562 Val ndcg_10 0.40806326270103455\n",
            "[INFO] 2023-04-07 13:52:03 - Current:0.40806326270103455 Best:0.40022698044776917\n",
            "[INFO] 2023-04-07 13:52:03 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:05 - Epoch : 2 Train loss: 12.796946533818753 Val loss: 57.805986967540925 Train ndcg_10 0.5710355043411255 Val ndcg_10 0.4119594693183899\n",
            "[INFO] 2023-04-07 13:52:05 - Current:0.4119594693183899 Best:0.40806326270103455\n",
            "[INFO] 2023-04-07 13:52:05 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:08 - Epoch : 3 Train loss: 12.816824690409284 Val loss: 57.733407520112536 Train ndcg_10 0.5740162134170532 Val ndcg_10 0.4179072380065918\n",
            "[INFO] 2023-04-07 13:52:08 - Current:0.4179072380065918 Best:0.4119594693183899\n",
            "[INFO] 2023-04-07 13:52:08 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:12 - Epoch : 4 Train loss: 12.819253000814564 Val loss: 57.70633130754743 Train ndcg_10 0.5830378532409668 Val ndcg_10 0.4216521084308624\n",
            "[INFO] 2023-04-07 13:52:12 - Current:0.4216521084308624 Best:0.4179072380065918\n",
            "[INFO] 2023-04-07 13:52:12 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:14 - Epoch : 5 Train loss: 12.79376229222982 Val loss: 57.61318562825521 Train ndcg_10 0.5958743691444397 Val ndcg_10 0.42499658465385437\n",
            "[INFO] 2023-04-07 13:52:14 - Current:0.42499658465385437 Best:0.4216521084308624\n",
            "[INFO] 2023-04-07 13:52:14 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:17 - Epoch : 6 Train loss: 12.803725237118064 Val loss: 57.55261114211309 Train ndcg_10 0.5994113087654114 Val ndcg_10 0.42691901326179504\n",
            "[INFO] 2023-04-07 13:52:17 - Current:0.42691901326179504 Best:0.42499658465385437\n",
            "[INFO] 2023-04-07 13:52:17 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:19 - Epoch : 7 Train loss: 12.77200766461727 Val loss: 57.597954958961125 Train ndcg_10 0.6045639514923096 Val ndcg_10 0.43149206042289734\n",
            "[INFO] 2023-04-07 13:52:19 - Current:0.43149206042289734 Best:0.42691901326179504\n",
            "[INFO] 2023-04-07 13:52:19 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:23 - Epoch : 8 Train loss: 12.733891816922497 Val loss: 57.46964169456845 Train ndcg_10 0.6189386248588562 Val ndcg_10 0.44471678137779236\n",
            "[INFO] 2023-04-07 13:52:23 - Current:0.44471678137779236 Best:0.43149206042289734\n",
            "[INFO] 2023-04-07 13:52:23 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:26 - Epoch : 9 Train loss: 12.734853689540017 Val loss: 57.49983985537575 Train ndcg_10 0.6106332540512085 Val ndcg_10 0.4474346339702606\n",
            "[INFO] 2023-04-07 13:52:26 - Current:0.4474346339702606 Best:0.44471678137779236\n",
            "[INFO] 2023-04-07 13:52:26 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:28 - Epoch : 10 Train loss: 12.683751529506717 Val loss: 57.44800291515532 Train ndcg_10 0.6222400665283203 Val ndcg_10 0.4591521620750427\n",
            "[INFO] 2023-04-07 13:52:28 - Current:0.4591521620750427 Best:0.4474346339702606\n",
            "[INFO] 2023-04-07 13:52:28 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:31 - Epoch : 11 Train loss: 12.705829903440447 Val loss: 57.458670479910715 Train ndcg_10 0.6299329400062561 Val ndcg_10 0.4652746915817261\n",
            "[INFO] 2023-04-07 13:52:31 - Current:0.4652746915817261 Best:0.4591521620750427\n",
            "[INFO] 2023-04-07 13:52:31 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:33 - Epoch : 12 Train loss: 12.700704426174548 Val loss: 57.42561354864211 Train ndcg_10 0.6131033897399902 Val ndcg_10 0.4718635082244873\n",
            "[INFO] 2023-04-07 13:52:33 - Current:0.4718635082244873 Best:0.4652746915817261\n",
            "[INFO] 2023-04-07 13:52:33 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:37 - Epoch : 13 Train loss: 12.696531262796276 Val loss: 57.38549368722098 Train ndcg_10 0.6277902126312256 Val ndcg_10 0.4785884916782379\n",
            "[INFO] 2023-04-07 13:52:37 - Current:0.4785884916782379 Best:0.4718635082244873\n",
            "[INFO] 2023-04-07 13:52:37 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:52:40 - Epoch : 14 Train loss: 12.688141501259048 Val loss: 57.36280335925874 Train ndcg_10 0.6364925503730774 Val ndcg_10 0.48510026931762695\n",
            "[INFO] 2023-04-07 13:52:40 - Current:0.48510026931762695 Best:0.4785884916782379\n",
            "[INFO] 2023-04-07 13:52:40 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:52:42 - Epoch : 15 Train loss: 12.661691308365087 Val loss: 57.28908001127697 Train ndcg_10 0.6292052865028381 Val ndcg_10 0.4855572581291199\n",
            "[INFO] 2023-04-07 13:52:42 - Current:0.4855572581291199 Best:0.48510026931762695\n",
            "[INFO] 2023-04-07 13:52:42 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:52:45 - Epoch : 16 Train loss: 12.64752523218864 Val loss: 57.352526346842446 Train ndcg_10 0.6347456574440002 Val ndcg_10 0.4863159954547882\n",
            "[INFO] 2023-04-07 13:52:45 - Current:0.4863159954547882 Best:0.4855572581291199\n",
            "[INFO] 2023-04-07 13:52:45 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:52:47 - Epoch : 17 Train loss: 12.691225247012092 Val loss: 57.38021545410156 Train ndcg_10 0.6505615711212158 Val ndcg_10 0.48998889327049255\n",
            "[INFO] 2023-04-07 13:52:47 - Current:0.48998889327049255 Best:0.4863159954547882\n",
            "[INFO] 2023-04-07 13:52:47 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:52:51 - Epoch : 18 Train loss: 12.666502482609378 Val loss: 57.37664736793155 Train ndcg_10 0.6438419222831726 Val ndcg_10 0.4920309782028198\n",
            "[INFO] 2023-04-07 13:52:51 - Current:0.4920309782028198 Best:0.48998889327049255\n",
            "[INFO] 2023-04-07 13:52:51 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:52:54 - Epoch : 19 Train loss: 12.690243836438622 Val loss: 57.34200730096726 Train ndcg_10 0.6472611427307129 Val ndcg_10 0.4939420521259308\n",
            "[INFO] 2023-04-07 13:52:54 - Current:0.4939420521259308 Best:0.4920309782028198\n",
            "[INFO] 2023-04-07 13:52:54 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:52:56 - Epoch : 20 Train loss: 12.632167794175382 Val loss: 57.389503115699405 Train ndcg_10 0.644072949886322 Val ndcg_10 0.4943835437297821\n",
            "[INFO] 2023-04-07 13:52:56 - Current:0.4943835437297821 Best:0.4939420521259308\n",
            "[INFO] 2023-04-07 13:52:56 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:52:59 - Epoch : 21 Train loss: 12.671888890115259 Val loss: 57.37643236432757 Train ndcg_10 0.6335095167160034 Val ndcg_10 0.4980010390281677\n",
            "[INFO] 2023-04-07 13:52:59 - Current:0.4980010390281677 Best:0.4943835437297821\n",
            "[INFO] 2023-04-07 13:52:59 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:01 - Epoch : 22 Train loss: 12.677954121350554 Val loss: 57.38526226225353 Train ndcg_10 0.646705687046051 Val ndcg_10 0.49847114086151123\n",
            "[INFO] 2023-04-07 13:53:01 - Current:0.49847114086151123 Best:0.4980010390281677\n",
            "[INFO] 2023-04-07 13:53:01 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:05 - Epoch : 23 Train loss: 12.635065312344334 Val loss: 57.33361380440848 Train ndcg_10 0.6472126841545105 Val ndcg_10 0.5043480396270752\n",
            "[INFO] 2023-04-07 13:53:05 - Current:0.5043480396270752 Best:0.49847114086151123\n",
            "[INFO] 2023-04-07 13:53:05 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:08 - Epoch : 24 Train loss: 12.658554646055018 Val loss: 57.29149446033296 Train ndcg_10 0.6317551136016846 Val ndcg_10 0.5080472826957703\n",
            "[INFO] 2023-04-07 13:53:08 - Current:0.5080472826957703 Best:0.5043480396270752\n",
            "[INFO] 2023-04-07 13:53:08 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:11 - Epoch : 25 Train loss: 12.697879134406273 Val loss: 57.33377634684245 Train ndcg_10 0.6372103691101074 Val ndcg_10 0.5113146305084229\n",
            "[INFO] 2023-04-07 13:53:11 - Current:0.5113146305084229 Best:0.5080472826957703\n",
            "[INFO] 2023-04-07 13:53:11 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:13 - Epoch : 26 Train loss: 12.687452247575655 Val loss: 57.32378401983352 Train ndcg_10 0.6388899683952332 Val ndcg_10 0.513360321521759\n",
            "[INFO] 2023-04-07 13:53:13 - Current:0.513360321521759 Best:0.5113146305084229\n",
            "[INFO] 2023-04-07 13:53:13 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:16 - Epoch : 27 Train loss: 12.657282911734898 Val loss: 57.323015776134675 Train ndcg_10 0.6530085802078247 Val ndcg_10 0.5147278308868408\n",
            "[INFO] 2023-04-07 13:53:16 - Current:0.5147278308868408 Best:0.513360321521759\n",
            "[INFO] 2023-04-07 13:53:16 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:19 - Epoch : 28 Train loss: 12.677831853157505 Val loss: 57.31728290376209 Train ndcg_10 0.6524778008460999 Val ndcg_10 0.5168586373329163\n",
            "[INFO] 2023-04-07 13:53:19 - Current:0.5168586373329163 Best:0.5147278308868408\n",
            "[INFO] 2023-04-07 13:53:19 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:53:22 - Epoch : 29 Train loss: 12.651182144450866 Val loss: 57.3346923828125 Train ndcg_10 0.6305898427963257 Val ndcg_10 0.5171421766281128\n",
            "[INFO] 2023-04-07 13:53:22 - Current:0.5171421766281128 Best:0.5168586373329163\n",
            "[INFO] 2023-04-07 13:53:22 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:25 - Epoch : 30 Train loss: 12.638793585279833 Val loss: 57.328543672107514 Train ndcg_10 0.6263548731803894 Val ndcg_10 0.5171421766281128\n",
            "[INFO] 2023-04-07 13:53:25 - Current:0.5171421766281128 Best:0.5171421766281128\n",
            "[INFO] 2023-04-07 13:53:25 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:27 - Epoch : 31 Train loss: 12.654914636089753 Val loss: 57.337998599097844 Train ndcg_10 0.6459234952926636 Val ndcg_10 0.5171923041343689\n",
            "[INFO] 2023-04-07 13:53:27 - Current:0.5171923041343689 Best:0.5171421766281128\n",
            "[INFO] 2023-04-07 13:53:27 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:30 - Epoch : 32 Train loss: 12.662914693870874 Val loss: 57.270420910063244 Train ndcg_10 0.6385681629180908 Val ndcg_10 0.5176715850830078\n",
            "[INFO] 2023-04-07 13:53:30 - Current:0.5176715850830078 Best:0.5171923041343689\n",
            "[INFO] 2023-04-07 13:53:30 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:33 - Epoch : 33 Train loss: 12.64629069666354 Val loss: 57.34830271402995 Train ndcg_10 0.6436575651168823 Val ndcg_10 0.5175253748893738\n",
            "[INFO] 2023-04-07 13:53:33 - Current:0.5175253748893738 Best:0.5176715850830078\n",
            "[INFO] 2023-04-07 13:53:33 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:36 - Epoch : 34 Train loss: 12.66485561623697 Val loss: 57.38619188581194 Train ndcg_10 0.6460745334625244 Val ndcg_10 0.5175253748893738\n",
            "[INFO] 2023-04-07 13:53:36 - Current:0.5175253748893738 Best:0.5176715850830078\n",
            "[INFO] 2023-04-07 13:53:36 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:39 - Epoch : 35 Train loss: 12.664782268513177 Val loss: 57.31566816057477 Train ndcg_10 0.6269913911819458 Val ndcg_10 0.5179685950279236\n",
            "[INFO] 2023-04-07 13:53:39 - Current:0.5179685950279236 Best:0.5176715850830078\n",
            "[INFO] 2023-04-07 13:53:39 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:41 - Epoch : 36 Train loss: 12.636474430732838 Val loss: 57.347156125023254 Train ndcg_10 0.6461380124092102 Val ndcg_10 0.5182235240936279\n",
            "[INFO] 2023-04-07 13:53:41 - Current:0.5182235240936279 Best:0.5179685950279236\n",
            "[INFO] 2023-04-07 13:53:41 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:44 - Epoch : 37 Train loss: 12.629678959805272 Val loss: 57.25423307872954 Train ndcg_10 0.6303673386573792 Val ndcg_10 0.5188019275665283\n",
            "[INFO] 2023-04-07 13:53:44 - Current:0.5188019275665283 Best:0.5182235240936279\n",
            "[INFO] 2023-04-07 13:53:44 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:47 - Epoch : 38 Train loss: 12.67751934480255 Val loss: 57.35040457589286 Train ndcg_10 0.6462258696556091 Val ndcg_10 0.5191653370857239\n",
            "[INFO] 2023-04-07 13:53:47 - Current:0.5191653370857239 Best:0.5188019275665283\n",
            "[INFO] 2023-04-07 13:53:47 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:50 - Epoch : 39 Train loss: 12.661247574973862 Val loss: 57.35805882045201 Train ndcg_10 0.6572787761688232 Val ndcg_10 0.5192230343818665\n",
            "[INFO] 2023-04-07 13:53:50 - Current:0.5192230343818665 Best:0.5191653370857239\n",
            "[INFO] 2023-04-07 13:53:50 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:53 - Epoch : 40 Train loss: 12.655389549409279 Val loss: 57.26003679547991 Train ndcg_10 0.6560764312744141 Val ndcg_10 0.5192230343818665\n",
            "[INFO] 2023-04-07 13:53:53 - Current:0.5192230343818665 Best:0.5192230343818665\n",
            "[INFO] 2023-04-07 13:53:53 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:55 - Epoch : 41 Train loss: 12.656004592389813 Val loss: 57.37128077915737 Train ndcg_10 0.6458368897438049 Val ndcg_10 0.5193392038345337\n",
            "[INFO] 2023-04-07 13:53:55 - Current:0.5193392038345337 Best:0.5192230343818665\n",
            "[INFO] 2023-04-07 13:53:55 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:53:58 - Epoch : 42 Train loss: 12.661982550057623 Val loss: 57.35823189871652 Train ndcg_10 0.6425036191940308 Val ndcg_10 0.5196329951286316\n",
            "[INFO] 2023-04-07 13:53:58 - Current:0.5196329951286316 Best:0.5193392038345337\n",
            "[INFO] 2023-04-07 13:53:58 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:54:02 - Epoch : 43 Train loss: 12.633852150666954 Val loss: 57.34725770496187 Train ndcg_10 0.657532811164856 Val ndcg_10 0.519447922706604\n",
            "[INFO] 2023-04-07 13:54:02 - Current:0.519447922706604 Best:0.5196329951286316\n",
            "[INFO] 2023-04-07 13:54:02 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:54:04 - Epoch : 44 Train loss: 12.599363467191756 Val loss: 57.351569330124626 Train ndcg_10 0.655037522315979 Val ndcg_10 0.5195520520210266\n",
            "[INFO] 2023-04-07 13:54:04 - Current:0.5195520520210266 Best:0.5196329951286316\n",
            "[INFO] 2023-04-07 13:54:04 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:54:07 - Epoch : 45 Train loss: 12.667490228108782 Val loss: 57.279080636160714 Train ndcg_10 0.6327731013298035 Val ndcg_10 0.5195520520210266\n",
            "[INFO] 2023-04-07 13:54:07 - Current:0.5195520520210266 Best:0.5196329951286316\n",
            "[INFO] 2023-04-07 13:54:07 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:54:09 - Epoch : 46 Train loss: 12.64362558920033 Val loss: 57.3164794195266 Train ndcg_10 0.6544551253318787 Val ndcg_10 0.5195520520210266\n",
            "[INFO] 2023-04-07 13:54:09 - Current:0.5195520520210266 Best:0.5196329951286316\n",
            "[INFO] 2023-04-07 13:54:09 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:54:12 - Epoch : 47 Train loss: 12.643343810045753 Val loss: 57.323343258812315 Train ndcg_10 0.6672614216804504 Val ndcg_10 0.5196102857589722\n",
            "[INFO] 2023-04-07 13:54:12 - Current:0.5196102857589722 Best:0.5196329951286316\n",
            "[INFO] 2023-04-07 13:54:12 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:54:16 - Epoch : 48 Train loss: 12.67076667104056 Val loss: 57.31523880731492 Train ndcg_10 0.6565887928009033 Val ndcg_10 0.5196102857589722\n",
            "[INFO] 2023-04-07 13:54:16 - Current:0.5196102857589722 Best:0.5196329951286316\n",
            "[INFO] 2023-04-07 13:54:16 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:54:18 - Epoch : 49 Train loss: 12.634263687243722 Val loss: 57.37614520844959 Train ndcg_10 0.6453298330307007 Val ndcg_10 0.5196102857589722\n",
            "[INFO] 2023-04-07 13:54:18 - Current:0.5196102857589722 Best:0.5196329951286316\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:54:20 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold3-ordinal', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold3-ordinal', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:54:20 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold3/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='ordinal', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:54:20 - will execute cp config.json output/results/MQ2008-Fold3-ordinal/used_config.json\n",
            "[INFO] 2023-04-07 13:54:20 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:54:20 - will load train data from MQ2008/Fold3/train.txt\n",
            "[INFO] 2023-04-07 13:54:20 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/train.txt'> and got x shape (6821, 46), y shape (6821,) and query_ids shape (6821,)\n",
            "[INFO] 2023-04-07 13:54:20 - loaded dataset with 347 queries\n",
            "[INFO] 2023-04-07 13:54:20 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:54:20 - train DS shape: [347, 119, 46]\n",
            "[INFO] 2023-04-07 13:54:20 - will load vali data from MQ2008/Fold3/vali.txt\n",
            "[INFO] 2023-04-07 13:54:20 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/vali.txt'> and got x shape (2287, 46), y shape (2287,) and query_ids shape (2287,)\n",
            "[INFO] 2023-04-07 13:54:20 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:54:20 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:54:20 - vali DS shape: [105, 118, 46]\n",
            "[INFO] 2023-04-07 13:54:20 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:54:20 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:54:20 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:54:20 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:54:20 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: ordinal() missing 1 required positional argument: 'n'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:54:23 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold3-lambdaLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold3-lambdaLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:54:23 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold3/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='lambdaLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:54:23 - will execute cp config.json output/results/MQ2008-Fold3-lambdaLoss/used_config.json\n",
            "[INFO] 2023-04-07 13:54:23 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:54:23 - will load train data from MQ2008/Fold3/train.txt\n",
            "[INFO] 2023-04-07 13:54:23 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/train.txt'> and got x shape (6821, 46), y shape (6821,) and query_ids shape (6821,)\n",
            "[INFO] 2023-04-07 13:54:23 - loaded dataset with 347 queries\n",
            "[INFO] 2023-04-07 13:54:23 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:54:23 - train DS shape: [347, 119, 46]\n",
            "[INFO] 2023-04-07 13:54:23 - will load vali data from MQ2008/Fold3/vali.txt\n",
            "[INFO] 2023-04-07 13:54:23 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/vali.txt'> and got x shape (2287, 46), y shape (2287,) and query_ids shape (2287,)\n",
            "[INFO] 2023-04-07 13:54:23 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:54:23 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:54:23 - vali DS shape: [105, 118, 46]\n",
            "[INFO] 2023-04-07 13:54:23 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:54:23 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:54:23 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:54:23 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:54:23 - Current learning rate: 0.001\n",
            "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in Log2Backward. Traceback of forward call that caused the error:\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/models/losses/lambdaLoss.py\", line 70, in lambdaLoss\n",
            "losses = torch.log2(weighted_probas)\n",
            "(Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
            "Variable._execution_engine.run_backward(\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 23, in loss_batch\n",
            "loss.backward()\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/tensor.py\", line 245, in backward\n",
            "torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 145, in backward\n",
            "Variable._execution_engine.run_backward(\n",
            "RuntimeError: Function 'Log2Backward' returned nan values in its 0th output.\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:54:25 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold3-approxNDCGLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:54:25 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold3/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='approxNDCGLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:54:25 - will execute cp config.json output/results/MQ2008-Fold3-approxNDCGLoss/used_config.json\n",
            "[INFO] 2023-04-07 13:54:25 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:54:25 - will load train data from MQ2008/Fold3/train.txt\n",
            "[INFO] 2023-04-07 13:54:25 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/train.txt'> and got x shape (6821, 46), y shape (6821,) and query_ids shape (6821,)\n",
            "[INFO] 2023-04-07 13:54:25 - loaded dataset with 347 queries\n",
            "[INFO] 2023-04-07 13:54:25 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:54:25 - train DS shape: [347, 119, 46]\n",
            "[INFO] 2023-04-07 13:54:25 - will load vali data from MQ2008/Fold3/vali.txt\n",
            "[INFO] 2023-04-07 13:54:25 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/vali.txt'> and got x shape (2287, 46), y shape (2287,) and query_ids shape (2287,)\n",
            "[INFO] 2023-04-07 13:54:25 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:54:25 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:54:25 - vali DS shape: [105, 118, 46]\n",
            "[INFO] 2023-04-07 13:54:25 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:54:25 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:54:25 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:54:25 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:54:25 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:29 - Epoch : 0 Train loss: -0.49330010434736093 Val loss: -0.46854912042617797 Train ndcg_10 0.5776880979537964 Val ndcg_10 0.3975410759449005\n",
            "[INFO] 2023-04-07 13:54:29 - Current:0.3975410759449005 Best:0.0\n",
            "[INFO] 2023-04-07 13:54:29 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:32 - Epoch : 1 Train loss: -0.49476625467583496 Val loss: -0.4685737255073729 Train ndcg_10 0.5613784790039062 Val ndcg_10 0.3970263600349426\n",
            "[INFO] 2023-04-07 13:54:32 - Current:0.3970263600349426 Best:0.3975410759449005\n",
            "[INFO] 2023-04-07 13:54:32 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:34 - Epoch : 2 Train loss: -0.49217582221333506 Val loss: -0.46859748874391827 Train ndcg_10 0.5714904069900513 Val ndcg_10 0.3972404897212982\n",
            "[INFO] 2023-04-07 13:54:34 - Current:0.3972404897212982 Best:0.3975410759449005\n",
            "[INFO] 2023-04-07 13:54:34 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:37 - Epoch : 3 Train loss: -0.49279964408201166 Val loss: -0.468620331798281 Train ndcg_10 0.5942850708961487 Val ndcg_10 0.39754801988601685\n",
            "[INFO] 2023-04-07 13:54:37 - Current:0.39754801988601685 Best:0.3975410759449005\n",
            "[INFO] 2023-04-07 13:54:37 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:40 - Epoch : 4 Train loss: -0.49262121122233804 Val loss: -0.46864402294158936 Train ndcg_10 0.5722556114196777 Val ndcg_10 0.39772695302963257\n",
            "[INFO] 2023-04-07 13:54:40 - Current:0.39772695302963257 Best:0.39754801988601685\n",
            "[INFO] 2023-04-07 13:54:40 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:43 - Epoch : 5 Train loss: -0.4909426403320488 Val loss: -0.4686682238465264 Train ndcg_10 0.5810495615005493 Val ndcg_10 0.3984251618385315\n",
            "[INFO] 2023-04-07 13:54:43 - Current:0.3984251618385315 Best:0.39772695302963257\n",
            "[INFO] 2023-04-07 13:54:43 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:46 - Epoch : 6 Train loss: -0.4918644793267209 Val loss: -0.4686936775843302 Train ndcg_10 0.5840163826942444 Val ndcg_10 0.39990851283073425\n",
            "[INFO] 2023-04-07 13:54:46 - Current:0.39990851283073425 Best:0.3984251618385315\n",
            "[INFO] 2023-04-07 13:54:46 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:49 - Epoch : 7 Train loss: -0.49175084702219674 Val loss: -0.4687173653216589 Train ndcg_10 0.5852161645889282 Val ndcg_10 0.40023142099380493\n",
            "[INFO] 2023-04-07 13:54:49 - Current:0.40023142099380493 Best:0.39990851283073425\n",
            "[INFO] 2023-04-07 13:54:49 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:51 - Epoch : 8 Train loss: -0.4929776189134856 Val loss: -0.46874260618573144 Train ndcg_10 0.5802343487739563 Val ndcg_10 0.4003889858722687\n",
            "[INFO] 2023-04-07 13:54:51 - Current:0.4003889858722687 Best:0.40023142099380493\n",
            "[INFO] 2023-04-07 13:54:51 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:54 - Epoch : 9 Train loss: -0.4917118342362495 Val loss: -0.4687661994071234 Train ndcg_10 0.5751473903656006 Val ndcg_10 0.40075257420539856\n",
            "[INFO] 2023-04-07 13:54:54 - Current:0.40075257420539856 Best:0.4003889858722687\n",
            "[INFO] 2023-04-07 13:54:54 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:54:58 - Epoch : 10 Train loss: -0.49671534219117947 Val loss: -0.46878912846247356 Train ndcg_10 0.5855490565299988 Val ndcg_10 0.40126723051071167\n",
            "[INFO] 2023-04-07 13:54:58 - Current:0.40126723051071167 Best:0.40075257420539856\n",
            "[INFO] 2023-04-07 13:54:58 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:55:00 - Epoch : 11 Train loss: -0.4924931493547533 Val loss: -0.46881244210969836 Train ndcg_10 0.5731527209281921 Val ndcg_10 0.40126723051071167\n",
            "[INFO] 2023-04-07 13:55:00 - Current:0.40126723051071167 Best:0.40126723051071167\n",
            "[INFO] 2023-04-07 13:55:00 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:55:03 - Epoch : 12 Train loss: -0.495543834111876 Val loss: -0.46883727397237507 Train ndcg_10 0.5691439509391785 Val ndcg_10 0.4013002812862396\n",
            "[INFO] 2023-04-07 13:55:03 - Current:0.4013002812862396 Best:0.40126723051071167\n",
            "[INFO] 2023-04-07 13:55:03 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:55:05 - Epoch : 13 Train loss: -0.4930611391060634 Val loss: -0.4688617249329885 Train ndcg_10 0.56926029920578 Val ndcg_10 0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:05 - Current:0.4017843008041382 Best:0.4013002812862396\n",
            "[INFO] 2023-04-07 13:55:05 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:55:08 - Epoch : 14 Train loss: -0.49408176532739867 Val loss: -0.4688835742927733 Train ndcg_10 0.5962895750999451 Val ndcg_10 0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:08 - Current:0.4017843008041382 Best:0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:08 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:12 - Epoch : 15 Train loss: -0.4939718117631478 Val loss: -0.4688883545852843 Train ndcg_10 0.5755601525306702 Val ndcg_10 0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:12 - Current:0.4017843008041382 Best:0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:12 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:14 - Epoch : 16 Train loss: -0.4946050927343561 Val loss: -0.46889297053927464 Train ndcg_10 0.5863974690437317 Val ndcg_10 0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:14 - Current:0.4017843008041382 Best:0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:14 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:17 - Epoch : 17 Train loss: -0.49398474442512225 Val loss: -0.4688975141161964 Train ndcg_10 0.5801036953926086 Val ndcg_10 0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:17 - Current:0.4017843008041382 Best:0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:17 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:19 - Epoch : 18 Train loss: -0.49355123752475816 Val loss: -0.4689021022546859 Train ndcg_10 0.5761153697967529 Val ndcg_10 0.40180569887161255\n",
            "[INFO] 2023-04-07 13:55:19 - Current:0.40180569887161255 Best:0.4017843008041382\n",
            "[INFO] 2023-04-07 13:55:19 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:23 - Epoch : 19 Train loss: -0.4918636412544965 Val loss: -0.4689069211483002 Train ndcg_10 0.5872589945793152 Val ndcg_10 0.40180569887161255\n",
            "[INFO] 2023-04-07 13:55:23 - Current:0.40180569887161255 Best:0.40180569887161255\n",
            "[INFO] 2023-04-07 13:55:23 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:26 - Epoch : 20 Train loss: -0.49383668047550433 Val loss: -0.4689114491144816 Train ndcg_10 0.5942234396934509 Val ndcg_10 0.4019426703453064\n",
            "[INFO] 2023-04-07 13:55:26 - Current:0.4019426703453064 Best:0.40180569887161255\n",
            "[INFO] 2023-04-07 13:55:26 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:28 - Epoch : 21 Train loss: -0.49321744283956476 Val loss: -0.4689162123770941 Train ndcg_10 0.5784975290298462 Val ndcg_10 0.4022170901298523\n",
            "[INFO] 2023-04-07 13:55:28 - Current:0.4022170901298523 Best:0.4019426703453064\n",
            "[INFO] 2023-04-07 13:55:28 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:31 - Epoch : 22 Train loss: -0.4947637848620456 Val loss: -0.46892081584249223 Train ndcg_10 0.568446159362793 Val ndcg_10 0.4024984836578369\n",
            "[INFO] 2023-04-07 13:55:31 - Current:0.4024984836578369 Best:0.4022170901298523\n",
            "[INFO] 2023-04-07 13:55:31 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:34 - Epoch : 23 Train loss: -0.4927356269758098 Val loss: -0.4689256174223764 Train ndcg_10 0.5706291198730469 Val ndcg_10 0.4024984836578369\n",
            "[INFO] 2023-04-07 13:55:34 - Current:0.4024984836578369 Best:0.4024984836578369\n",
            "[INFO] 2023-04-07 13:55:34 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:38 - Epoch : 24 Train loss: -0.497635946493671 Val loss: -0.468930458171027 Train ndcg_10 0.5757085084915161 Val ndcg_10 0.40278172492980957\n",
            "[INFO] 2023-04-07 13:55:38 - Current:0.40278172492980957 Best:0.4024984836578369\n",
            "[INFO] 2023-04-07 13:55:38 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:41 - Epoch : 25 Train loss: -0.4954974510484195 Val loss: -0.46893535795665925 Train ndcg_10 0.5791584849357605 Val ndcg_10 0.40278172492980957\n",
            "[INFO] 2023-04-07 13:55:41 - Current:0.40278172492980957 Best:0.40278172492980957\n",
            "[INFO] 2023-04-07 13:55:41 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:43 - Epoch : 26 Train loss: -0.4961467169719058 Val loss: -0.46893990919703527 Train ndcg_10 0.5849139094352722 Val ndcg_10 0.40279731154441833\n",
            "[INFO] 2023-04-07 13:55:43 - Current:0.40279731154441833 Best:0.40278172492980957\n",
            "[INFO] 2023-04-07 13:55:43 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:46 - Epoch : 27 Train loss: -0.49275413138378593 Val loss: -0.4689448546795618 Train ndcg_10 0.5813700556755066 Val ndcg_10 0.40279731154441833\n",
            "[INFO] 2023-04-07 13:55:46 - Current:0.40279731154441833 Best:0.40279731154441833\n",
            "[INFO] 2023-04-07 13:55:46 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:49 - Epoch : 28 Train loss: -0.49315951760289306 Val loss: -0.4689493676026662 Train ndcg_10 0.5753090977668762 Val ndcg_10 0.4028204083442688\n",
            "[INFO] 2023-04-07 13:55:49 - Current:0.4028204083442688 Best:0.40279731154441833\n",
            "[INFO] 2023-04-07 13:55:49 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:55:52 - Epoch : 29 Train loss: -0.49340638114670854 Val loss: -0.4689541958627247 Train ndcg_10 0.5937731862068176 Val ndcg_10 0.4032377600669861\n",
            "[INFO] 2023-04-07 13:55:52 - Current:0.4032377600669861 Best:0.4028204083442688\n",
            "[INFO] 2023-04-07 13:55:52 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:55:55 - Epoch : 30 Train loss: -0.49450473517437143 Val loss: -0.4689551168964023 Train ndcg_10 0.5680385231971741 Val ndcg_10 0.4032377600669861\n",
            "[INFO] 2023-04-07 13:55:55 - Current:0.4032377600669861 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:55:55 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:55:58 - Epoch : 31 Train loss: -0.49309163813288687 Val loss: -0.46895608334314254 Train ndcg_10 0.5756621360778809 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:55:58 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:55:58 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:00 - Epoch : 32 Train loss: -0.4937224206903826 Val loss: -0.4689569666272118 Train ndcg_10 0.5898277163505554 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:00 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:00 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:03 - Epoch : 33 Train loss: -0.49516598085848673 Val loss: -0.46895786552202134 Train ndcg_10 0.5825989246368408 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:03 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:03 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:07 - Epoch : 34 Train loss: -0.49442499575422544 Val loss: -0.46895879677363805 Train ndcg_10 0.5835029482841492 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:07 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:07 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:09 - Epoch : 35 Train loss: -0.4950229894874419 Val loss: -0.4689597166719891 Train ndcg_10 0.5691361427307129 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:09 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:09 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:12 - Epoch : 36 Train loss: -0.49505462091663044 Val loss: -0.46896063884099326 Train ndcg_10 0.5909993052482605 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:12 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:12 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:14 - Epoch : 37 Train loss: -0.49389902667628244 Val loss: -0.4689615805943807 Train ndcg_10 0.5667864680290222 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:14 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:14 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:18 - Epoch : 38 Train loss: -0.4953993876660592 Val loss: -0.4689624911262876 Train ndcg_10 0.5716522336006165 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:18 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:18 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:21 - Epoch : 39 Train loss: -0.4936265126085419 Val loss: -0.46896342777070543 Train ndcg_10 0.5788361430168152 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:21 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:21 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:56:23 - Epoch : 40 Train loss: -0.4941915029751121 Val loss: -0.4689643681049347 Train ndcg_10 0.575835108757019 Val ndcg_10 0.40301385521888733\n",
            "[INFO] 2023-04-07 13:56:23 - Current:0.40301385521888733 Best:0.4032377600669861\n",
            "[INFO] 2023-04-07 13:56:23 - early stopping at epoch 40 since ndcg_10 didn't improve from epoch no 29. Best value 0.4032377600669861, current value 0.40301385521888733\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:56:26 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold3-pointwise_rmse', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold3-pointwise_rmse', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:56:26 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold3/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='pointwise_rmse', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:56:26 - will execute cp config.json output/results/MQ2008-Fold3-pointwise_rmse/used_config.json\n",
            "[INFO] 2023-04-07 13:56:26 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:56:26 - will load train data from MQ2008/Fold3/train.txt\n",
            "[INFO] 2023-04-07 13:56:26 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/train.txt'> and got x shape (6821, 46), y shape (6821,) and query_ids shape (6821,)\n",
            "[INFO] 2023-04-07 13:56:26 - loaded dataset with 347 queries\n",
            "[INFO] 2023-04-07 13:56:26 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:56:26 - train DS shape: [347, 119, 46]\n",
            "[INFO] 2023-04-07 13:56:26 - will load vali data from MQ2008/Fold3/vali.txt\n",
            "[INFO] 2023-04-07 13:56:26 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/vali.txt'> and got x shape (2287, 46), y shape (2287,) and query_ids shape (2287,)\n",
            "[INFO] 2023-04-07 13:56:26 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:56:26 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:56:26 - vali DS shape: [105, 118, 46]\n",
            "[INFO] 2023-04-07 13:56:26 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:56:26 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:56:26 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:56:26 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:56:26 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: pointwise_rmse() missing 1 required positional argument: 'no_of_levels'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:56:28 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold3-neuralNDCG', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold3-neuralNDCG', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:56:28 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold3/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='neuralNDCG', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:56:28 - will execute cp config.json output/results/MQ2008-Fold3-neuralNDCG/used_config.json\n",
            "[INFO] 2023-04-07 13:56:28 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:56:28 - will load train data from MQ2008/Fold3/train.txt\n",
            "[INFO] 2023-04-07 13:56:28 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/train.txt'> and got x shape (6821, 46), y shape (6821,) and query_ids shape (6821,)\n",
            "[INFO] 2023-04-07 13:56:28 - loaded dataset with 347 queries\n",
            "[INFO] 2023-04-07 13:56:28 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:56:28 - train DS shape: [347, 119, 46]\n",
            "[INFO] 2023-04-07 13:56:28 - will load vali data from MQ2008/Fold3/vali.txt\n",
            "[INFO] 2023-04-07 13:56:28 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold3/vali.txt'> and got x shape (2287, 46), y shape (2287,) and query_ids shape (2287,)\n",
            "[INFO] 2023-04-07 13:56:28 - loaded dataset with 105 queries\n",
            "[INFO] 2023-04-07 13:56:28 - longest query had 118 documents\n",
            "[INFO] 2023-04-07 13:56:28 - vali DS shape: [105, 118, 46]\n",
            "[INFO] 2023-04-07 13:56:28 - Will pad to the longest slate: 118\n",
            "[INFO] 2023-04-07 13:56:28 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:56:28 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:56:28 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:56:28 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:32 - Epoch : 0 Train loss: -0.5909828789295999 Val loss: -0.5498652940704709 Train ndcg_10 0.580797016620636 Val ndcg_10 0.3993509113788605\n",
            "[INFO] 2023-04-07 13:56:32 - Current:0.3993509113788605 Best:0.0\n",
            "[INFO] 2023-04-07 13:56:32 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:36 - Epoch : 1 Train loss: -0.5961610137213891 Val loss: -0.5513960123062134 Train ndcg_10 0.5656808018684387 Val ndcg_10 0.4016992449760437\n",
            "[INFO] 2023-04-07 13:56:36 - Current:0.4016992449760437 Best:0.3993509113788605\n",
            "[INFO] 2023-04-07 13:56:36 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:39 - Epoch : 2 Train loss: -0.5873085021629114 Val loss: -0.552880689076015 Train ndcg_10 0.5733680129051208 Val ndcg_10 0.40683454275131226\n",
            "[INFO] 2023-04-07 13:56:39 - Current:0.40683454275131226 Best:0.4016992449760437\n",
            "[INFO] 2023-04-07 13:56:39 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:42 - Epoch : 3 Train loss: -0.5973046714015928 Val loss: -0.5543290291513715 Train ndcg_10 0.6005561947822571 Val ndcg_10 0.4101497232913971\n",
            "[INFO] 2023-04-07 13:56:42 - Current:0.4101497232913971 Best:0.40683454275131226\n",
            "[INFO] 2023-04-07 13:56:42 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:46 - Epoch : 4 Train loss: -0.593679823002829 Val loss: -0.5559115228198823 Train ndcg_10 0.5816982984542847 Val ndcg_10 0.4107082784175873\n",
            "[INFO] 2023-04-07 13:56:46 - Current:0.4107082784175873 Best:0.4101497232913971\n",
            "[INFO] 2023-04-07 13:56:46 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:50 - Epoch : 5 Train loss: -0.5894374807904708 Val loss: -0.5574950933456421 Train ndcg_10 0.5952866673469543 Val ndcg_10 0.4140808880329132\n",
            "[INFO] 2023-04-07 13:56:50 - Current:0.4140808880329132 Best:0.4107082784175873\n",
            "[INFO] 2023-04-07 13:56:50 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:53 - Epoch : 6 Train loss: -0.5915032297115161 Val loss: -0.5591510710262116 Train ndcg_10 0.6022534370422363 Val ndcg_10 0.42130982875823975\n",
            "[INFO] 2023-04-07 13:56:53 - Current:0.42130982875823975 Best:0.4140808880329132\n",
            "[INFO] 2023-04-07 13:56:53 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:56:56 - Epoch : 7 Train loss: -0.593449957604367 Val loss: -0.5607011335236686 Train ndcg_10 0.597805917263031 Val ndcg_10 0.42484748363494873\n",
            "[INFO] 2023-04-07 13:56:56 - Current:0.42484748363494873 Best:0.42130982875823975\n",
            "[INFO] 2023-04-07 13:56:56 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:57:00 - Epoch : 8 Train loss: -0.5994729933889867 Val loss: -0.5623760075796218 Train ndcg_10 0.5931289196014404 Val ndcg_10 0.4261440932750702\n",
            "[INFO] 2023-04-07 13:57:00 - Current:0.4261440932750702 Best:0.42484748363494873\n",
            "[INFO] 2023-04-07 13:57:00 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:57:03 - Epoch : 9 Train loss: -0.5986299513052795 Val loss: -0.5638946641059149 Train ndcg_10 0.5937870740890503 Val ndcg_10 0.42773106694221497\n",
            "[INFO] 2023-04-07 13:57:03 - Current:0.42773106694221497 Best:0.4261440932750702\n",
            "[INFO] 2023-04-07 13:57:03 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:57:07 - Epoch : 10 Train loss: -0.6060161410216296 Val loss: -0.5654675364494324 Train ndcg_10 0.6036460399627686 Val ndcg_10 0.42993947863578796\n",
            "[INFO] 2023-04-07 13:57:07 - Current:0.42993947863578796 Best:0.42773106694221497\n",
            "[INFO] 2023-04-07 13:57:07 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:57:10 - Epoch : 11 Train loss: -0.602717218206664 Val loss: -0.567016263235183 Train ndcg_10 0.594258725643158 Val ndcg_10 0.4320625960826874\n",
            "[INFO] 2023-04-07 13:57:10 - Current:0.4320625960826874 Best:0.42993947863578796\n",
            "[INFO] 2023-04-07 13:57:10 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:57:14 - Epoch : 12 Train loss: -0.6084692325303466 Val loss: -0.5686818866502671 Train ndcg_10 0.5955997109413147 Val ndcg_10 0.43545812368392944\n",
            "[INFO] 2023-04-07 13:57:14 - Current:0.43545812368392944 Best:0.4320625960826874\n",
            "[INFO] 2023-04-07 13:57:14 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:57:17 - Epoch : 13 Train loss: -0.6085947810401147 Val loss: -0.5702992308707464 Train ndcg_10 0.5936079025268555 Val ndcg_10 0.4382724463939667\n",
            "[INFO] 2023-04-07 13:57:17 - Current:0.4382724463939667 Best:0.43545812368392944\n",
            "[INFO] 2023-04-07 13:57:17 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:57:20 - Epoch : 14 Train loss: -0.6053081549897318 Val loss: -0.571757017430805 Train ndcg_10 0.6230269074440002 Val ndcg_10 0.44664695858955383\n",
            "[INFO] 2023-04-07 13:57:20 - Current:0.44664695858955383 Best:0.4382724463939667\n",
            "[INFO] 2023-04-07 13:57:20 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:24 - Epoch : 15 Train loss: -0.6107134356965936 Val loss: -0.5720841163680667 Train ndcg_10 0.6014518737792969 Val ndcg_10 0.44584888219833374\n",
            "[INFO] 2023-04-07 13:57:24 - Current:0.44584888219833374 Best:0.44664695858955383\n",
            "[INFO] 2023-04-07 13:57:24 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:28 - Epoch : 16 Train loss: -0.6070075804630686 Val loss: -0.5723935291880653 Train ndcg_10 0.6148619055747986 Val ndcg_10 0.44802525639533997\n",
            "[INFO] 2023-04-07 13:57:28 - Current:0.44802525639533997 Best:0.44664695858955383\n",
            "[INFO] 2023-04-07 13:57:28 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:31 - Epoch : 17 Train loss: -0.6100291267939191 Val loss: -0.5726797291210719 Train ndcg_10 0.6144388318061829 Val ndcg_10 0.44826480746269226\n",
            "[INFO] 2023-04-07 13:57:31 - Current:0.44826480746269226 Best:0.44802525639533997\n",
            "[INFO] 2023-04-07 13:57:31 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:34 - Epoch : 18 Train loss: -0.6092199832630433 Val loss: -0.5729768474896749 Train ndcg_10 0.6065379977226257 Val ndcg_10 0.4490909278392792\n",
            "[INFO] 2023-04-07 13:57:34 - Current:0.4490909278392792 Best:0.44826480746269226\n",
            "[INFO] 2023-04-07 13:57:34 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:38 - Epoch : 19 Train loss: -0.6068661705904804 Val loss: -0.573292034580594 Train ndcg_10 0.6212082505226135 Val ndcg_10 0.4492605924606323\n",
            "[INFO] 2023-04-07 13:57:38 - Current:0.4492605924606323 Best:0.4490909278392792\n",
            "[INFO] 2023-04-07 13:57:38 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:42 - Epoch : 20 Train loss: -0.6160913187076448 Val loss: -0.5735903598013379 Train ndcg_10 0.6215597987174988 Val ndcg_10 0.45116546750068665\n",
            "[INFO] 2023-04-07 13:57:42 - Current:0.45116546750068665 Best:0.4492605924606323\n",
            "[INFO] 2023-04-07 13:57:42 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:46 - Epoch : 21 Train loss: -0.6120284374921398 Val loss: -0.5739031184287299 Train ndcg_10 0.6143985986709595 Val ndcg_10 0.452306866645813\n",
            "[INFO] 2023-04-07 13:57:46 - Current:0.452306866645813 Best:0.45116546750068665\n",
            "[INFO] 2023-04-07 13:57:46 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:49 - Epoch : 22 Train loss: -0.6115359784203235 Val loss: -0.5742056165422712 Train ndcg_10 0.5965056419372559 Val ndcg_10 0.45330810546875\n",
            "[INFO] 2023-04-07 13:57:49 - Current:0.45330810546875 Best:0.452306866645813\n",
            "[INFO] 2023-04-07 13:57:49 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:52 - Epoch : 23 Train loss: -0.6092909799529084 Val loss: -0.5745141977355593 Train ndcg_10 0.6031578183174133 Val ndcg_10 0.45332440733909607\n",
            "[INFO] 2023-04-07 13:57:52 - Current:0.45332440733909607 Best:0.45330810546875\n",
            "[INFO] 2023-04-07 13:57:52 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:57:57 - Epoch : 24 Train loss: -0.6202701290677535 Val loss: -0.5748315288907005 Train ndcg_10 0.6030246615409851 Val ndcg_10 0.4536358416080475\n",
            "[INFO] 2023-04-07 13:57:57 - Current:0.4536358416080475 Best:0.45332440733909607\n",
            "[INFO] 2023-04-07 13:57:57 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:58:00 - Epoch : 25 Train loss: -0.6134237159226058 Val loss: -0.5751489202181498 Train ndcg_10 0.6170128583908081 Val ndcg_10 0.45374757051467896\n",
            "[INFO] 2023-04-07 13:58:00 - Current:0.45374757051467896 Best:0.4536358416080475\n",
            "[INFO] 2023-04-07 13:58:00 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:58:03 - Epoch : 26 Train loss: -0.6149861077410343 Val loss: -0.5754539966583252 Train ndcg_10 0.6150275468826294 Val ndcg_10 0.4541667401790619\n",
            "[INFO] 2023-04-07 13:58:03 - Current:0.4541667401790619 Best:0.45374757051467896\n",
            "[INFO] 2023-04-07 13:58:03 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:58:07 - Epoch : 27 Train loss: -0.6080335612255833 Val loss: -0.5757793381100609 Train ndcg_10 0.6187713742256165 Val ndcg_10 0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:07 - Current:0.4547170400619507 Best:0.4541667401790619\n",
            "[INFO] 2023-04-07 13:58:07 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:58:11 - Epoch : 28 Train loss: -0.6098836428493862 Val loss: -0.5760747989018759 Train ndcg_10 0.6076290607452393 Val ndcg_10 0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:11 - Current:0.4547170400619507 Best:0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:11 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 13:58:14 - Epoch : 29 Train loss: -0.6108555299060833 Val loss: -0.5763892395155771 Train ndcg_10 0.6216830611228943 Val ndcg_10 0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:14 - Current:0.4547170400619507 Best:0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:14 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:18 - Epoch : 30 Train loss: -0.6153771140389895 Val loss: -0.5764495747429984 Train ndcg_10 0.5944042205810547 Val ndcg_10 0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:18 - Current:0.4547170400619507 Best:0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:18 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:21 - Epoch : 31 Train loss: -0.6111117592806088 Val loss: -0.5765139545713153 Train ndcg_10 0.6117379665374756 Val ndcg_10 0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:21 - Current:0.4547170400619507 Best:0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:21 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:26 - Epoch : 32 Train loss: -0.6158802249933182 Val loss: -0.5765730488867987 Train ndcg_10 0.6208372116088867 Val ndcg_10 0.45496389269828796\n",
            "[INFO] 2023-04-07 13:58:26 - Current:0.45496389269828796 Best:0.4547170400619507\n",
            "[INFO] 2023-04-07 13:58:26 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:29 - Epoch : 33 Train loss: -0.6150674715165789 Val loss: -0.5766345728011358 Train ndcg_10 0.621690034866333 Val ndcg_10 0.45498746633529663\n",
            "[INFO] 2023-04-07 13:58:29 - Current:0.45498746633529663 Best:0.45496389269828796\n",
            "[INFO] 2023-04-07 13:58:29 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:32 - Epoch : 34 Train loss: -0.6156501804373793 Val loss: -0.5766946145466396 Train ndcg_10 0.6117231249809265 Val ndcg_10 0.4564606845378876\n",
            "[INFO] 2023-04-07 13:58:32 - Current:0.4564606845378876 Best:0.45498746633529663\n",
            "[INFO] 2023-04-07 13:58:32 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:36 - Epoch : 35 Train loss: -0.615681694976191 Val loss: -0.5767523731504168 Train ndcg_10 0.6082562208175659 Val ndcg_10 0.4572586417198181\n",
            "[INFO] 2023-04-07 13:58:36 - Current:0.4572586417198181 Best:0.4564606845378876\n",
            "[INFO] 2023-04-07 13:58:36 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:40 - Epoch : 36 Train loss: -0.6202972551931222 Val loss: -0.5768126606941223 Train ndcg_10 0.6334909796714783 Val ndcg_10 0.4571617841720581\n",
            "[INFO] 2023-04-07 13:58:40 - Current:0.4571617841720581 Best:0.4572586417198181\n",
            "[INFO] 2023-04-07 13:58:40 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:43 - Epoch : 37 Train loss: -0.6111923782557507 Val loss: -0.5768700894855318 Train ndcg_10 0.6044577360153198 Val ndcg_10 0.4570837616920471\n",
            "[INFO] 2023-04-07 13:58:43 - Current:0.4570837616920471 Best:0.4572586417198181\n",
            "[INFO] 2023-04-07 13:58:43 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:46 - Epoch : 38 Train loss: -0.6143483121044697 Val loss: -0.5769311354273842 Train ndcg_10 0.6055083870887756 Val ndcg_10 0.4571441411972046\n",
            "[INFO] 2023-04-07 13:58:46 - Current:0.4571441411972046 Best:0.4572586417198181\n",
            "[INFO] 2023-04-07 13:58:46 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:50 - Epoch : 39 Train loss: -0.610172588303041 Val loss: -0.5769919054848808 Train ndcg_10 0.6163052916526794 Val ndcg_10 0.45754319429397583\n",
            "[INFO] 2023-04-07 13:58:50 - Current:0.45754319429397583 Best:0.4572586417198181\n",
            "[INFO] 2023-04-07 13:58:50 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:53 - Epoch : 40 Train loss: -0.6123599288099437 Val loss: -0.5770510679199582 Train ndcg_10 0.6093227863311768 Val ndcg_10 0.45754319429397583\n",
            "[INFO] 2023-04-07 13:58:53 - Current:0.45754319429397583 Best:0.45754319429397583\n",
            "[INFO] 2023-04-07 13:58:53 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:58:57 - Epoch : 41 Train loss: -0.6094207227745386 Val loss: -0.5771147018387204 Train ndcg_10 0.6129197478294373 Val ndcg_10 0.45873525738716125\n",
            "[INFO] 2023-04-07 13:58:57 - Current:0.45873525738716125 Best:0.45754319429397583\n",
            "[INFO] 2023-04-07 13:58:57 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:59:00 - Epoch : 42 Train loss: -0.6090139765560799 Val loss: -0.577175684202285 Train ndcg_10 0.6136534810066223 Val ndcg_10 0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:00 - Current:0.4594399929046631 Best:0.45873525738716125\n",
            "[INFO] 2023-04-07 13:59:00 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:59:04 - Epoch : 43 Train loss: -0.6066941491808603 Val loss: -0.5772343079249064 Train ndcg_10 0.606446385383606 Val ndcg_10 0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:04 - Current:0.4594399929046631 Best:0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:04 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 13:59:08 - Epoch : 44 Train loss: -0.6113310019976811 Val loss: -0.5772953953061785 Train ndcg_10 0.6035178899765015 Val ndcg_10 0.45917290449142456\n",
            "[INFO] 2023-04-07 13:59:08 - Current:0.45917290449142456 Best:0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:08 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:59:11 - Epoch : 45 Train loss: -0.6140825956286889 Val loss: -0.5773076863515945 Train ndcg_10 0.6234315633773804 Val ndcg_10 0.45917290449142456\n",
            "[INFO] 2023-04-07 13:59:11 - Current:0.45917290449142456 Best:0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:11 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:59:14 - Epoch : 46 Train loss: -0.6139927190387627 Val loss: -0.5773201153391884 Train ndcg_10 0.6200493574142456 Val ndcg_10 0.45917290449142456\n",
            "[INFO] 2023-04-07 13:59:14 - Current:0.45917290449142456 Best:0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:14 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:59:18 - Epoch : 47 Train loss: -0.6131927645859182 Val loss: -0.5773324046816145 Train ndcg_10 0.61098712682724 Val ndcg_10 0.45917290449142456\n",
            "[INFO] 2023-04-07 13:59:18 - Current:0.45917290449142456 Best:0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:18 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:59:22 - Epoch : 48 Train loss: -0.612682006888156 Val loss: -0.5773445895739964 Train ndcg_10 0.6114332675933838 Val ndcg_10 0.45917290449142456\n",
            "[INFO] 2023-04-07 13:59:22 - Current:0.45917290449142456 Best:0.4594399929046631\n",
            "[INFO] 2023-04-07 13:59:22 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 13:59:25 - Epoch : 49 Train loss: -0.6091026565870565 Val loss: -0.5773570906548273 Train ndcg_10 0.6152603030204773 Val ndcg_10 0.45917290449142456\n",
            "[INFO] 2023-04-07 13:59:25 - Current:0.45917290449142456 Best:0.4594399929046631\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 13:59:27 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold4-listMLE', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold4-listMLE', config_path='config.json')\n",
            "[INFO] 2023-04-07 13:59:27 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold4/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='listMLE', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 13:59:27 - will execute cp config.json output/results/MQ2008-Fold4-listMLE/used_config.json\n",
            "[INFO] 2023-04-07 13:59:27 - exit_code = 0\n",
            "[INFO] 2023-04-07 13:59:27 - will load train data from MQ2008/Fold4/train.txt\n",
            "[INFO] 2023-04-07 13:59:27 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/train.txt'> and got x shape (6486, 46), y shape (6486,) and query_ids shape (6486,)\n",
            "[INFO] 2023-04-07 13:59:27 - loaded dataset with 330 queries\n",
            "[INFO] 2023-04-07 13:59:27 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 13:59:27 - train DS shape: [330, 119, 46]\n",
            "[INFO] 2023-04-07 13:59:27 - will load vali data from MQ2008/Fold4/vali.txt\n",
            "[INFO] 2023-04-07 13:59:28 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/vali.txt'> and got x shape (2994, 46), y shape (2994,) and query_ids shape (2994,)\n",
            "[INFO] 2023-04-07 13:59:28 - loaded dataset with 112 queries\n",
            "[INFO] 2023-04-07 13:59:28 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 13:59:28 - vali DS shape: [112, 121, 46]\n",
            "[INFO] 2023-04-07 13:59:28 - Will pad to the longest slate: 121\n",
            "[INFO] 2023-04-07 13:59:28 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 13:59:28 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 13:59:28 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 13:59:28 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:30 - Epoch : 0 Train loss: 12.762416735562411 Val loss: 75.2198508126395 Train ndcg_10 0.5958622097969055 Val ndcg_10 0.35582947731018066\n",
            "[INFO] 2023-04-07 13:59:30 - Current:0.35582947731018066 Best:0.0\n",
            "[INFO] 2023-04-07 13:59:30 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:34 - Epoch : 1 Train loss: 12.748687420469341 Val loss: 75.17614637102399 Train ndcg_10 0.5844240784645081 Val ndcg_10 0.36535003781318665\n",
            "[INFO] 2023-04-07 13:59:34 - Current:0.36535003781318665 Best:0.35582947731018066\n",
            "[INFO] 2023-04-07 13:59:34 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:37 - Epoch : 2 Train loss: 12.691119332747025 Val loss: 75.13005556379046 Train ndcg_10 0.5875923037528992 Val ndcg_10 0.3703225255012512\n",
            "[INFO] 2023-04-07 13:59:37 - Current:0.3703225255012512 Best:0.36535003781318665\n",
            "[INFO] 2023-04-07 13:59:37 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:40 - Epoch : 3 Train loss: 12.672296940196643 Val loss: 75.32771791730609 Train ndcg_10 0.588333010673523 Val ndcg_10 0.3754005432128906\n",
            "[INFO] 2023-04-07 13:59:40 - Current:0.3754005432128906 Best:0.3703225255012512\n",
            "[INFO] 2023-04-07 13:59:40 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:42 - Epoch : 4 Train loss: 12.70540714263916 Val loss: 75.0681539263044 Train ndcg_10 0.5925800204277039 Val ndcg_10 0.3850288987159729\n",
            "[INFO] 2023-04-07 13:59:42 - Current:0.3850288987159729 Best:0.3754005432128906\n",
            "[INFO] 2023-04-07 13:59:42 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:46 - Epoch : 5 Train loss: 12.654924196185487 Val loss: 75.04750224522182 Train ndcg_10 0.5992581248283386 Val ndcg_10 0.3927299380302429\n",
            "[INFO] 2023-04-07 13:59:46 - Current:0.3927299380302429 Best:0.3850288987159729\n",
            "[INFO] 2023-04-07 13:59:46 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:49 - Epoch : 6 Train loss: 12.706630498712713 Val loss: 75.07294736589704 Train ndcg_10 0.6006540656089783 Val ndcg_10 0.39847397804260254\n",
            "[INFO] 2023-04-07 13:59:49 - Current:0.39847397804260254 Best:0.3927299380302429\n",
            "[INFO] 2023-04-07 13:59:49 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:52 - Epoch : 7 Train loss: 12.720236292752352 Val loss: 75.04045486450195 Train ndcg_10 0.5992134213447571 Val ndcg_10 0.4037943184375763\n",
            "[INFO] 2023-04-07 13:59:52 - Current:0.4037943184375763 Best:0.39847397804260254\n",
            "[INFO] 2023-04-07 13:59:52 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:55 - Epoch : 8 Train loss: 12.661286845351711 Val loss: 74.95266614641461 Train ndcg_10 0.5945200324058533 Val ndcg_10 0.4120551645755768\n",
            "[INFO] 2023-04-07 13:59:55 - Current:0.4120551645755768 Best:0.4037943184375763\n",
            "[INFO] 2023-04-07 13:59:55 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 13:59:58 - Epoch : 9 Train loss: 12.6637393951416 Val loss: 75.02786363874164 Train ndcg_10 0.6106808185577393 Val ndcg_10 0.42020735144615173\n",
            "[INFO] 2023-04-07 13:59:58 - Current:0.42020735144615173 Best:0.4120551645755768\n",
            "[INFO] 2023-04-07 13:59:58 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:00:01 - Epoch : 10 Train loss: 12.626468854961974 Val loss: 75.01261029924665 Train ndcg_10 0.6337342858314514 Val ndcg_10 0.4215293824672699\n",
            "[INFO] 2023-04-07 14:00:01 - Current:0.4215293824672699 Best:0.42020735144615173\n",
            "[INFO] 2023-04-07 14:00:01 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:00:04 - Epoch : 11 Train loss: 12.61988200563373 Val loss: 74.94371468680245 Train ndcg_10 0.6249620318412781 Val ndcg_10 0.4277310073375702\n",
            "[INFO] 2023-04-07 14:00:04 - Current:0.4277310073375702 Best:0.4215293824672699\n",
            "[INFO] 2023-04-07 14:00:04 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:00:07 - Epoch : 12 Train loss: 12.606943529302423 Val loss: 74.91405596051898 Train ndcg_10 0.6355715394020081 Val ndcg_10 0.43137016892433167\n",
            "[INFO] 2023-04-07 14:00:07 - Current:0.43137016892433167 Best:0.4277310073375702\n",
            "[INFO] 2023-04-07 14:00:07 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:00:09 - Epoch : 13 Train loss: 12.61989364046039 Val loss: 75.00391660417829 Train ndcg_10 0.625836193561554 Val ndcg_10 0.4372543394565582\n",
            "[INFO] 2023-04-07 14:00:09 - Current:0.4372543394565582 Best:0.43137016892433167\n",
            "[INFO] 2023-04-07 14:00:09 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:00:12 - Epoch : 14 Train loss: 12.619630981214119 Val loss: 74.93279865809849 Train ndcg_10 0.6250085234642029 Val ndcg_10 0.444729745388031\n",
            "[INFO] 2023-04-07 14:00:12 - Current:0.444729745388031 Best:0.4372543394565582\n",
            "[INFO] 2023-04-07 14:00:12 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:16 - Epoch : 15 Train loss: 12.587468181956899 Val loss: 74.97829273768834 Train ndcg_10 0.6258267760276794 Val ndcg_10 0.44449493288993835\n",
            "[INFO] 2023-04-07 14:00:16 - Current:0.44449493288993835 Best:0.444729745388031\n",
            "[INFO] 2023-04-07 14:00:16 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:18 - Epoch : 16 Train loss: 12.616161421573523 Val loss: 74.96833310808454 Train ndcg_10 0.627601683139801 Val ndcg_10 0.4449596107006073\n",
            "[INFO] 2023-04-07 14:00:18 - Current:0.4449596107006073 Best:0.444729745388031\n",
            "[INFO] 2023-04-07 14:00:18 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:21 - Epoch : 17 Train loss: 12.611387050513065 Val loss: 74.95604269845145 Train ndcg_10 0.643864631652832 Val ndcg_10 0.445998877286911\n",
            "[INFO] 2023-04-07 14:00:21 - Current:0.445998877286911 Best:0.4449596107006073\n",
            "[INFO] 2023-04-07 14:00:21 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:24 - Epoch : 18 Train loss: 12.612132482817678 Val loss: 74.84968675885882 Train ndcg_10 0.6272497773170471 Val ndcg_10 0.4452926516532898\n",
            "[INFO] 2023-04-07 14:00:24 - Current:0.4452926516532898 Best:0.445998877286911\n",
            "[INFO] 2023-04-07 14:00:24 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:27 - Epoch : 19 Train loss: 12.592639622543798 Val loss: 74.95654405866351 Train ndcg_10 0.6321871876716614 Val ndcg_10 0.44632744789123535\n",
            "[INFO] 2023-04-07 14:00:27 - Current:0.44632744789123535 Best:0.445998877286911\n",
            "[INFO] 2023-04-07 14:00:27 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:30 - Epoch : 20 Train loss: 12.586982553655451 Val loss: 74.97248186383929 Train ndcg_10 0.6139644980430603 Val ndcg_10 0.447995662689209\n",
            "[INFO] 2023-04-07 14:00:30 - Current:0.447995662689209 Best:0.44632744789123535\n",
            "[INFO] 2023-04-07 14:00:30 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:33 - Epoch : 21 Train loss: 12.56668216820919 Val loss: 74.86262348720005 Train ndcg_10 0.6446242928504944 Val ndcg_10 0.44855087995529175\n",
            "[INFO] 2023-04-07 14:00:33 - Current:0.44855087995529175 Best:0.447995662689209\n",
            "[INFO] 2023-04-07 14:00:33 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:36 - Epoch : 22 Train loss: 12.593736856633967 Val loss: 74.92955616542271 Train ndcg_10 0.642792820930481 Val ndcg_10 0.4507058560848236\n",
            "[INFO] 2023-04-07 14:00:36 - Current:0.4507058560848236 Best:0.44855087995529175\n",
            "[INFO] 2023-04-07 14:00:36 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:39 - Epoch : 23 Train loss: 12.580029077240916 Val loss: 74.91251046316964 Train ndcg_10 0.6478066444396973 Val ndcg_10 0.4527091383934021\n",
            "[INFO] 2023-04-07 14:00:39 - Current:0.4527091383934021 Best:0.4507058560848236\n",
            "[INFO] 2023-04-07 14:00:39 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:43 - Epoch : 24 Train loss: 12.586945776505903 Val loss: 74.87993512834821 Train ndcg_10 0.6316835880279541 Val ndcg_10 0.4535179138183594\n",
            "[INFO] 2023-04-07 14:00:43 - Current:0.4535179138183594 Best:0.4527091383934021\n",
            "[INFO] 2023-04-07 14:00:43 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:45 - Epoch : 25 Train loss: 12.567677041256067 Val loss: 74.96358871459961 Train ndcg_10 0.6195386648178101 Val ndcg_10 0.4534894526004791\n",
            "[INFO] 2023-04-07 14:00:45 - Current:0.4534894526004791 Best:0.4535179138183594\n",
            "[INFO] 2023-04-07 14:00:45 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:48 - Epoch : 26 Train loss: 12.586310080325966 Val loss: 74.96532549176898 Train ndcg_10 0.6479109525680542 Val ndcg_10 0.4551837742328644\n",
            "[INFO] 2023-04-07 14:00:48 - Current:0.4551837742328644 Best:0.4535179138183594\n",
            "[INFO] 2023-04-07 14:00:48 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:51 - Epoch : 27 Train loss: 12.580855826175574 Val loss: 74.91695186070034 Train ndcg_10 0.648415744304657 Val ndcg_10 0.4556962847709656\n",
            "[INFO] 2023-04-07 14:00:51 - Current:0.4556962847709656 Best:0.4551837742328644\n",
            "[INFO] 2023-04-07 14:00:51 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:54 - Epoch : 28 Train loss: 12.609626007080077 Val loss: 74.82526561192104 Train ndcg_10 0.6322847604751587 Val ndcg_10 0.45680925250053406\n",
            "[INFO] 2023-04-07 14:00:54 - Current:0.45680925250053406 Best:0.4556962847709656\n",
            "[INFO] 2023-04-07 14:00:54 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:00:58 - Epoch : 29 Train loss: 12.569422230576023 Val loss: 74.92362976074219 Train ndcg_10 0.6386728286743164 Val ndcg_10 0.45930787920951843\n",
            "[INFO] 2023-04-07 14:00:58 - Current:0.45930787920951843 Best:0.45680925250053406\n",
            "[INFO] 2023-04-07 14:00:58 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:00 - Epoch : 30 Train loss: 12.569078728646943 Val loss: 74.89272744315011 Train ndcg_10 0.6387419700622559 Val ndcg_10 0.4577920138835907\n",
            "[INFO] 2023-04-07 14:01:00 - Current:0.4577920138835907 Best:0.45930787920951843\n",
            "[INFO] 2023-04-07 14:01:00 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:03 - Epoch : 31 Train loss: 12.550921440124512 Val loss: 74.94127546037946 Train ndcg_10 0.6275746822357178 Val ndcg_10 0.4577920138835907\n",
            "[INFO] 2023-04-07 14:01:03 - Current:0.4577920138835907 Best:0.45930787920951843\n",
            "[INFO] 2023-04-07 14:01:03 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:06 - Epoch : 32 Train loss: 12.564249576221814 Val loss: 74.91749627249581 Train ndcg_10 0.6438416242599487 Val ndcg_10 0.4572841227054596\n",
            "[INFO] 2023-04-07 14:01:06 - Current:0.4572841227054596 Best:0.45930787920951843\n",
            "[INFO] 2023-04-07 14:01:06 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:10 - Epoch : 33 Train loss: 12.585949429598722 Val loss: 74.89983476911273 Train ndcg_10 0.645629346370697 Val ndcg_10 0.4572993814945221\n",
            "[INFO] 2023-04-07 14:01:10 - Current:0.4572993814945221 Best:0.45930787920951843\n",
            "[INFO] 2023-04-07 14:01:10 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:13 - Epoch : 34 Train loss: 12.567241524205063 Val loss: 74.92736380440849 Train ndcg_10 0.6500345468521118 Val ndcg_10 0.4578133225440979\n",
            "[INFO] 2023-04-07 14:01:13 - Current:0.4578133225440979 Best:0.45930787920951843\n",
            "[INFO] 2023-04-07 14:01:13 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:15 - Epoch : 35 Train loss: 12.584467373472272 Val loss: 74.96053205217633 Train ndcg_10 0.6137974858283997 Val ndcg_10 0.460369735956192\n",
            "[INFO] 2023-04-07 14:01:15 - Current:0.460369735956192 Best:0.45930787920951843\n",
            "[INFO] 2023-04-07 14:01:15 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:18 - Epoch : 36 Train loss: 12.545671486132072 Val loss: 74.8919928414481 Train ndcg_10 0.6186433434486389 Val ndcg_10 0.460494726896286\n",
            "[INFO] 2023-04-07 14:01:18 - Current:0.460494726896286 Best:0.460369735956192\n",
            "[INFO] 2023-04-07 14:01:18 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:21 - Epoch : 37 Train loss: 12.520111812244762 Val loss: 74.92442321777344 Train ndcg_10 0.6414417028427124 Val ndcg_10 0.4605552852153778\n",
            "[INFO] 2023-04-07 14:01:21 - Current:0.4605552852153778 Best:0.460494726896286\n",
            "[INFO] 2023-04-07 14:01:21 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:25 - Epoch : 38 Train loss: 12.585857772827149 Val loss: 74.8960326058524 Train ndcg_10 0.6280704140663147 Val ndcg_10 0.4600687026977539\n",
            "[INFO] 2023-04-07 14:01:25 - Current:0.4600687026977539 Best:0.4605552852153778\n",
            "[INFO] 2023-04-07 14:01:25 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:27 - Epoch : 39 Train loss: 12.561024394179835 Val loss: 74.87696129935128 Train ndcg_10 0.6328905820846558 Val ndcg_10 0.4600687026977539\n",
            "[INFO] 2023-04-07 14:01:27 - Current:0.4600687026977539 Best:0.4605552852153778\n",
            "[INFO] 2023-04-07 14:01:27 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:30 - Epoch : 40 Train loss: 12.575454798611728 Val loss: 74.89345714024135 Train ndcg_10 0.6318880915641785 Val ndcg_10 0.4601114094257355\n",
            "[INFO] 2023-04-07 14:01:30 - Current:0.4601114094257355 Best:0.4605552852153778\n",
            "[INFO] 2023-04-07 14:01:30 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:33 - Epoch : 41 Train loss: 12.579306307705966 Val loss: 74.90401949201312 Train ndcg_10 0.6464861035346985 Val ndcg_10 0.460164874792099\n",
            "[INFO] 2023-04-07 14:01:33 - Current:0.460164874792099 Best:0.4605552852153778\n",
            "[INFO] 2023-04-07 14:01:33 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:37 - Epoch : 42 Train loss: 12.56280946442575 Val loss: 74.85944039481026 Train ndcg_10 0.6311789751052856 Val ndcg_10 0.4602859318256378\n",
            "[INFO] 2023-04-07 14:01:37 - Current:0.4602859318256378 Best:0.4605552852153778\n",
            "[INFO] 2023-04-07 14:01:37 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:39 - Epoch : 43 Train loss: 12.573711909669818 Val loss: 74.89432198660714 Train ndcg_10 0.6332318782806396 Val ndcg_10 0.4608345031738281\n",
            "[INFO] 2023-04-07 14:01:39 - Current:0.4608345031738281 Best:0.4605552852153778\n",
            "[INFO] 2023-04-07 14:01:39 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:01:42 - Epoch : 44 Train loss: 12.547943837714918 Val loss: 74.88114765712193 Train ndcg_10 0.6267036199569702 Val ndcg_10 0.460909366607666\n",
            "[INFO] 2023-04-07 14:01:42 - Current:0.460909366607666 Best:0.4608345031738281\n",
            "[INFO] 2023-04-07 14:01:42 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:01:45 - Epoch : 45 Train loss: 12.563433624036385 Val loss: 74.93792288643974 Train ndcg_10 0.6343440413475037 Val ndcg_10 0.460909366607666\n",
            "[INFO] 2023-04-07 14:01:45 - Current:0.460909366607666 Best:0.460909366607666\n",
            "[INFO] 2023-04-07 14:01:45 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:01:48 - Epoch : 46 Train loss: 12.581794507575758 Val loss: 74.91144616263253 Train ndcg_10 0.6376780867576599 Val ndcg_10 0.46174293756484985\n",
            "[INFO] 2023-04-07 14:01:48 - Current:0.46174293756484985 Best:0.460909366607666\n",
            "[INFO] 2023-04-07 14:01:48 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:01:52 - Epoch : 47 Train loss: 12.5613782304706 Val loss: 74.87322126116071 Train ndcg_10 0.6215959191322327 Val ndcg_10 0.46174293756484985\n",
            "[INFO] 2023-04-07 14:01:52 - Current:0.46174293756484985 Best:0.46174293756484985\n",
            "[INFO] 2023-04-07 14:01:52 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:01:55 - Epoch : 48 Train loss: 12.564888827006023 Val loss: 74.89364787510463 Train ndcg_10 0.6349902749061584 Val ndcg_10 0.46174293756484985\n",
            "[INFO] 2023-04-07 14:01:55 - Current:0.46174293756484985 Best:0.46174293756484985\n",
            "[INFO] 2023-04-07 14:01:55 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:01:58 - Epoch : 49 Train loss: 12.5667386488481 Val loss: 74.85687746320453 Train ndcg_10 0.6379901170730591 Val ndcg_10 0.46174293756484985\n",
            "[INFO] 2023-04-07 14:01:58 - Current:0.46174293756484985 Best:0.46174293756484985\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:02:00 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold4-ordinal', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold4-ordinal', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:02:00 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold4/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='ordinal', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:02:00 - will execute cp config.json output/results/MQ2008-Fold4-ordinal/used_config.json\n",
            "[INFO] 2023-04-07 14:02:00 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:02:00 - will load train data from MQ2008/Fold4/train.txt\n",
            "[INFO] 2023-04-07 14:02:00 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/train.txt'> and got x shape (6486, 46), y shape (6486,) and query_ids shape (6486,)\n",
            "[INFO] 2023-04-07 14:02:00 - loaded dataset with 330 queries\n",
            "[INFO] 2023-04-07 14:02:00 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:02:00 - train DS shape: [330, 119, 46]\n",
            "[INFO] 2023-04-07 14:02:00 - will load vali data from MQ2008/Fold4/vali.txt\n",
            "[INFO] 2023-04-07 14:02:00 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/vali.txt'> and got x shape (2994, 46), y shape (2994,) and query_ids shape (2994,)\n",
            "[INFO] 2023-04-07 14:02:00 - loaded dataset with 112 queries\n",
            "[INFO] 2023-04-07 14:02:00 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:02:00 - vali DS shape: [112, 121, 46]\n",
            "[INFO] 2023-04-07 14:02:00 - Will pad to the longest slate: 121\n",
            "[INFO] 2023-04-07 14:02:00 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:02:00 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:02:00 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:02:00 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: ordinal() missing 1 required positional argument: 'n'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:02:03 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold4-lambdaLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold4-lambdaLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:02:03 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold4/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='lambdaLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:02:03 - will execute cp config.json output/results/MQ2008-Fold4-lambdaLoss/used_config.json\n",
            "[INFO] 2023-04-07 14:02:03 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:02:03 - will load train data from MQ2008/Fold4/train.txt\n",
            "[INFO] 2023-04-07 14:02:03 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/train.txt'> and got x shape (6486, 46), y shape (6486,) and query_ids shape (6486,)\n",
            "[INFO] 2023-04-07 14:02:03 - loaded dataset with 330 queries\n",
            "[INFO] 2023-04-07 14:02:03 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:02:03 - train DS shape: [330, 119, 46]\n",
            "[INFO] 2023-04-07 14:02:03 - will load vali data from MQ2008/Fold4/vali.txt\n",
            "[INFO] 2023-04-07 14:02:04 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/vali.txt'> and got x shape (2994, 46), y shape (2994,) and query_ids shape (2994,)\n",
            "[INFO] 2023-04-07 14:02:04 - loaded dataset with 112 queries\n",
            "[INFO] 2023-04-07 14:02:04 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:02:04 - vali DS shape: [112, 121, 46]\n",
            "[INFO] 2023-04-07 14:02:04 - Will pad to the longest slate: 121\n",
            "[INFO] 2023-04-07 14:02:04 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:02:04 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:02:04 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:02:04 - Current learning rate: 0.001\n",
            "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in Log2Backward. Traceback of forward call that caused the error:\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/models/losses/lambdaLoss.py\", line 70, in lambdaLoss\n",
            "losses = torch.log2(weighted_probas)\n",
            "(Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
            "Variable._execution_engine.run_backward(\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 23, in loss_batch\n",
            "loss.backward()\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/tensor.py\", line 245, in backward\n",
            "torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 145, in backward\n",
            "Variable._execution_engine.run_backward(\n",
            "RuntimeError: Function 'Log2Backward' returned nan values in its 0th output.\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:02:06 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold4-approxNDCGLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:02:06 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold4/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='approxNDCGLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:02:06 - will execute cp config.json output/results/MQ2008-Fold4-approxNDCGLoss/used_config.json\n",
            "[INFO] 2023-04-07 14:02:06 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:02:06 - will load train data from MQ2008/Fold4/train.txt\n",
            "[INFO] 2023-04-07 14:02:06 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/train.txt'> and got x shape (6486, 46), y shape (6486,) and query_ids shape (6486,)\n",
            "[INFO] 2023-04-07 14:02:06 - loaded dataset with 330 queries\n",
            "[INFO] 2023-04-07 14:02:06 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:02:06 - train DS shape: [330, 119, 46]\n",
            "[INFO] 2023-04-07 14:02:06 - will load vali data from MQ2008/Fold4/vali.txt\n",
            "[INFO] 2023-04-07 14:02:06 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/vali.txt'> and got x shape (2994, 46), y shape (2994,) and query_ids shape (2994,)\n",
            "[INFO] 2023-04-07 14:02:06 - loaded dataset with 112 queries\n",
            "[INFO] 2023-04-07 14:02:06 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:02:06 - vali DS shape: [112, 121, 46]\n",
            "[INFO] 2023-04-07 14:02:06 - Will pad to the longest slate: 121\n",
            "[INFO] 2023-04-07 14:02:06 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:02:06 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:02:06 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:02:06 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:09 - Epoch : 0 Train loss: -0.4922926097205191 Val loss: -0.4460577837058476 Train ndcg_10 0.5614970326423645 Val ndcg_10 0.35415321588516235\n",
            "[INFO] 2023-04-07 14:02:09 - Current:0.35415321588516235 Best:0.0\n",
            "[INFO] 2023-04-07 14:02:09 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:11 - Epoch : 1 Train loss: -0.49132930040359496 Val loss: -0.4460798714842115 Train ndcg_10 0.5734653472900391 Val ndcg_10 0.3548312783241272\n",
            "[INFO] 2023-04-07 14:02:11 - Current:0.3548312783241272 Best:0.35415321588516235\n",
            "[INFO] 2023-04-07 14:02:11 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:14 - Epoch : 2 Train loss: -0.4922009897954536 Val loss: -0.4460997922079904 Train ndcg_10 0.5811307430267334 Val ndcg_10 0.3557124137878418\n",
            "[INFO] 2023-04-07 14:02:14 - Current:0.3557124137878418 Best:0.3548312783241272\n",
            "[INFO] 2023-04-07 14:02:14 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:18 - Epoch : 3 Train loss: -0.4885383983453115 Val loss: -0.44612095185688566 Train ndcg_10 0.5688803195953369 Val ndcg_10 0.35589393973350525\n",
            "[INFO] 2023-04-07 14:02:18 - Current:0.35589393973350525 Best:0.3557124137878418\n",
            "[INFO] 2023-04-07 14:02:18 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:21 - Epoch : 4 Train loss: -0.49180546113938994 Val loss: -0.4461434142930167 Train ndcg_10 0.5816547274589539 Val ndcg_10 0.35627099871635437\n",
            "[INFO] 2023-04-07 14:02:21 - Current:0.35627099871635437 Best:0.35589393973350525\n",
            "[INFO] 2023-04-07 14:02:21 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:23 - Epoch : 5 Train loss: -0.4917013195427981 Val loss: -0.4461643397808075 Train ndcg_10 0.5789309144020081 Val ndcg_10 0.35627099871635437\n",
            "[INFO] 2023-04-07 14:02:23 - Current:0.35627099871635437 Best:0.35627099871635437\n",
            "[INFO] 2023-04-07 14:02:23 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:26 - Epoch : 6 Train loss: -0.49370059714172826 Val loss: -0.44618495873042513 Train ndcg_10 0.5788026452064514 Val ndcg_10 0.35627099871635437\n",
            "[INFO] 2023-04-07 14:02:26 - Current:0.35627099871635437 Best:0.35627099871635437\n",
            "[INFO] 2023-04-07 14:02:26 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:29 - Epoch : 7 Train loss: -0.4901002775539051 Val loss: -0.4462054456983294 Train ndcg_10 0.5683286786079407 Val ndcg_10 0.35644981265068054\n",
            "[INFO] 2023-04-07 14:02:29 - Current:0.35644981265068054 Best:0.35627099871635437\n",
            "[INFO] 2023-04-07 14:02:29 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:33 - Epoch : 8 Train loss: -0.4911864248189059 Val loss: -0.44622631583895 Train ndcg_10 0.577319324016571 Val ndcg_10 0.3566597104072571\n",
            "[INFO] 2023-04-07 14:02:33 - Current:0.3566597104072571 Best:0.35644981265068054\n",
            "[INFO] 2023-04-07 14:02:33 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:35 - Epoch : 9 Train loss: -0.4897442590106617 Val loss: -0.4462479054927826 Train ndcg_10 0.5783820152282715 Val ndcg_10 0.35670140385627747\n",
            "[INFO] 2023-04-07 14:02:35 - Current:0.35670140385627747 Best:0.3566597104072571\n",
            "[INFO] 2023-04-07 14:02:35 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:38 - Epoch : 10 Train loss: -0.4929299939762462 Val loss: -0.44626798800059725 Train ndcg_10 0.5815832018852234 Val ndcg_10 0.3567453920841217\n",
            "[INFO] 2023-04-07 14:02:38 - Current:0.3567453920841217 Best:0.35670140385627747\n",
            "[INFO] 2023-04-07 14:02:38 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:40 - Epoch : 11 Train loss: -0.4915435642907114 Val loss: -0.446290237562997 Train ndcg_10 0.5745328664779663 Val ndcg_10 0.35697147250175476\n",
            "[INFO] 2023-04-07 14:02:40 - Current:0.35697147250175476 Best:0.3567453920841217\n",
            "[INFO] 2023-04-07 14:02:40 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:44 - Epoch : 12 Train loss: -0.49116194356571546 Val loss: -0.4463094728333609 Train ndcg_10 0.569524347782135 Val ndcg_10 0.35576072335243225\n",
            "[INFO] 2023-04-07 14:02:44 - Current:0.35576072335243225 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:02:44 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:47 - Epoch : 13 Train loss: -0.4925799684091048 Val loss: -0.4463298533643995 Train ndcg_10 0.5811473727226257 Val ndcg_10 0.3555998206138611\n",
            "[INFO] 2023-04-07 14:02:47 - Current:0.3555998206138611 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:02:47 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:02:50 - Epoch : 14 Train loss: -0.4903799223177361 Val loss: -0.44635198371750967 Train ndcg_10 0.5892190337181091 Val ndcg_10 0.35619476437568665\n",
            "[INFO] 2023-04-07 14:02:50 - Current:0.35619476437568665 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:02:50 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:02:52 - Epoch : 15 Train loss: -0.49243767586621373 Val loss: -0.4463562199047634 Train ndcg_10 0.5755975842475891 Val ndcg_10 0.35619476437568665\n",
            "[INFO] 2023-04-07 14:02:52 - Current:0.35619476437568665 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:02:52 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:02:55 - Epoch : 16 Train loss: -0.4931376861803459 Val loss: -0.44636022193091257 Train ndcg_10 0.5845464468002319 Val ndcg_10 0.35619476437568665\n",
            "[INFO] 2023-04-07 14:02:55 - Current:0.35619476437568665 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:02:55 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:02:59 - Epoch : 17 Train loss: -0.49512625029592805 Val loss: -0.4463644453457424 Train ndcg_10 0.5762519836425781 Val ndcg_10 0.35619476437568665\n",
            "[INFO] 2023-04-07 14:02:59 - Current:0.35619476437568665 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:02:59 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:03:02 - Epoch : 18 Train loss: -0.48922220902009445 Val loss: -0.446368864604405 Train ndcg_10 0.5801742076873779 Val ndcg_10 0.35619476437568665\n",
            "[INFO] 2023-04-07 14:03:02 - Current:0.35619476437568665 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:03:02 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:03:04 - Epoch : 19 Train loss: -0.4934294404405536 Val loss: -0.44637285811560495 Train ndcg_10 0.579467236995697 Val ndcg_10 0.35619476437568665\n",
            "[INFO] 2023-04-07 14:03:04 - Current:0.35619476437568665 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:03:04 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:03:07 - Epoch : 20 Train loss: -0.4909007845502911 Val loss: -0.4463772731167929 Train ndcg_10 0.5788924098014832 Val ndcg_10 0.3563331961631775\n",
            "[INFO] 2023-04-07 14:03:07 - Current:0.3563331961631775 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:03:07 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:03:10 - Epoch : 21 Train loss: -0.4942770369125135 Val loss: -0.4463816796030317 Train ndcg_10 0.5824283957481384 Val ndcg_10 0.3563331961631775\n",
            "[INFO] 2023-04-07 14:03:10 - Current:0.3563331961631775 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:03:10 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:03:14 - Epoch : 22 Train loss: -0.4892793637333494 Val loss: -0.44638541766575407 Train ndcg_10 0.5661346912384033 Val ndcg_10 0.3563331961631775\n",
            "[INFO] 2023-04-07 14:03:14 - Current:0.3563331961631775 Best:0.35697147250175476\n",
            "[INFO] 2023-04-07 14:03:14 - early stopping at epoch 22 since ndcg_10 didn't improve from epoch no 11. Best value 0.35697147250175476, current value 0.3563331961631775\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:03:16 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold4-pointwise_rmse', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold4-pointwise_rmse', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:03:16 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold4/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='pointwise_rmse', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:03:16 - will execute cp config.json output/results/MQ2008-Fold4-pointwise_rmse/used_config.json\n",
            "[INFO] 2023-04-07 14:03:16 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:03:16 - will load train data from MQ2008/Fold4/train.txt\n",
            "[INFO] 2023-04-07 14:03:16 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/train.txt'> and got x shape (6486, 46), y shape (6486,) and query_ids shape (6486,)\n",
            "[INFO] 2023-04-07 14:03:16 - loaded dataset with 330 queries\n",
            "[INFO] 2023-04-07 14:03:16 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:03:16 - train DS shape: [330, 119, 46]\n",
            "[INFO] 2023-04-07 14:03:16 - will load vali data from MQ2008/Fold4/vali.txt\n",
            "[INFO] 2023-04-07 14:03:16 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/vali.txt'> and got x shape (2994, 46), y shape (2994,) and query_ids shape (2994,)\n",
            "[INFO] 2023-04-07 14:03:16 - loaded dataset with 112 queries\n",
            "[INFO] 2023-04-07 14:03:16 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:03:16 - vali DS shape: [112, 121, 46]\n",
            "[INFO] 2023-04-07 14:03:16 - Will pad to the longest slate: 121\n",
            "[INFO] 2023-04-07 14:03:16 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:03:16 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:03:16 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:03:16 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: pointwise_rmse() missing 1 required positional argument: 'no_of_levels'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:03:18 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold4-neuralNDCG', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold4-neuralNDCG', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:03:18 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold4/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='neuralNDCG', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:03:18 - will execute cp config.json output/results/MQ2008-Fold4-neuralNDCG/used_config.json\n",
            "[INFO] 2023-04-07 14:03:18 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:03:18 - will load train data from MQ2008/Fold4/train.txt\n",
            "[INFO] 2023-04-07 14:03:18 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/train.txt'> and got x shape (6486, 46), y shape (6486,) and query_ids shape (6486,)\n",
            "[INFO] 2023-04-07 14:03:18 - loaded dataset with 330 queries\n",
            "[INFO] 2023-04-07 14:03:18 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:03:18 - train DS shape: [330, 119, 46]\n",
            "[INFO] 2023-04-07 14:03:18 - will load vali data from MQ2008/Fold4/vali.txt\n",
            "[INFO] 2023-04-07 14:03:18 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold4/vali.txt'> and got x shape (2994, 46), y shape (2994,) and query_ids shape (2994,)\n",
            "[INFO] 2023-04-07 14:03:18 - loaded dataset with 112 queries\n",
            "[INFO] 2023-04-07 14:03:18 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:03:18 - vali DS shape: [112, 121, 46]\n",
            "[INFO] 2023-04-07 14:03:18 - Will pad to the longest slate: 121\n",
            "[INFO] 2023-04-07 14:03:18 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:03:18 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:03:18 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:03:18 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:22 - Epoch : 0 Train loss: -0.5873120838945562 Val loss: -0.5231110113007682 Train ndcg_10 0.562783420085907 Val ndcg_10 0.35627099871635437\n",
            "[INFO] 2023-04-07 14:03:22 - Current:0.35627099871635437 Best:0.0\n",
            "[INFO] 2023-04-07 14:03:22 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:25 - Epoch : 1 Train loss: -0.5868308681430239 Val loss: -0.5245324969291687 Train ndcg_10 0.5772210359573364 Val ndcg_10 0.35663607716560364\n",
            "[INFO] 2023-04-07 14:03:25 - Current:0.35663607716560364 Best:0.35627099871635437\n",
            "[INFO] 2023-04-07 14:03:25 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:29 - Epoch : 2 Train loss: -0.5912910064061483 Val loss: -0.5259178621428353 Train ndcg_10 0.5848057866096497 Val ndcg_10 0.35922160744667053\n",
            "[INFO] 2023-04-07 14:03:29 - Current:0.35922160744667053 Best:0.35663607716560364\n",
            "[INFO] 2023-04-07 14:03:29 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:33 - Epoch : 3 Train loss: -0.588454870744185 Val loss: -0.5273467387471881 Train ndcg_10 0.5786476135253906 Val ndcg_10 0.3704696297645569\n",
            "[INFO] 2023-04-07 14:03:33 - Current:0.3704696297645569 Best:0.35922160744667053\n",
            "[INFO] 2023-04-07 14:03:33 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:36 - Epoch : 4 Train loss: -0.5889544887976212 Val loss: -0.5288407376834324 Train ndcg_10 0.592431902885437 Val ndcg_10 0.37333086133003235\n",
            "[INFO] 2023-04-07 14:03:36 - Current:0.37333086133003235 Best:0.3704696297645569\n",
            "[INFO] 2023-04-07 14:03:36 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:40 - Epoch : 5 Train loss: -0.5903462370236715 Val loss: -0.5301955257143293 Train ndcg_10 0.5869428515434265 Val ndcg_10 0.3750678598880768\n",
            "[INFO] 2023-04-07 14:03:40 - Current:0.3750678598880768 Best:0.37333086133003235\n",
            "[INFO] 2023-04-07 14:03:40 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:44 - Epoch : 6 Train loss: -0.5975963339661107 Val loss: -0.5315979463713509 Train ndcg_10 0.5917546153068542 Val ndcg_10 0.38064655661582947\n",
            "[INFO] 2023-04-07 14:03:44 - Current:0.38064655661582947 Best:0.3750678598880768\n",
            "[INFO] 2023-04-07 14:03:44 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:47 - Epoch : 7 Train loss: -0.5970617507443283 Val loss: -0.533062219619751 Train ndcg_10 0.5776669979095459 Val ndcg_10 0.39011216163635254\n",
            "[INFO] 2023-04-07 14:03:47 - Current:0.39011216163635254 Best:0.38064655661582947\n",
            "[INFO] 2023-04-07 14:03:47 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:51 - Epoch : 8 Train loss: -0.5949394276647857 Val loss: -0.5344059552465167 Train ndcg_10 0.5989108681678772 Val ndcg_10 0.3927341103553772\n",
            "[INFO] 2023-04-07 14:03:51 - Current:0.3927341103553772 Best:0.39011216163635254\n",
            "[INFO] 2023-04-07 14:03:51 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:55 - Epoch : 9 Train loss: -0.5941909446860805 Val loss: -0.535822434084756 Train ndcg_10 0.5980625748634338 Val ndcg_10 0.39684733748435974\n",
            "[INFO] 2023-04-07 14:03:55 - Current:0.39684733748435974 Best:0.3927341103553772\n",
            "[INFO] 2023-04-07 14:03:55 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:03:59 - Epoch : 10 Train loss: -0.6018898118625987 Val loss: -0.5372016600200108 Train ndcg_10 0.6043097972869873 Val ndcg_10 0.4063989818096161\n",
            "[INFO] 2023-04-07 14:03:59 - Current:0.4063989818096161 Best:0.39684733748435974\n",
            "[INFO] 2023-04-07 14:03:59 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:04:02 - Epoch : 11 Train loss: -0.6036097638534778 Val loss: -0.5387498480933053 Train ndcg_10 0.5974204540252686 Val ndcg_10 0.4028986990451813\n",
            "[INFO] 2023-04-07 14:04:02 - Current:0.4028986990451813 Best:0.4063989818096161\n",
            "[INFO] 2023-04-07 14:04:02 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:04:05 - Epoch : 12 Train loss: -0.601033273971442 Val loss: -0.5400922639029366 Train ndcg_10 0.5854455232620239 Val ndcg_10 0.40587085485458374\n",
            "[INFO] 2023-04-07 14:04:05 - Current:0.40587085485458374 Best:0.4063989818096161\n",
            "[INFO] 2023-04-07 14:04:05 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:04:10 - Epoch : 13 Train loss: -0.6041505026094841 Val loss: -0.5415800128664289 Train ndcg_10 0.6061004400253296 Val ndcg_10 0.40726929903030396\n",
            "[INFO] 2023-04-07 14:04:10 - Current:0.40726929903030396 Best:0.4063989818096161\n",
            "[INFO] 2023-04-07 14:04:10 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:04:13 - Epoch : 14 Train loss: -0.6050740827213634 Val loss: -0.5430390664509365 Train ndcg_10 0.6140655875205994 Val ndcg_10 0.4076007902622223\n",
            "[INFO] 2023-04-07 14:04:13 - Current:0.4076007902622223 Best:0.40726929903030396\n",
            "[INFO] 2023-04-07 14:04:13 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:17 - Epoch : 15 Train loss: -0.6049772974216576 Val loss: -0.5433274677821568 Train ndcg_10 0.6060397624969482 Val ndcg_10 0.4081355035305023\n",
            "[INFO] 2023-04-07 14:04:17 - Current:0.4081355035305023 Best:0.4076007902622223\n",
            "[INFO] 2023-04-07 14:04:17 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:21 - Epoch : 16 Train loss: -0.6067672758391409 Val loss: -0.5435965061187744 Train ndcg_10 0.6137832403182983 Val ndcg_10 0.40820345282554626\n",
            "[INFO] 2023-04-07 14:04:21 - Current:0.40820345282554626 Best:0.4081355035305023\n",
            "[INFO] 2023-04-07 14:04:21 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:25 - Epoch : 17 Train loss: -0.6098096175627274 Val loss: -0.5438902378082275 Train ndcg_10 0.6055791974067688 Val ndcg_10 0.40742260217666626\n",
            "[INFO] 2023-04-07 14:04:25 - Current:0.40742260217666626 Best:0.40820345282554626\n",
            "[INFO] 2023-04-07 14:04:25 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:28 - Epoch : 18 Train loss: -0.6038055947332671 Val loss: -0.5441986918449402 Train ndcg_10 0.6095008850097656 Val ndcg_10 0.40868210792541504\n",
            "[INFO] 2023-04-07 14:04:28 - Current:0.40868210792541504 Best:0.40820345282554626\n",
            "[INFO] 2023-04-07 14:04:28 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:32 - Epoch : 19 Train loss: -0.6079324079282356 Val loss: -0.5444708977426801 Train ndcg_10 0.6099350452423096 Val ndcg_10 0.4090351164340973\n",
            "[INFO] 2023-04-07 14:04:32 - Current:0.4090351164340973 Best:0.40868210792541504\n",
            "[INFO] 2023-04-07 14:04:32 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:36 - Epoch : 20 Train loss: -0.6094687823093299 Val loss: -0.5447746004377093 Train ndcg_10 0.6062833070755005 Val ndcg_10 0.4093713164329529\n",
            "[INFO] 2023-04-07 14:04:36 - Current:0.4093713164329529 Best:0.4090351164340973\n",
            "[INFO] 2023-04-07 14:04:36 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:39 - Epoch : 21 Train loss: -0.6086170944300565 Val loss: -0.545079163142613 Train ndcg_10 0.6152241230010986 Val ndcg_10 0.41051191091537476\n",
            "[INFO] 2023-04-07 14:04:39 - Current:0.41051191091537476 Best:0.4093713164329529\n",
            "[INFO] 2023-04-07 14:04:39 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:43 - Epoch : 22 Train loss: -0.599676929098187 Val loss: -0.5453398653439113 Train ndcg_10 0.5932936072349548 Val ndcg_10 0.4115960896015167\n",
            "[INFO] 2023-04-07 14:04:43 - Current:0.4115960896015167 Best:0.41051191091537476\n",
            "[INFO] 2023-04-07 14:04:43 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:46 - Epoch : 23 Train loss: -0.6058077974752947 Val loss: -0.5456316471099854 Train ndcg_10 0.6120854616165161 Val ndcg_10 0.4122784435749054\n",
            "[INFO] 2023-04-07 14:04:46 - Current:0.4122784435749054 Best:0.4115960896015167\n",
            "[INFO] 2023-04-07 14:04:46 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:51 - Epoch : 24 Train loss: -0.605131371454759 Val loss: -0.545927916254316 Train ndcg_10 0.6039113998413086 Val ndcg_10 0.4123815596103668\n",
            "[INFO] 2023-04-07 14:04:51 - Current:0.4123815596103668 Best:0.4122784435749054\n",
            "[INFO] 2023-04-07 14:04:51 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:54 - Epoch : 25 Train loss: -0.6109610441959266 Val loss: -0.5462173649242946 Train ndcg_10 0.6085072159767151 Val ndcg_10 0.41241613030433655\n",
            "[INFO] 2023-04-07 14:04:54 - Current:0.41241613030433655 Best:0.4123815596103668\n",
            "[INFO] 2023-04-07 14:04:54 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:04:58 - Epoch : 26 Train loss: -0.6017109885360256 Val loss: -0.5465113094874791 Train ndcg_10 0.6128116250038147 Val ndcg_10 0.41267508268356323\n",
            "[INFO] 2023-04-07 14:04:58 - Current:0.41267508268356323 Best:0.41241613030433655\n",
            "[INFO] 2023-04-07 14:04:58 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:05:01 - Epoch : 27 Train loss: -0.5982946966633652 Val loss: -0.5468232546533857 Train ndcg_10 0.6057538390159607 Val ndcg_10 0.4162398874759674\n",
            "[INFO] 2023-04-07 14:05:01 - Current:0.4162398874759674 Best:0.41267508268356323\n",
            "[INFO] 2023-04-07 14:05:01 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:05:06 - Epoch : 28 Train loss: -0.6052186723911401 Val loss: -0.5471377202442714 Train ndcg_10 0.6224406361579895 Val ndcg_10 0.41972726583480835\n",
            "[INFO] 2023-04-07 14:05:06 - Current:0.41972726583480835 Best:0.4162398874759674\n",
            "[INFO] 2023-04-07 14:05:06 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:05:09 - Epoch : 29 Train loss: -0.6075104883222869 Val loss: -0.5474051066807338 Train ndcg_10 0.5997003316879272 Val ndcg_10 0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:09 - Current:0.42039865255355835 Best:0.41972726583480835\n",
            "[INFO] 2023-04-07 14:05:09 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:12 - Epoch : 30 Train loss: -0.6142072435581323 Val loss: -0.5474652562822614 Train ndcg_10 0.6049210429191589 Val ndcg_10 0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:12 - Current:0.42039865255355835 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:12 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:16 - Epoch : 31 Train loss: -0.6065693241177184 Val loss: -0.5475193347249713 Train ndcg_10 0.5940208435058594 Val ndcg_10 0.42033571004867554\n",
            "[INFO] 2023-04-07 14:05:16 - Current:0.42033571004867554 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:16 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:20 - Epoch : 32 Train loss: -0.6079665845090693 Val loss: -0.5475795779909406 Train ndcg_10 0.6057820916175842 Val ndcg_10 0.42033571004867554\n",
            "[INFO] 2023-04-07 14:05:20 - Current:0.42033571004867554 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:20 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:24 - Epoch : 33 Train loss: -0.6081666458736766 Val loss: -0.5476376329149518 Train ndcg_10 0.5986390709877014 Val ndcg_10 0.4203762412071228\n",
            "[INFO] 2023-04-07 14:05:24 - Current:0.4203762412071228 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:24 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:27 - Epoch : 34 Train loss: -0.6097188017585061 Val loss: -0.547697126865387 Train ndcg_10 0.611754834651947 Val ndcg_10 0.4203762412071228\n",
            "[INFO] 2023-04-07 14:05:27 - Current:0.4203762412071228 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:27 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:32 - Epoch : 35 Train loss: -0.6066398732589953 Val loss: -0.5477599586759295 Train ndcg_10 0.618939995765686 Val ndcg_10 0.4203762412071228\n",
            "[INFO] 2023-04-07 14:05:32 - Current:0.4203762412071228 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:32 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:35 - Epoch : 36 Train loss: -0.6066684256900441 Val loss: -0.5478166512080601 Train ndcg_10 0.6143356561660767 Val ndcg_10 0.4203762412071228\n",
            "[INFO] 2023-04-07 14:05:35 - Current:0.4203762412071228 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:35 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:38 - Epoch : 37 Train loss: -0.6100225986856402 Val loss: -0.5478722112519401 Train ndcg_10 0.6025539040565491 Val ndcg_10 0.4205573499202728\n",
            "[INFO] 2023-04-07 14:05:38 - Current:0.4205573499202728 Best:0.42039865255355835\n",
            "[INFO] 2023-04-07 14:05:38 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:42 - Epoch : 38 Train loss: -0.6103422793475064 Val loss: -0.5479296616145543 Train ndcg_10 0.6064537167549133 Val ndcg_10 0.4205573499202728\n",
            "[INFO] 2023-04-07 14:05:42 - Current:0.4205573499202728 Best:0.4205573499202728\n",
            "[INFO] 2023-04-07 14:05:42 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:46 - Epoch : 39 Train loss: -0.6040505763256189 Val loss: -0.547988942691258 Train ndcg_10 0.6006476283073425 Val ndcg_10 0.4205755591392517\n",
            "[INFO] 2023-04-07 14:05:46 - Current:0.4205755591392517 Best:0.4205573499202728\n",
            "[INFO] 2023-04-07 14:05:46 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:50 - Epoch : 40 Train loss: -0.6092212211002003 Val loss: -0.5480488027845111 Train ndcg_10 0.6074584126472473 Val ndcg_10 0.4205755591392517\n",
            "[INFO] 2023-04-07 14:05:50 - Current:0.4205755591392517 Best:0.4205755591392517\n",
            "[INFO] 2023-04-07 14:05:50 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:53 - Epoch : 41 Train loss: -0.6103962760983092 Val loss: -0.5481043883732387 Train ndcg_10 0.5963582992553711 Val ndcg_10 0.4205755591392517\n",
            "[INFO] 2023-04-07 14:05:53 - Current:0.4205755591392517 Best:0.4205755591392517\n",
            "[INFO] 2023-04-07 14:05:53 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:05:56 - Epoch : 42 Train loss: -0.6072511514027913 Val loss: -0.5481611149651664 Train ndcg_10 0.611918032169342 Val ndcg_10 0.4205755591392517\n",
            "[INFO] 2023-04-07 14:05:56 - Current:0.4205755591392517 Best:0.4205755591392517\n",
            "[INFO] 2023-04-07 14:05:56 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:06:01 - Epoch : 43 Train loss: -0.6091381484811956 Val loss: -0.5482209239687238 Train ndcg_10 0.6070277094841003 Val ndcg_10 0.42089134454727173\n",
            "[INFO] 2023-04-07 14:06:01 - Current:0.42089134454727173 Best:0.4205755591392517\n",
            "[INFO] 2023-04-07 14:06:01 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:06:04 - Epoch : 44 Train loss: -0.6057408144979766 Val loss: -0.5482790470123291 Train ndcg_10 0.6049156188964844 Val ndcg_10 0.42084285616874695\n",
            "[INFO] 2023-04-07 14:06:04 - Current:0.42084285616874695 Best:0.42089134454727173\n",
            "[INFO] 2023-04-07 14:06:04 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:06:08 - Epoch : 45 Train loss: -0.6098782048080906 Val loss: -0.5482912233897618 Train ndcg_10 0.5985473394393921 Val ndcg_10 0.42084285616874695\n",
            "[INFO] 2023-04-07 14:06:08 - Current:0.42084285616874695 Best:0.42089134454727173\n",
            "[INFO] 2023-04-07 14:06:08 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:06:11 - Epoch : 46 Train loss: -0.5998442628166892 Val loss: -0.5483024971825736 Train ndcg_10 0.6033862233161926 Val ndcg_10 0.42084285616874695\n",
            "[INFO] 2023-04-07 14:06:11 - Current:0.42084285616874695 Best:0.42089134454727173\n",
            "[INFO] 2023-04-07 14:06:11 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:06:15 - Epoch : 47 Train loss: -0.6122166398799781 Val loss: -0.5483138476099286 Train ndcg_10 0.6100032925605774 Val ndcg_10 0.42084285616874695\n",
            "[INFO] 2023-04-07 14:06:15 - Current:0.42084285616874695 Best:0.42089134454727173\n",
            "[INFO] 2023-04-07 14:06:15 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:06:19 - Epoch : 48 Train loss: -0.607839255621939 Val loss: -0.5483255812100002 Train ndcg_10 0.6087666153907776 Val ndcg_10 0.42084285616874695\n",
            "[INFO] 2023-04-07 14:06:19 - Current:0.42084285616874695 Best:0.42089134454727173\n",
            "[INFO] 2023-04-07 14:06:19 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:06:22 - Epoch : 49 Train loss: -0.6116306987675754 Val loss: -0.5483371445110866 Train ndcg_10 0.613849937915802 Val ndcg_10 0.42084285616874695\n",
            "[INFO] 2023-04-07 14:06:22 - Current:0.42084285616874695 Best:0.42089134454727173\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:06:25 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold5-listMLE', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold5-listMLE', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:06:25 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold5/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='listMLE', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:06:25 - will execute cp config.json output/results/MQ2008-Fold5-listMLE/used_config.json\n",
            "[INFO] 2023-04-07 14:06:25 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:06:25 - will load train data from MQ2008/Fold5/train.txt\n",
            "[INFO] 2023-04-07 14:06:25 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/train.txt'> and got x shape (7376, 46), y shape (7376,) and query_ids shape (7376,)\n",
            "[INFO] 2023-04-07 14:06:25 - loaded dataset with 322 queries\n",
            "[INFO] 2023-04-07 14:06:25 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:06:25 - train DS shape: [322, 121, 46]\n",
            "[INFO] 2023-04-07 14:06:25 - will load vali data from MQ2008/Fold5/vali.txt\n",
            "[INFO] 2023-04-07 14:06:25 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/vali.txt'> and got x shape (2622, 46), y shape (2622,) and query_ids shape (2622,)\n",
            "[INFO] 2023-04-07 14:06:25 - loaded dataset with 122 queries\n",
            "[INFO] 2023-04-07 14:06:25 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:06:25 - vali DS shape: [122, 119, 46]\n",
            "[INFO] 2023-04-07 14:06:25 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 14:06:25 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:06:25 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:06:25 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:06:25 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:29 - Epoch : 0 Train loss: 13.001817537390668 Val loss: 55.512339357469905 Train ndcg_10 0.5734253525733948 Val ndcg_10 0.4029025733470917\n",
            "[INFO] 2023-04-07 14:06:29 - Current:0.4029025733470917 Best:0.0\n",
            "[INFO] 2023-04-07 14:06:29 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:31 - Epoch : 1 Train loss: 12.942376498109805 Val loss: 55.57801368588307 Train ndcg_10 0.5734269022941589 Val ndcg_10 0.4081689715385437\n",
            "[INFO] 2023-04-07 14:06:31 - Current:0.4081689715385437 Best:0.4029025733470917\n",
            "[INFO] 2023-04-07 14:06:31 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:34 - Epoch : 2 Train loss: 12.970492706535765 Val loss: 55.57697808937948 Train ndcg_10 0.5635288953781128 Val ndcg_10 0.41049787402153015\n",
            "[INFO] 2023-04-07 14:06:34 - Current:0.41049787402153015 Best:0.4081689715385437\n",
            "[INFO] 2023-04-07 14:06:34 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:37 - Epoch : 3 Train loss: 12.980175509956313 Val loss: 55.54766332907755 Train ndcg_10 0.5822428464889526 Val ndcg_10 0.4176141023635864\n",
            "[INFO] 2023-04-07 14:06:37 - Current:0.4176141023635864 Best:0.41049787402153015\n",
            "[INFO] 2023-04-07 14:06:37 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:40 - Epoch : 4 Train loss: 12.927894888457304 Val loss: 55.5303837510406 Train ndcg_10 0.588165283203125 Val ndcg_10 0.4242563843727112\n",
            "[INFO] 2023-04-07 14:06:40 - Current:0.4242563843727112 Best:0.4176141023635864\n",
            "[INFO] 2023-04-07 14:06:40 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:43 - Epoch : 5 Train loss: 12.89648837924744 Val loss: 55.48460188068327 Train ndcg_10 0.5788694024085999 Val ndcg_10 0.4291723072528839\n",
            "[INFO] 2023-04-07 14:06:43 - Current:0.4291723072528839 Best:0.4242563843727112\n",
            "[INFO] 2023-04-07 14:06:43 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:46 - Epoch : 6 Train loss: 12.900196223525526 Val loss: 55.381252976714585 Train ndcg_10 0.5895017981529236 Val ndcg_10 0.43908652663230896\n",
            "[INFO] 2023-04-07 14:06:46 - Current:0.43908652663230896 Best:0.4291723072528839\n",
            "[INFO] 2023-04-07 14:06:46 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:49 - Epoch : 7 Train loss: 12.907195967917117 Val loss: 55.48970431968814 Train ndcg_10 0.5909174680709839 Val ndcg_10 0.4486303925514221\n",
            "[INFO] 2023-04-07 14:06:49 - Current:0.4486303925514221 Best:0.43908652663230896\n",
            "[INFO] 2023-04-07 14:06:49 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:52 - Epoch : 8 Train loss: 12.879723140171595 Val loss: 55.40380284043609 Train ndcg_10 0.6052237749099731 Val ndcg_10 0.4553857445716858\n",
            "[INFO] 2023-04-07 14:06:52 - Current:0.4553857445716858 Best:0.4486303925514221\n",
            "[INFO] 2023-04-07 14:06:52 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:55 - Epoch : 9 Train loss: 12.846269228443596 Val loss: 55.4378249371638 Train ndcg_10 0.6005967259407043 Val ndcg_10 0.46328461170196533\n",
            "[INFO] 2023-04-07 14:06:55 - Current:0.46328461170196533 Best:0.4553857445716858\n",
            "[INFO] 2023-04-07 14:06:55 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:06:58 - Epoch : 10 Train loss: 12.86492399251239 Val loss: 55.29228010333952 Train ndcg_10 0.6002376675605774 Val ndcg_10 0.46410420536994934\n",
            "[INFO] 2023-04-07 14:06:58 - Current:0.46410420536994934 Best:0.46328461170196533\n",
            "[INFO] 2023-04-07 14:06:58 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:07:01 - Epoch : 11 Train loss: 12.846063507269628 Val loss: 55.39953588266842 Train ndcg_10 0.5963140726089478 Val ndcg_10 0.46912533044815063\n",
            "[INFO] 2023-04-07 14:07:01 - Current:0.46912533044815063 Best:0.46410420536994934\n",
            "[INFO] 2023-04-07 14:07:01 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:07:03 - Epoch : 12 Train loss: 12.86193136843095 Val loss: 55.365187973272604 Train ndcg_10 0.6054064631462097 Val ndcg_10 0.47647857666015625\n",
            "[INFO] 2023-04-07 14:07:03 - Current:0.47647857666015625 Best:0.46912533044815063\n",
            "[INFO] 2023-04-07 14:07:03 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:07:07 - Epoch : 13 Train loss: 12.801822810439589 Val loss: 55.30744421286661 Train ndcg_10 0.6164231300354004 Val ndcg_10 0.48202601075172424\n",
            "[INFO] 2023-04-07 14:07:07 - Current:0.48202601075172424 Best:0.47647857666015625\n",
            "[INFO] 2023-04-07 14:07:07 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:07:10 - Epoch : 14 Train loss: 12.812876174168558 Val loss: 55.30416169713755 Train ndcg_10 0.6224476099014282 Val ndcg_10 0.4863131642341614\n",
            "[INFO] 2023-04-07 14:07:10 - Current:0.4863131642341614 Best:0.48202601075172424\n",
            "[INFO] 2023-04-07 14:07:10 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:13 - Epoch : 15 Train loss: 12.833030037257982 Val loss: 55.235827586689936 Train ndcg_10 0.6162524819374084 Val ndcg_10 0.4870121479034424\n",
            "[INFO] 2023-04-07 14:07:13 - Current:0.4870121479034424 Best:0.4863131642341614\n",
            "[INFO] 2023-04-07 14:07:13 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:15 - Epoch : 16 Train loss: 12.809409135617084 Val loss: 55.33894235579694 Train ndcg_10 0.6102092862129211 Val ndcg_10 0.49192866683006287\n",
            "[INFO] 2023-04-07 14:07:15 - Current:0.49192866683006287 Best:0.4870121479034424\n",
            "[INFO] 2023-04-07 14:07:15 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:18 - Epoch : 17 Train loss: 12.818361566673895 Val loss: 55.324512919441595 Train ndcg_10 0.6085585951805115 Val ndcg_10 0.4948180317878723\n",
            "[INFO] 2023-04-07 14:07:18 - Current:0.4948180317878723 Best:0.49192866683006287\n",
            "[INFO] 2023-04-07 14:07:18 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:22 - Epoch : 18 Train loss: 12.827711294896854 Val loss: 55.2963617043417 Train ndcg_10 0.6141564846038818 Val ndcg_10 0.4950333535671234\n",
            "[INFO] 2023-04-07 14:07:22 - Current:0.4950333535671234 Best:0.4948180317878723\n",
            "[INFO] 2023-04-07 14:07:22 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:25 - Epoch : 19 Train loss: 12.793462551898838 Val loss: 55.27988834068423 Train ndcg_10 0.6249040961265564 Val ndcg_10 0.4957091510295868\n",
            "[INFO] 2023-04-07 14:07:25 - Current:0.4957091510295868 Best:0.4950333535671234\n",
            "[INFO] 2023-04-07 14:07:25 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:27 - Epoch : 20 Train loss: 12.809017631578149 Val loss: 55.27898544561668 Train ndcg_10 0.6392594575881958 Val ndcg_10 0.49767234921455383\n",
            "[INFO] 2023-04-07 14:07:27 - Current:0.49767234921455383 Best:0.4957091510295868\n",
            "[INFO] 2023-04-07 14:07:27 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:30 - Epoch : 21 Train loss: 12.830928541858745 Val loss: 55.28742505683274 Train ndcg_10 0.6103528141975403 Val ndcg_10 0.5004866123199463\n",
            "[INFO] 2023-04-07 14:07:30 - Current:0.5004866123199463 Best:0.49767234921455383\n",
            "[INFO] 2023-04-07 14:07:30 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:33 - Epoch : 22 Train loss: 12.767149315117308 Val loss: 55.295346494580876 Train ndcg_10 0.6045164465904236 Val ndcg_10 0.5021510720252991\n",
            "[INFO] 2023-04-07 14:07:33 - Current:0.5021510720252991 Best:0.5004866123199463\n",
            "[INFO] 2023-04-07 14:07:33 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:37 - Epoch : 23 Train loss: 12.806015606992734 Val loss: 55.32724724441278 Train ndcg_10 0.6336053013801575 Val ndcg_10 0.502161979675293\n",
            "[INFO] 2023-04-07 14:07:37 - Current:0.502161979675293 Best:0.5021510720252991\n",
            "[INFO] 2023-04-07 14:07:37 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:40 - Epoch : 24 Train loss: 12.788083704362004 Val loss: 55.3324812592053 Train ndcg_10 0.6051819324493408 Val ndcg_10 0.5028601884841919\n",
            "[INFO] 2023-04-07 14:07:40 - Current:0.5028601884841919 Best:0.502161979675293\n",
            "[INFO] 2023-04-07 14:07:40 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:43 - Epoch : 25 Train loss: 12.7975683863859 Val loss: 55.293074686019146 Train ndcg_10 0.6276190876960754 Val ndcg_10 0.5062564015388489\n",
            "[INFO] 2023-04-07 14:07:43 - Current:0.5062564015388489 Best:0.5028601884841919\n",
            "[INFO] 2023-04-07 14:07:43 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:45 - Epoch : 26 Train loss: 12.784300371726848 Val loss: 55.28915205158171 Train ndcg_10 0.6159841418266296 Val ndcg_10 0.5073080062866211\n",
            "[INFO] 2023-04-07 14:07:45 - Current:0.5073080062866211 Best:0.5062564015388489\n",
            "[INFO] 2023-04-07 14:07:45 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:49 - Epoch : 27 Train loss: 12.802112478647173 Val loss: 55.26414414702869 Train ndcg_10 0.6130948662757874 Val ndcg_10 0.5078742504119873\n",
            "[INFO] 2023-04-07 14:07:49 - Current:0.5078742504119873 Best:0.5073080062866211\n",
            "[INFO] 2023-04-07 14:07:49 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:52 - Epoch : 28 Train loss: 12.787482907312997 Val loss: 55.31668978831807 Train ndcg_10 0.625902533531189 Val ndcg_10 0.508709192276001\n",
            "[INFO] 2023-04-07 14:07:52 - Current:0.508709192276001 Best:0.5078742504119873\n",
            "[INFO] 2023-04-07 14:07:52 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:07:55 - Epoch : 29 Train loss: 12.797829006029213 Val loss: 55.30662186419377 Train ndcg_10 0.6282302141189575 Val ndcg_10 0.509341835975647\n",
            "[INFO] 2023-04-07 14:07:55 - Current:0.509341835975647 Best:0.508709192276001\n",
            "[INFO] 2023-04-07 14:07:55 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:07:57 - Epoch : 30 Train loss: 12.773205976308503 Val loss: 55.2042619048572 Train ndcg_10 0.6163795590400696 Val ndcg_10 0.509341835975647\n",
            "[INFO] 2023-04-07 14:07:57 - Current:0.509341835975647 Best:0.509341835975647\n",
            "[INFO] 2023-04-07 14:07:57 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:00 - Epoch : 31 Train loss: 12.81907528231603 Val loss: 55.29742769335137 Train ndcg_10 0.6191878318786621 Val ndcg_10 0.5092452168464661\n",
            "[INFO] 2023-04-07 14:08:00 - Current:0.5092452168464661 Best:0.509341835975647\n",
            "[INFO] 2023-04-07 14:08:00 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:04 - Epoch : 32 Train loss: 12.775989710173992 Val loss: 55.262925570128395 Train ndcg_10 0.6133505702018738 Val ndcg_10 0.5093467831611633\n",
            "[INFO] 2023-04-07 14:08:04 - Current:0.5093467831611633 Best:0.509341835975647\n",
            "[INFO] 2023-04-07 14:08:04 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:07 - Epoch : 33 Train loss: 12.80630712331452 Val loss: 55.32398961801998 Train ndcg_10 0.6235983371734619 Val ndcg_10 0.5093812346458435\n",
            "[INFO] 2023-04-07 14:08:07 - Current:0.5093812346458435 Best:0.5093467831611633\n",
            "[INFO] 2023-04-07 14:08:07 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:09 - Epoch : 34 Train loss: 12.810879837652172 Val loss: 55.28118296138576 Train ndcg_10 0.6198757290840149 Val ndcg_10 0.5092629194259644\n",
            "[INFO] 2023-04-07 14:08:09 - Current:0.5092629194259644 Best:0.5093812346458435\n",
            "[INFO] 2023-04-07 14:08:09 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:12 - Epoch : 35 Train loss: 12.797389788657242 Val loss: 55.28054446861392 Train ndcg_10 0.6114591956138611 Val ndcg_10 0.5093690752983093\n",
            "[INFO] 2023-04-07 14:08:12 - Current:0.5093690752983093 Best:0.5093812346458435\n",
            "[INFO] 2023-04-07 14:08:12 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:15 - Epoch : 36 Train loss: 12.813065060917635 Val loss: 55.300193036188844 Train ndcg_10 0.6378450393676758 Val ndcg_10 0.5088270306587219\n",
            "[INFO] 2023-04-07 14:08:15 - Current:0.5088270306587219 Best:0.5093812346458435\n",
            "[INFO] 2023-04-07 14:08:15 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:19 - Epoch : 37 Train loss: 12.798807369255872 Val loss: 55.29866177918481 Train ndcg_10 0.6269769072532654 Val ndcg_10 0.5090938806533813\n",
            "[INFO] 2023-04-07 14:08:19 - Current:0.5090938806533813 Best:0.5093812346458435\n",
            "[INFO] 2023-04-07 14:08:19 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:22 - Epoch : 38 Train loss: 12.794642525430051 Val loss: 55.29106602903272 Train ndcg_10 0.6375218033790588 Val ndcg_10 0.5094578266143799\n",
            "[INFO] 2023-04-07 14:08:22 - Current:0.5094578266143799 Best:0.5093812346458435\n",
            "[INFO] 2023-04-07 14:08:22 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:24 - Epoch : 39 Train loss: 12.803091534916659 Val loss: 55.30512750344198 Train ndcg_10 0.6215110421180725 Val ndcg_10 0.5094689726829529\n",
            "[INFO] 2023-04-07 14:08:24 - Current:0.5094689726829529 Best:0.5094578266143799\n",
            "[INFO] 2023-04-07 14:08:24 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:27 - Epoch : 40 Train loss: 12.819092365525524 Val loss: 55.262676176477655 Train ndcg_10 0.6184624433517456 Val ndcg_10 0.5096235871315002\n",
            "[INFO] 2023-04-07 14:08:27 - Current:0.5096235871315002 Best:0.5094689726829529\n",
            "[INFO] 2023-04-07 14:08:27 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:30 - Epoch : 41 Train loss: 12.796432424035872 Val loss: 55.27178905049308 Train ndcg_10 0.629411518573761 Val ndcg_10 0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:30 - Current:0.5096830129623413 Best:0.5096235871315002\n",
            "[INFO] 2023-04-07 14:08:30 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:33 - Epoch : 42 Train loss: 12.752955786189677 Val loss: 55.25977988321273 Train ndcg_10 0.6049667000770569 Val ndcg_10 0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:33 - Current:0.5096830129623413 Best:0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:33 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:36 - Epoch : 43 Train loss: 12.75036409034492 Val loss: 55.302193813636656 Train ndcg_10 0.6343544721603394 Val ndcg_10 0.5085421800613403\n",
            "[INFO] 2023-04-07 14:08:36 - Current:0.5085421800613403 Best:0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:36 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:08:39 - Epoch : 44 Train loss: 12.790483972300654 Val loss: 55.32253415467309 Train ndcg_10 0.6304674744606018 Val ndcg_10 0.5086750984191895\n",
            "[INFO] 2023-04-07 14:08:39 - Current:0.5086750984191895 Best:0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:39 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:08:41 - Epoch : 45 Train loss: 12.828367896701979 Val loss: 55.27202468621926 Train ndcg_10 0.6271823048591614 Val ndcg_10 0.5086750984191895\n",
            "[INFO] 2023-04-07 14:08:41 - Current:0.5086750984191895 Best:0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:41 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:08:45 - Epoch : 46 Train loss: 12.767204018113036 Val loss: 55.29203233562532 Train ndcg_10 0.6197026968002319 Val ndcg_10 0.508957028388977\n",
            "[INFO] 2023-04-07 14:08:45 - Current:0.508957028388977 Best:0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:45 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:08:48 - Epoch : 47 Train loss: 12.785357125797628 Val loss: 55.34862030529585 Train ndcg_10 0.625037431716919 Val ndcg_10 0.5089288353919983\n",
            "[INFO] 2023-04-07 14:08:48 - Current:0.5089288353919983 Best:0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:48 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:08:51 - Epoch : 48 Train loss: 12.816624511102711 Val loss: 55.30503645099577 Train ndcg_10 0.6081518530845642 Val ndcg_10 0.5104870200157166\n",
            "[INFO] 2023-04-07 14:08:51 - Current:0.5104870200157166 Best:0.5096830129623413\n",
            "[INFO] 2023-04-07 14:08:51 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:08:53 - Epoch : 49 Train loss: 12.79346161599485 Val loss: 55.26607388355693 Train ndcg_10 0.6185993552207947 Val ndcg_10 0.5105145573616028\n",
            "[INFO] 2023-04-07 14:08:53 - Current:0.5105145573616028 Best:0.5104870200157166\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:08:55 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold5-ordinal', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold5-ordinal', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:08:55 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold5/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='ordinal', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:08:55 - will execute cp config.json output/results/MQ2008-Fold5-ordinal/used_config.json\n",
            "[INFO] 2023-04-07 14:08:55 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:08:55 - will load train data from MQ2008/Fold5/train.txt\n",
            "[INFO] 2023-04-07 14:08:55 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/train.txt'> and got x shape (7376, 46), y shape (7376,) and query_ids shape (7376,)\n",
            "[INFO] 2023-04-07 14:08:55 - loaded dataset with 322 queries\n",
            "[INFO] 2023-04-07 14:08:55 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:08:55 - train DS shape: [322, 121, 46]\n",
            "[INFO] 2023-04-07 14:08:55 - will load vali data from MQ2008/Fold5/vali.txt\n",
            "[INFO] 2023-04-07 14:08:55 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/vali.txt'> and got x shape (2622, 46), y shape (2622,) and query_ids shape (2622,)\n",
            "[INFO] 2023-04-07 14:08:55 - loaded dataset with 122 queries\n",
            "[INFO] 2023-04-07 14:08:55 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:08:55 - vali DS shape: [122, 119, 46]\n",
            "[INFO] 2023-04-07 14:08:55 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 14:08:55 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:08:55 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:08:55 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:08:55 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: ordinal() missing 1 required positional argument: 'n'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:08:59 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold5-lambdaLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold5-lambdaLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:08:59 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold5/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='lambdaLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:08:59 - will execute cp config.json output/results/MQ2008-Fold5-lambdaLoss/used_config.json\n",
            "[INFO] 2023-04-07 14:08:59 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:08:59 - will load train data from MQ2008/Fold5/train.txt\n",
            "[INFO] 2023-04-07 14:08:59 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/train.txt'> and got x shape (7376, 46), y shape (7376,) and query_ids shape (7376,)\n",
            "[INFO] 2023-04-07 14:08:59 - loaded dataset with 322 queries\n",
            "[INFO] 2023-04-07 14:08:59 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:08:59 - train DS shape: [322, 121, 46]\n",
            "[INFO] 2023-04-07 14:08:59 - will load vali data from MQ2008/Fold5/vali.txt\n",
            "[INFO] 2023-04-07 14:08:59 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/vali.txt'> and got x shape (2622, 46), y shape (2622,) and query_ids shape (2622,)\n",
            "[INFO] 2023-04-07 14:08:59 - loaded dataset with 122 queries\n",
            "[INFO] 2023-04-07 14:08:59 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:08:59 - vali DS shape: [122, 119, 46]\n",
            "[INFO] 2023-04-07 14:08:59 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 14:08:59 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:08:59 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:08:59 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:08:59 - Current learning rate: 0.001\n",
            "/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py:145: UserWarning: Error detected in Log2Backward. Traceback of forward call that caused the error:\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/models/losses/lambdaLoss.py\", line 70, in lambdaLoss\n",
            "losses = torch.log2(weighted_probas)\n",
            "(Triggered internally at  /pytorch/torch/csrc/autograd/python_anomaly_mode.cpp:104.)\n",
            "Variable._execution_engine.run_backward(\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 23, in loss_batch\n",
            "loss.backward()\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/tensor.py\", line 245, in backward\n",
            "torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "File \"/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\", line 145, in backward\n",
            "Variable._execution_engine.run_backward(\n",
            "RuntimeError: Function 'Log2Backward' returned nan values in its 0th output.\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:09:01 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold5-approxNDCGLoss', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:09:01 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold5/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='approxNDCGLoss', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:09:01 - will execute cp config.json output/results/MQ2008-Fold5-approxNDCGLoss/used_config.json\n",
            "[INFO] 2023-04-07 14:09:01 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:09:01 - will load train data from MQ2008/Fold5/train.txt\n",
            "[INFO] 2023-04-07 14:09:01 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/train.txt'> and got x shape (7376, 46), y shape (7376,) and query_ids shape (7376,)\n",
            "[INFO] 2023-04-07 14:09:01 - loaded dataset with 322 queries\n",
            "[INFO] 2023-04-07 14:09:01 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:09:01 - train DS shape: [322, 121, 46]\n",
            "[INFO] 2023-04-07 14:09:01 - will load vali data from MQ2008/Fold5/vali.txt\n",
            "[INFO] 2023-04-07 14:09:01 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/vali.txt'> and got x shape (2622, 46), y shape (2622,) and query_ids shape (2622,)\n",
            "[INFO] 2023-04-07 14:09:01 - loaded dataset with 122 queries\n",
            "[INFO] 2023-04-07 14:09:01 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:09:01 - vali DS shape: [122, 119, 46]\n",
            "[INFO] 2023-04-07 14:09:01 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 14:09:01 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:09:01 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:09:01 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:09:01 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:04 - Epoch : 0 Train loss: -0.48769785306468516 Val loss: -0.47311481344895284 Train ndcg_10 0.5628173351287842 Val ndcg_10 0.39854696393013\n",
            "[INFO] 2023-04-07 14:09:04 - Current:0.39854696393013 Best:0.0\n",
            "[INFO] 2023-04-07 14:09:04 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:07 - Epoch : 1 Train loss: -0.4854477930883443 Val loss: -0.47313910087601085 Train ndcg_10 0.5686050057411194 Val ndcg_10 0.39859768748283386\n",
            "[INFO] 2023-04-07 14:09:07 - Current:0.39859768748283386 Best:0.39854696393013\n",
            "[INFO] 2023-04-07 14:09:07 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:09 - Epoch : 2 Train loss: -0.48520410616205345 Val loss: -0.4731673979368366 Train ndcg_10 0.5680254697799683 Val ndcg_10 0.3992374539375305\n",
            "[INFO] 2023-04-07 14:09:09 - Current:0.3992374539375305 Best:0.39859768748283386\n",
            "[INFO] 2023-04-07 14:09:09 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:13 - Epoch : 3 Train loss: -0.48772836064700015 Val loss: -0.47319197068448926 Train ndcg_10 0.5740569233894348 Val ndcg_10 0.40068307518959045\n",
            "[INFO] 2023-04-07 14:09:13 - Current:0.40068307518959045 Best:0.3992374539375305\n",
            "[INFO] 2023-04-07 14:09:13 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:16 - Epoch : 4 Train loss: -0.48694052033542845 Val loss: -0.4732193761184567 Train ndcg_10 0.5613769292831421 Val ndcg_10 0.40187838673591614\n",
            "[INFO] 2023-04-07 14:09:16 - Current:0.40187838673591614 Best:0.40068307518959045\n",
            "[INFO] 2023-04-07 14:09:16 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:19 - Epoch : 5 Train loss: -0.4872150669186752 Val loss: -0.47324981259517984 Train ndcg_10 0.5751762390136719 Val ndcg_10 0.40200263261795044\n",
            "[INFO] 2023-04-07 14:09:19 - Current:0.40200263261795044 Best:0.40187838673591614\n",
            "[INFO] 2023-04-07 14:09:19 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:21 - Epoch : 6 Train loss: -0.48897511785074793 Val loss: -0.4732786211811128 Train ndcg_10 0.5774482488632202 Val ndcg_10 0.402075856924057\n",
            "[INFO] 2023-04-07 14:09:21 - Current:0.402075856924057 Best:0.40200263261795044\n",
            "[INFO] 2023-04-07 14:09:21 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:25 - Epoch : 7 Train loss: -0.49003159666653745 Val loss: -0.4733020492264482 Train ndcg_10 0.5689198970794678 Val ndcg_10 0.40221861004829407\n",
            "[INFO] 2023-04-07 14:09:25 - Current:0.40221861004829407 Best:0.402075856924057\n",
            "[INFO] 2023-04-07 14:09:25 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:28 - Epoch : 8 Train loss: -0.4871373350575844 Val loss: -0.4733287223049852 Train ndcg_10 0.5823918581008911 Val ndcg_10 0.4021070599555969\n",
            "[INFO] 2023-04-07 14:09:28 - Current:0.4021070599555969 Best:0.40221861004829407\n",
            "[INFO] 2023-04-07 14:09:28 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:31 - Epoch : 9 Train loss: -0.48790535460347717 Val loss: -0.4733480864860972 Train ndcg_10 0.5644189715385437 Val ndcg_10 0.40219298005104065\n",
            "[INFO] 2023-04-07 14:09:31 - Current:0.40219298005104065 Best:0.40221861004829407\n",
            "[INFO] 2023-04-07 14:09:31 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:33 - Epoch : 10 Train loss: -0.48861417414979164 Val loss: -0.4733725664068441 Train ndcg_10 0.5608484745025635 Val ndcg_10 0.40318194031715393\n",
            "[INFO] 2023-04-07 14:09:33 - Current:0.40318194031715393 Best:0.40221861004829407\n",
            "[INFO] 2023-04-07 14:09:33 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:36 - Epoch : 11 Train loss: -0.4857692329779915 Val loss: -0.4733991935604908 Train ndcg_10 0.5653480887413025 Val ndcg_10 0.403068482875824\n",
            "[INFO] 2023-04-07 14:09:36 - Current:0.403068482875824 Best:0.40318194031715393\n",
            "[INFO] 2023-04-07 14:09:36 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:40 - Epoch : 12 Train loss: -0.4920502912183726 Val loss: -0.4734194171233255 Train ndcg_10 0.5498403310775757 Val ndcg_10 0.4032752215862274\n",
            "[INFO] 2023-04-07 14:09:40 - Current:0.4032752215862274 Best:0.40318194031715393\n",
            "[INFO] 2023-04-07 14:09:40 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:43 - Epoch : 13 Train loss: -0.4857996206105866 Val loss: -0.4734479819164901 Train ndcg_10 0.5719164609909058 Val ndcg_10 0.4033607244491577\n",
            "[INFO] 2023-04-07 14:09:43 - Current:0.4033607244491577 Best:0.4032752215862274\n",
            "[INFO] 2023-04-07 14:09:43 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:09:45 - Epoch : 14 Train loss: -0.4881037718020611 Val loss: -0.47347229230599325 Train ndcg_10 0.5830035209655762 Val ndcg_10 0.40363776683807373\n",
            "[INFO] 2023-04-07 14:09:45 - Current:0.40363776683807373 Best:0.4033607244491577\n",
            "[INFO] 2023-04-07 14:09:45 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:09:48 - Epoch : 15 Train loss: -0.4859670348789381 Val loss: -0.4734772849278372 Train ndcg_10 0.5723519921302795 Val ndcg_10 0.40363776683807373\n",
            "[INFO] 2023-04-07 14:09:48 - Current:0.40363776683807373 Best:0.40363776683807373\n",
            "[INFO] 2023-04-07 14:09:48 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:09:51 - Epoch : 16 Train loss: -0.4865411340449908 Val loss: -0.4734824900744391 Train ndcg_10 0.5772329568862915 Val ndcg_10 0.40366122126579285\n",
            "[INFO] 2023-04-07 14:09:51 - Current:0.40366122126579285 Best:0.40363776683807373\n",
            "[INFO] 2023-04-07 14:09:51 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:09:55 - Epoch : 17 Train loss: -0.488753040384802 Val loss: -0.4734873116993513 Train ndcg_10 0.5616846680641174 Val ndcg_10 0.4037555456161499\n",
            "[INFO] 2023-04-07 14:09:55 - Current:0.4037555456161499 Best:0.40366122126579285\n",
            "[INFO] 2023-04-07 14:09:55 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:09:58 - Epoch : 18 Train loss: -0.4890741269040552 Val loss: -0.4734924718981884 Train ndcg_10 0.5744052529335022 Val ndcg_10 0.40498611330986023\n",
            "[INFO] 2023-04-07 14:09:58 - Current:0.40498611330986023 Best:0.4037555456161499\n",
            "[INFO] 2023-04-07 14:09:58 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:00 - Epoch : 19 Train loss: -0.4913576256414378 Val loss: -0.4734965548163555 Train ndcg_10 0.5755456686019897 Val ndcg_10 0.40498611330986023\n",
            "[INFO] 2023-04-07 14:10:00 - Current:0.40498611330986023 Best:0.40498611330986023\n",
            "[INFO] 2023-04-07 14:10:00 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:03 - Epoch : 20 Train loss: -0.48624698123576476 Val loss: -0.4735026032221122 Train ndcg_10 0.572577714920044 Val ndcg_10 0.40498611330986023\n",
            "[INFO] 2023-04-07 14:10:03 - Current:0.40498611330986023 Best:0.40498611330986023\n",
            "[INFO] 2023-04-07 14:10:03 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:07 - Epoch : 21 Train loss: -0.48527096295208666 Val loss: -0.47350694947555416 Train ndcg_10 0.5635501742362976 Val ndcg_10 0.4050240218639374\n",
            "[INFO] 2023-04-07 14:10:07 - Current:0.4050240218639374 Best:0.40498611330986023\n",
            "[INFO] 2023-04-07 14:10:07 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:10 - Epoch : 22 Train loss: -0.485683464485666 Val loss: -0.4735118209338579 Train ndcg_10 0.5655742287635803 Val ndcg_10 0.4050240218639374\n",
            "[INFO] 2023-04-07 14:10:10 - Current:0.4050240218639374 Best:0.4050240218639374\n",
            "[INFO] 2023-04-07 14:10:10 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:14 - Epoch : 23 Train loss: -0.486743826673638 Val loss: -0.4735166621012766 Train ndcg_10 0.5633994936943054 Val ndcg_10 0.4050240218639374\n",
            "[INFO] 2023-04-07 14:10:14 - Current:0.4050240218639374 Best:0.4050240218639374\n",
            "[INFO] 2023-04-07 14:10:14 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:17 - Epoch : 24 Train loss: -0.48737420502656736 Val loss: -0.47352135523420863 Train ndcg_10 0.5769505500793457 Val ndcg_10 0.4052751958370209\n",
            "[INFO] 2023-04-07 14:10:17 - Current:0.4052751958370209 Best:0.4050240218639374\n",
            "[INFO] 2023-04-07 14:10:17 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:20 - Epoch : 25 Train loss: -0.48553789254301083 Val loss: -0.4735260385958875 Train ndcg_10 0.5717288851737976 Val ndcg_10 0.4052751958370209\n",
            "[INFO] 2023-04-07 14:10:20 - Current:0.4052751958370209 Best:0.4052751958370209\n",
            "[INFO] 2023-04-07 14:10:20 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:23 - Epoch : 26 Train loss: -0.4841528810699534 Val loss: -0.47353074443144877 Train ndcg_10 0.569927453994751 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:23 - Current:0.4053000807762146 Best:0.4052751958370209\n",
            "[INFO] 2023-04-07 14:10:23 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:26 - Epoch : 27 Train loss: -0.48845041020316365 Val loss: -0.47353572239641284 Train ndcg_10 0.570876955986023 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:26 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:26 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:29 - Epoch : 28 Train loss: -0.488924445202632 Val loss: -0.4735402117987148 Train ndcg_10 0.5637749433517456 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:29 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:29 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:10:31 - Epoch : 29 Train loss: -0.48435680133215386 Val loss: -0.4735456289815121 Train ndcg_10 0.5793945789337158 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:31 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:31 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:35 - Epoch : 30 Train loss: -0.4908361479362346 Val loss: -0.4735466061068363 Train ndcg_10 0.5702731013298035 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:35 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:35 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:38 - Epoch : 31 Train loss: -0.488341228191897 Val loss: -0.473547740060775 Train ndcg_10 0.5737344026565552 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:38 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:38 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:41 - Epoch : 32 Train loss: -0.48801603783731873 Val loss: -0.47354860921375086 Train ndcg_10 0.5754987001419067 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:41 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:41 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:43 - Epoch : 33 Train loss: -0.48770910205307955 Val loss: -0.47354963177540266 Train ndcg_10 0.5777199268341064 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:43 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:43 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:46 - Epoch : 34 Train loss: -0.4900055736858652 Val loss: -0.4735504222697899 Train ndcg_10 0.5873324871063232 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:46 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:46 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:50 - Epoch : 35 Train loss: -0.4875223347859353 Val loss: -0.47355145118275627 Train ndcg_10 0.5630674362182617 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:50 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:50 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:53 - Epoch : 36 Train loss: -0.4866481393760776 Val loss: -0.4735525411660554 Train ndcg_10 0.5621256232261658 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:53 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:53 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:10:56 - Epoch : 37 Train loss: -0.4895184886011278 Val loss: -0.47355362821797853 Train ndcg_10 0.5799639821052551 Val ndcg_10 0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:56 - Current:0.4053000807762146 Best:0.4053000807762146\n",
            "[INFO] 2023-04-07 14:10:56 - early stopping at epoch 37 since ndcg_10 didn't improve from epoch no 26. Best value 0.4053000807762146, current value 0.4053000807762146\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:10:57 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold5-pointwise_rmse', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold5-pointwise_rmse', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:10:57 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold5/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='pointwise_rmse', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:10:57 - will execute cp config.json output/results/MQ2008-Fold5-pointwise_rmse/used_config.json\n",
            "[INFO] 2023-04-07 14:10:57 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:10:57 - will load train data from MQ2008/Fold5/train.txt\n",
            "[INFO] 2023-04-07 14:10:58 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/train.txt'> and got x shape (7376, 46), y shape (7376,) and query_ids shape (7376,)\n",
            "[INFO] 2023-04-07 14:10:58 - loaded dataset with 322 queries\n",
            "[INFO] 2023-04-07 14:10:58 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:10:58 - train DS shape: [322, 121, 46]\n",
            "[INFO] 2023-04-07 14:10:58 - will load vali data from MQ2008/Fold5/vali.txt\n",
            "[INFO] 2023-04-07 14:10:58 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/vali.txt'> and got x shape (2622, 46), y shape (2622,) and query_ids shape (2622,)\n",
            "[INFO] 2023-04-07 14:10:58 - loaded dataset with 122 queries\n",
            "[INFO] 2023-04-07 14:10:58 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:10:58 - vali DS shape: [122, 119, 46]\n",
            "[INFO] 2023-04-07 14:10:58 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 14:10:58 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:10:58 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:10:58 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:10:58 - Current learning rate: 0.001\n",
            "Traceback (most recent call last):\n",
            "File \"/usr/local/bin/allRank\", line 8, in <module>\n",
            "sys.exit(run())\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/main.py\", line 91, in run\n",
            "result = fit(\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in fit\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 95, in <listcomp>\n",
            "*[loss_batch(model, loss_func, xb.to(device=device), yb.to(device=device), indices.to(device=device),\n",
            "File \"/usr/local/lib/python3.9/dist-packages/allrank/training/train_utils.py\", line 20, in loss_batch\n",
            "loss = loss_func(model(xb, mask, indices), yb)\n",
            "TypeError: pointwise_rmse() missing 1 required positional argument: 'no_of_levels'\n",
            "will read config from config.json\n",
            "[INFO] 2023-04-07 14:11:00 - created paths container PathsContainer(local_base_output_path='output', base_output_path='output', output_dir='output/results/MQ2008-Fold5-neuralNDCG', tensorboard_output_path='output/tb_evals/single/MQ2008-Fold5-neuralNDCG', config_path='config.json')\n",
            "[INFO] 2023-04-07 14:11:00 - Config:\n",
            "{'click_model': None,\n",
            "'data': DataConfig(path='MQ2008/Fold5/', num_workers=4, batch_size=32, slate_length=10, validation_ds_role='vali'),\n",
            "'detect_anomaly': True,\n",
            "'expected_metrics': {'val': {'ndcg_10': 0.25}},\n",
            "'loss': NameArgsConfig(name='neuralNDCG', args={}),\n",
            "'lr_scheduler': NameArgsConfig(name='StepLR', args={'step_size': 15, 'gamma': 0.2}),\n",
            "'metrics': defaultdict(<class 'list'>,\n",
            "{'ndcg': [10]}),\n",
            "'model': ModelConfig(fc_model={'sizes': [128, 64], 'input_norm': True, 'activation': 'ReLU', 'dropout': 0.1}, transformer=TransformerConfig(N=2, d_ff=128, h=8, positional_encoding=PositionalEncoding(strategy='fixed', max_indices=500), dropout=0.1), post_model={'output_activation': 'Sigmoid', 'd_output': 1}),\n",
            "'optimizer': NameArgsConfig(name='SGD', args={'lr': 0.001}),\n",
            "'training': TrainingConfig(epochs=50, gradient_clipping_norm=1.0, early_stopping_patience=10),\n",
            "'val_metric': 'ndcg_10'}\n",
            "[INFO] 2023-04-07 14:11:00 - will execute cp config.json output/results/MQ2008-Fold5-neuralNDCG/used_config.json\n",
            "[INFO] 2023-04-07 14:11:00 - exit_code = 0\n",
            "[INFO] 2023-04-07 14:11:00 - will load train data from MQ2008/Fold5/train.txt\n",
            "[INFO] 2023-04-07 14:11:01 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/train.txt'> and got x shape (7376, 46), y shape (7376,) and query_ids shape (7376,)\n",
            "[INFO] 2023-04-07 14:11:01 - loaded dataset with 322 queries\n",
            "[INFO] 2023-04-07 14:11:01 - longest query had 121 documents\n",
            "[INFO] 2023-04-07 14:11:01 - train DS shape: [322, 121, 46]\n",
            "[INFO] 2023-04-07 14:11:01 - will load vali data from MQ2008/Fold5/vali.txt\n",
            "[INFO] 2023-04-07 14:11:01 - loaded dataset from <_io.BufferedReader name='MQ2008/Fold5/vali.txt'> and got x shape (2622, 46), y shape (2622,) and query_ids shape (2622,)\n",
            "[INFO] 2023-04-07 14:11:01 - loaded dataset with 122 queries\n",
            "[INFO] 2023-04-07 14:11:01 - longest query had 119 documents\n",
            "[INFO] 2023-04-07 14:11:01 - vali DS shape: [122, 119, 46]\n",
            "[INFO] 2023-04-07 14:11:01 - Will pad to the longest slate: 119\n",
            "[INFO] 2023-04-07 14:11:01 - total batch size is 32\n",
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "warnings.warn(_create_warning_msg(\n",
            "[INFO] 2023-04-07 14:11:01 - Model training will execute on cpu\n",
            "/usr/local/lib/python3.9/dist-packages/allrank/main.py:89: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "with torch.autograd.detect_anomaly() if config.detect_anomaly else dummy_context_mgr():  # type: ignore\n",
            "[INFO] 2023-04-07 14:11:01 - Model has 81501 trainable parameters\n",
            "[INFO] 2023-04-07 14:11:01 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:05 - Epoch : 0 Train loss: -0.5766068163865842 Val loss: -0.5567337196381366 Train ndcg_10 0.5641841292381287 Val ndcg_10 0.4020519256591797\n",
            "[INFO] 2023-04-07 14:11:05 - Current:0.4020519256591797 Best:0.0\n",
            "[INFO] 2023-04-07 14:11:05 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:08 - Epoch : 1 Train loss: -0.5751090286681371 Val loss: -0.5582711071264549 Train ndcg_10 0.5725518465042114 Val ndcg_10 0.4031376540660858\n",
            "[INFO] 2023-04-07 14:11:08 - Current:0.4031376540660858 Best:0.4020519256591797\n",
            "[INFO] 2023-04-07 14:11:08 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:12 - Epoch : 2 Train loss: -0.5861922998606048 Val loss: -0.5600432558137862 Train ndcg_10 0.5710489749908447 Val ndcg_10 0.4072033166885376\n",
            "[INFO] 2023-04-07 14:11:12 - Current:0.4072033166885376 Best:0.4031376540660858\n",
            "[INFO] 2023-04-07 14:11:12 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:16 - Epoch : 3 Train loss: -0.5908363957582794 Val loss: -0.5618308968231326 Train ndcg_10 0.5799961686134338 Val ndcg_10 0.4097896218299866\n",
            "[INFO] 2023-04-07 14:11:16 - Current:0.4097896218299866 Best:0.4072033166885376\n",
            "[INFO] 2023-04-07 14:11:16 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:20 - Epoch : 4 Train loss: -0.5798685269326157 Val loss: -0.5637037177554897 Train ndcg_10 0.5709266662597656 Val ndcg_10 0.41202253103256226\n",
            "[INFO] 2023-04-07 14:11:20 - Current:0.41202253103256226 Best:0.4097896218299866\n",
            "[INFO] 2023-04-07 14:11:20 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:23 - Epoch : 5 Train loss: -0.5854734715467654 Val loss: -0.5657515574674137 Train ndcg_10 0.5858978629112244 Val ndcg_10 0.41415244340896606\n",
            "[INFO] 2023-04-07 14:11:23 - Current:0.41415244340896606 Best:0.41202253103256226\n",
            "[INFO] 2023-04-07 14:11:23 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:26 - Epoch : 6 Train loss: -0.5893965852186547 Val loss: -0.5676269355367441 Train ndcg_10 0.5937837958335876 Val ndcg_10 0.41840803623199463\n",
            "[INFO] 2023-04-07 14:11:26 - Current:0.41840803623199463 Best:0.41415244340896606\n",
            "[INFO] 2023-04-07 14:11:26 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:31 - Epoch : 7 Train loss: -0.5900758986147294 Val loss: -0.569319266764844 Train ndcg_10 0.5793275833129883 Val ndcg_10 0.4227878749370575\n",
            "[INFO] 2023-04-07 14:11:31 - Current:0.4227878749370575 Best:0.41840803623199463\n",
            "[INFO] 2023-04-07 14:11:31 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:34 - Epoch : 8 Train loss: -0.5911691285068204 Val loss: -0.5710982445810662 Train ndcg_10 0.5962817072868347 Val ndcg_10 0.42759597301483154\n",
            "[INFO] 2023-04-07 14:11:34 - Current:0.42759597301483154 Best:0.4227878749370575\n",
            "[INFO] 2023-04-07 14:11:34 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:38 - Epoch : 9 Train loss: -0.5895676609151852 Val loss: -0.5725809619074962 Train ndcg_10 0.5809889435768127 Val ndcg_10 0.4274827241897583\n",
            "[INFO] 2023-04-07 14:11:38 - Current:0.4274827241897583 Best:0.42759597301483154\n",
            "[INFO] 2023-04-07 14:11:38 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:41 - Epoch : 10 Train loss: -0.5985969545678322 Val loss: -0.5744241593313999 Train ndcg_10 0.5786445140838623 Val ndcg_10 0.4285229742527008\n",
            "[INFO] 2023-04-07 14:11:41 - Current:0.4285229742527008 Best:0.42759597301483154\n",
            "[INFO] 2023-04-07 14:11:41 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:46 - Epoch : 11 Train loss: -0.5921639363217798 Val loss: -0.5763565458235194 Train ndcg_10 0.5811153650283813 Val ndcg_10 0.4346992075443268\n",
            "[INFO] 2023-04-07 14:11:46 - Current:0.4346992075443268 Best:0.4285229742527008\n",
            "[INFO] 2023-04-07 14:11:46 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:49 - Epoch : 12 Train loss: -0.5987338768029065 Val loss: -0.5779162791908764 Train ndcg_10 0.5672948360443115 Val ndcg_10 0.43867725133895874\n",
            "[INFO] 2023-04-07 14:11:49 - Current:0.43867725133895874 Best:0.4346992075443268\n",
            "[INFO] 2023-04-07 14:11:49 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:53 - Epoch : 13 Train loss: -0.5907799279467659 Val loss: -0.579886369040755 Train ndcg_10 0.5975958108901978 Val ndcg_10 0.44298213720321655\n",
            "[INFO] 2023-04-07 14:11:53 - Current:0.44298213720321655 Best:0.43867725133895874\n",
            "[INFO] 2023-04-07 14:11:53 - Current learning rate: 0.001\n",
            "[INFO] 2023-04-07 14:11:57 - Epoch : 14 Train loss: -0.6011750350087326 Val loss: -0.5817399005420872 Train ndcg_10 0.6052597165107727 Val ndcg_10 0.44413936138153076\n",
            "[INFO] 2023-04-07 14:11:57 - Current:0.44413936138153076 Best:0.44298213720321655\n",
            "[INFO] 2023-04-07 14:11:57 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:01 - Epoch : 15 Train loss: -0.5959170834618326 Val loss: -0.5821051001548767 Train ndcg_10 0.599592387676239 Val ndcg_10 0.44607076048851013\n",
            "[INFO] 2023-04-07 14:12:01 - Current:0.44607076048851013 Best:0.44413936138153076\n",
            "[INFO] 2023-04-07 14:12:01 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:04 - Epoch : 16 Train loss: -0.5971732780059672 Val loss: -0.5824643402803139 Train ndcg_10 0.6109805703163147 Val ndcg_10 0.4463527202606201\n",
            "[INFO] 2023-04-07 14:12:04 - Current:0.4463527202606201 Best:0.44607076048851013\n",
            "[INFO] 2023-04-07 14:12:04 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:08 - Epoch : 17 Train loss: -0.6019133055432243 Val loss: -0.5828093016733888 Train ndcg_10 0.5877142548561096 Val ndcg_10 0.4472750723361969\n",
            "[INFO] 2023-04-07 14:12:08 - Current:0.4472750723361969 Best:0.4463527202606201\n",
            "[INFO] 2023-04-07 14:12:08 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:12 - Epoch : 18 Train loss: -0.5993701504624408 Val loss: -0.5831530895389494 Train ndcg_10 0.609809935092926 Val ndcg_10 0.4473699629306793\n",
            "[INFO] 2023-04-07 14:12:12 - Current:0.4473699629306793 Best:0.4472750723361969\n",
            "[INFO] 2023-04-07 14:12:12 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:16 - Epoch : 19 Train loss: -0.599513358210925 Val loss: -0.5834625851912577 Train ndcg_10 0.6049013137817383 Val ndcg_10 0.4485830068588257\n",
            "[INFO] 2023-04-07 14:12:16 - Current:0.4485830068588257 Best:0.4473699629306793\n",
            "[INFO] 2023-04-07 14:12:16 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:19 - Epoch : 20 Train loss: -0.5914452290683059 Val loss: -0.5838488184037756 Train ndcg_10 0.5983843803405762 Val ndcg_10 0.4491685926914215\n",
            "[INFO] 2023-04-07 14:12:19 - Current:0.4491685926914215 Best:0.4485830068588257\n",
            "[INFO] 2023-04-07 14:12:19 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:23 - Epoch : 21 Train loss: -0.5914317191757771 Val loss: -0.5841628645287186 Train ndcg_10 0.5910571217536926 Val ndcg_10 0.4493909180164337\n",
            "[INFO] 2023-04-07 14:12:23 - Current:0.4493909180164337 Best:0.4491685926914215\n",
            "[INFO] 2023-04-07 14:12:23 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:27 - Epoch : 22 Train loss: -0.5921170341302149 Val loss: -0.5845132010882018 Train ndcg_10 0.598793625831604 Val ndcg_10 0.44996726512908936\n",
            "[INFO] 2023-04-07 14:12:27 - Current:0.44996726512908936 Best:0.4493909180164337\n",
            "[INFO] 2023-04-07 14:12:27 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:31 - Epoch : 23 Train loss: -0.5940076120151496 Val loss: -0.5848476134362768 Train ndcg_10 0.5900033712387085 Val ndcg_10 0.4510273039340973\n",
            "[INFO] 2023-04-07 14:12:31 - Current:0.4510273039340973 Best:0.44996726512908936\n",
            "[INFO] 2023-04-07 14:12:31 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:34 - Epoch : 24 Train loss: -0.6004199570750598 Val loss: -0.5851744872624757 Train ndcg_10 0.5984195470809937 Val ndcg_10 0.4515874683856964\n",
            "[INFO] 2023-04-07 14:12:34 - Current:0.4515874683856964 Best:0.4510273039340973\n",
            "[INFO] 2023-04-07 14:12:34 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:38 - Epoch : 25 Train loss: -0.6003663321459516 Val loss: -0.58551019332448 Train ndcg_10 0.599628210067749 Val ndcg_10 0.45195645093917847\n",
            "[INFO] 2023-04-07 14:12:38 - Current:0.45195645093917847 Best:0.4515874683856964\n",
            "[INFO] 2023-04-07 14:12:38 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:42 - Epoch : 26 Train loss: -0.5954024325246396 Val loss: -0.5858463244360002 Train ndcg_10 0.5986924171447754 Val ndcg_10 0.45216700434684753\n",
            "[INFO] 2023-04-07 14:12:42 - Current:0.45216700434684753 Best:0.45195645093917847\n",
            "[INFO] 2023-04-07 14:12:42 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:46 - Epoch : 27 Train loss: -0.5959669247917507 Val loss: -0.586199224972334 Train ndcg_10 0.6047119498252869 Val ndcg_10 0.4516792595386505\n",
            "[INFO] 2023-04-07 14:12:46 - Current:0.4516792595386505 Best:0.45216700434684753\n",
            "[INFO] 2023-04-07 14:12:46 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:49 - Epoch : 28 Train loss: -0.6028968347525745 Val loss: -0.5865194729117097 Train ndcg_10 0.5948043465614319 Val ndcg_10 0.4526488184928894\n",
            "[INFO] 2023-04-07 14:12:49 - Current:0.4526488184928894 Best:0.45216700434684753\n",
            "[INFO] 2023-04-07 14:12:49 - Current learning rate: 0.0002\n",
            "[INFO] 2023-04-07 14:12:54 - Epoch : 29 Train loss: -0.5951062829598136 Val loss: -0.586892367386427 Train ndcg_10 0.6087440252304077 Val ndcg_10 0.4532036781311035\n",
            "[INFO] 2023-04-07 14:12:54 - Current:0.4532036781311035 Best:0.4526488184928894\n",
            "[INFO] 2023-04-07 14:12:54 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:12:57 - Epoch : 30 Train loss: -0.6078228839435933 Val loss: -0.5869584073785876 Train ndcg_10 0.6009044051170349 Val ndcg_10 0.45349347591400146\n",
            "[INFO] 2023-04-07 14:12:57 - Current:0.45349347591400146 Best:0.4532036781311035\n",
            "[INFO] 2023-04-07 14:12:57 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:01 - Epoch : 31 Train loss: -0.5986687019004585 Val loss: -0.5870289870950042 Train ndcg_10 0.6009870767593384 Val ndcg_10 0.4532449543476105\n",
            "[INFO] 2023-04-07 14:13:01 - Current:0.4532449543476105 Best:0.45349347591400146\n",
            "[INFO] 2023-04-07 14:13:01 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:04 - Epoch : 32 Train loss: -0.5999946046319807 Val loss: -0.5870923224042673 Train ndcg_10 0.5969448089599609 Val ndcg_10 0.4534797668457031\n",
            "[INFO] 2023-04-07 14:13:04 - Current:0.4534797668457031 Best:0.45349347591400146\n",
            "[INFO] 2023-04-07 14:13:04 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:09 - Epoch : 33 Train loss: -0.6015472019681279 Val loss: -0.5871640004095484 Train ndcg_10 0.6045405268669128 Val ndcg_10 0.4535141587257385\n",
            "[INFO] 2023-04-07 14:13:09 - Current:0.4535141587257385 Best:0.45349347591400146\n",
            "[INFO] 2023-04-07 14:13:09 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:12 - Epoch : 34 Train loss: -0.6052377912568749 Val loss: -0.5872264973452834 Train ndcg_10 0.6181708574295044 Val ndcg_10 0.4535141587257385\n",
            "[INFO] 2023-04-07 14:13:12 - Current:0.4535141587257385 Best:0.4535141587257385\n",
            "[INFO] 2023-04-07 14:13:12 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:15 - Epoch : 35 Train loss: -0.6058614957406654 Val loss: -0.5872919872158864 Train ndcg_10 0.5954616069793701 Val ndcg_10 0.45368877053260803\n",
            "[INFO] 2023-04-07 14:13:15 - Current:0.45368877053260803 Best:0.4535141587257385\n",
            "[INFO] 2023-04-07 14:13:15 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:20 - Epoch : 36 Train loss: -0.5964098805966584 Val loss: -0.5873596023340695 Train ndcg_10 0.5981793403625488 Val ndcg_10 0.453723669052124\n",
            "[INFO] 2023-04-07 14:13:20 - Current:0.453723669052124 Best:0.45368877053260803\n",
            "[INFO] 2023-04-07 14:13:20 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:24 - Epoch : 37 Train loss: -0.6043255092194362 Val loss: -0.5874288658626744 Train ndcg_10 0.6055717468261719 Val ndcg_10 0.45385441184043884\n",
            "[INFO] 2023-04-07 14:13:24 - Current:0.45385441184043884 Best:0.453723669052124\n",
            "[INFO] 2023-04-07 14:13:24 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:27 - Epoch : 38 Train loss: -0.6057306924221678 Val loss: -0.5874975988122283 Train ndcg_10 0.6109516620635986 Val ndcg_10 0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:27 - Current:0.4538947343826294 Best:0.45385441184043884\n",
            "[INFO] 2023-04-07 14:13:27 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:30 - Epoch : 39 Train loss: -0.5990606350187929 Val loss: -0.5875623060054467 Train ndcg_10 0.5965960621833801 Val ndcg_10 0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:30 - Current:0.4538947343826294 Best:0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:30 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:35 - Epoch : 40 Train loss: -0.5972733734557347 Val loss: -0.5876275105554549 Train ndcg_10 0.5986019372940063 Val ndcg_10 0.45377978682518005\n",
            "[INFO] 2023-04-07 14:13:35 - Current:0.45377978682518005 Best:0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:35 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:38 - Epoch : 41 Train loss: -0.6036645992202048 Val loss: -0.5876922060231693 Train ndcg_10 0.6046037673950195 Val ndcg_10 0.4536932408809662\n",
            "[INFO] 2023-04-07 14:13:38 - Current:0.4536932408809662 Best:0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:38 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:42 - Epoch : 42 Train loss: -0.5994668369707854 Val loss: -0.58775209794279 Train ndcg_10 0.6194320917129517 Val ndcg_10 0.4536393880844116\n",
            "[INFO] 2023-04-07 14:13:42 - Current:0.4536393880844116 Best:0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:42 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:45 - Epoch : 43 Train loss: -0.5993547654300002 Val loss: -0.5878180959185616 Train ndcg_10 0.601943850517273 Val ndcg_10 0.4536499083042145\n",
            "[INFO] 2023-04-07 14:13:45 - Current:0.4536499083042145 Best:0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:45 - Current learning rate: 4e-05\n",
            "[INFO] 2023-04-07 14:13:50 - Epoch : 44 Train loss: -0.6039591172467107 Val loss: -0.5878826819482397 Train ndcg_10 0.6124814748764038 Val ndcg_10 0.4556189179420471\n",
            "[INFO] 2023-04-07 14:13:50 - Current:0.4556189179420471 Best:0.4538947343826294\n",
            "[INFO] 2023-04-07 14:13:50 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:13:54 - Epoch : 45 Train loss: -0.5978626734721735 Val loss: -0.587896710536519 Train ndcg_10 0.6032411456108093 Val ndcg_10 0.4556189179420471\n",
            "[INFO] 2023-04-07 14:13:54 - Current:0.4556189179420471 Best:0.4556189179420471\n",
            "[INFO] 2023-04-07 14:13:54 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:13:57 - Epoch : 46 Train loss: -0.5959917773371157 Val loss: -0.5879108837393464 Train ndcg_10 0.5934669375419617 Val ndcg_10 0.4556189179420471\n",
            "[INFO] 2023-04-07 14:13:57 - Current:0.4556189179420471 Best:0.4556189179420471\n",
            "[INFO] 2023-04-07 14:13:57 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:14:01 - Epoch : 47 Train loss: -0.600500868714374 Val loss: -0.5879253305372645 Train ndcg_10 0.6055238842964172 Val ndcg_10 0.4556189179420471\n",
            "[INFO] 2023-04-07 14:14:01 - Current:0.4556189179420471 Best:0.4556189179420471\n",
            "[INFO] 2023-04-07 14:14:01 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:14:05 - Epoch : 48 Train loss: -0.604015612824363 Val loss: -0.5879395662761125 Train ndcg_10 0.5959742069244385 Val ndcg_10 0.4556189179420471\n",
            "[INFO] 2023-04-07 14:14:05 - Current:0.4556189179420471 Best:0.4556189179420471\n",
            "[INFO] 2023-04-07 14:14:05 - Current learning rate: 8.000000000000001e-06\n",
            "[INFO] 2023-04-07 14:14:08 - Epoch : 49 Train loss: -0.5967093517321237 Val loss: -0.5879525786540547 Train ndcg_10 0.6028515100479126 Val ndcg_10 0.4556189179420471\n",
            "[INFO] 2023-04-07 14:14:08 - Current:0.4556189179420471 Best:0.4556189179420471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r output.zip output\n",
        "from google.colab import files\n",
        "files.download('output.zip')"
      ],
      "metadata": {
        "id": "4NlluQap_ImR",
        "outputId": "b66e617b-e103-4114-b729-c56956c5d959",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: output/ (stored 0%)\n",
            "updating: output/tb_evals/ (stored 0%)\n",
            "updating: output/tb_evals/single/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/loss_val/events.out.tfevents.1680872824.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/lr_train/events.out.tfevents.1680872824.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/ndcg_10_val/events.out.tfevents.1680872824.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/loss_train/events.out.tfevents.1680872824.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-neuralNDCG/ndcg_10_train/events.out.tfevents.1680872824.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/loss_val/events.out.tfevents.1680869580.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/lr_train/events.out.tfevents.1680869580.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/ndcg_10_val/events.out.tfevents.1680869580.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/loss_train/events.out.tfevents.1680869580.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-listMLE/ndcg_10_train/events.out.tfevents.1680869580.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/loss_val/events.out.tfevents.1680871810.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/lr_train/events.out.tfevents.1680871810.569b3b3284b9 (deflated 61%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/ndcg_10_val/events.out.tfevents.1680871810.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/loss_train/events.out.tfevents.1680871810.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-listMLE/ndcg_10_train/events.out.tfevents.1680871810.569b3b3284b9 (deflated 57%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/loss_val/events.out.tfevents.1680873851.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/lr_train/events.out.tfevents.1680873851.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/ndcg_10_val/events.out.tfevents.1680873851.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/loss_train/events.out.tfevents.1680873851.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-neuralNDCG/ndcg_10_train/events.out.tfevents.1680873851.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/loss_val/events.out.tfevents.1680869202.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/lr_train/events.out.tfevents.1680869202.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/ndcg_10_val/events.out.tfevents.1680869202.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/loss_train/events.out.tfevents.1680869202.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-neuralNDCG/ndcg_10_train/events.out.tfevents.1680869202.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/loss_val/events.out.tfevents.1680869880.569b3b3284b9 (deflated 54%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/lr_train/events.out.tfevents.1680869880.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680869880.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/loss_train/events.out.tfevents.1680869880.569b3b3284b9 (deflated 53%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680869880.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/loss_val/events.out.tfevents.1680870427.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/lr_train/events.out.tfevents.1680870427.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/ndcg_10_val/events.out.tfevents.1680870427.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/loss_train/events.out.tfevents.1680870427.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-listMLE/ndcg_10_train/events.out.tfevents.1680870427.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/loss_val/events.out.tfevents.1680872129.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/lr_train/events.out.tfevents.1680872129.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/ndcg_10_val/events.out.tfevents.1680872129.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/loss_train/events.out.tfevents.1680872129.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-lambdaLoss/ndcg_10_train/events.out.tfevents.1680872129.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_val/events.out.tfevents.1680874243.569b3b3284b9 (deflated 32%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/lr_train/events.out.tfevents.1680874243.569b3b3284b9 (deflated 36%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_val/events.out.tfevents.1680874243.569b3b3284b9 (deflated 31%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_train/events.out.tfevents.1680874243.569b3b3284b9 (deflated 32%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_train/events.out.tfevents.1680874243.569b3b3284b9 (deflated 31%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/loss_val/events.out.tfevents.1680872521.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/lr_train/events.out.tfevents.1680872521.569b3b3284b9 (deflated 61%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680872521.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/loss_train/events.out.tfevents.1680872521.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold4-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680872521.569b3b3284b9 (deflated 57%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/loss_val/events.out.tfevents.1680870068.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/lr_train/events.out.tfevents.1680870068.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/ndcg_10_val/events.out.tfevents.1680870068.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/loss_train/events.out.tfevents.1680870068.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold2-neuralNDCG/ndcg_10_train/events.out.tfevents.1680870068.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/loss_val/events.out.tfevents.1680868610.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/loss_val/events.out.tfevents.1680868570.569b3b3284b9 (deflated 36%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/lr_train/events.out.tfevents.1680868610.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/lr_train/events.out.tfevents.1680868570.569b3b3284b9 (deflated 41%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/ndcg_10_val/events.out.tfevents.1680868610.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/ndcg_10_val/events.out.tfevents.1680868570.569b3b3284b9 (deflated 37%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/loss_train/events.out.tfevents.1680868610.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/loss_train/events.out.tfevents.1680868570.569b3b3284b9 (deflated 36%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/ndcg_10_train/events.out.tfevents.1680868610.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-listMLE/ndcg_10_train/events.out.tfevents.1680868570.569b3b3284b9 (deflated 36%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/loss_val/events.out.tfevents.1680871068.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/lr_train/events.out.tfevents.1680871068.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680871068.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/loss_train/events.out.tfevents.1680871068.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680871068.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/loss_val/events.out.tfevents.1680873534.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/lr_train/events.out.tfevents.1680873534.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680873534.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/loss_train/events.out.tfevents.1680873534.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680873534.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/loss_val/events.out.tfevents.1680871397.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/lr_train/events.out.tfevents.1680871397.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/ndcg_10_val/events.out.tfevents.1680871397.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/loss_train/events.out.tfevents.1680871397.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-neuralNDCG/ndcg_10_train/events.out.tfevents.1680871397.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/loss_val/events.out.tfevents.1680873311.569b3b3284b9 (deflated 54%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/lr_train/events.out.tfevents.1680873311.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/ndcg_10_val/events.out.tfevents.1680873311.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/loss_train/events.out.tfevents.1680873311.569b3b3284b9 (deflated 54%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold5-listMLE/ndcg_10_train/events.out.tfevents.1680873311.569b3b3284b9 (deflated 57%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/loss_val/events.out.tfevents.1680870748.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/lr_train/events.out.tfevents.1680870748.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/ndcg_10_val/events.out.tfevents.1680870748.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/loss_train/events.out.tfevents.1680870748.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold3-lambdaLoss/ndcg_10_train/events.out.tfevents.1680870748.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/loss_val/events.out.tfevents.1680868887.569b3b3284b9 (deflated 57%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/lr_train/events.out.tfevents.1680868887.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680868887.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/loss_train/events.out.tfevents.1680868887.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2007-Fold1-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680868887.569b3b3284b9 (deflated 58%)\n",
            "updating: output/results/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/training.log (deflated 79%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/training.log (deflated 79%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/model.pkl (deflated 11%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/training.log (deflated 84%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/model.pkl (deflated 11%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/training.log (deflated 86%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/training.log (deflated 81%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/model.pkl (deflated 11%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold4-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/training.log (deflated 81%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/model.pkl (deflated 11%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/training.log (deflated 83%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/training.log (deflated 88%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/training.log (deflated 79%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/training.log (deflated 80%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold3-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold5-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/training.log (deflated 82%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold1-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/training.log (deflated 66%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2007-Fold2-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/loss_val/events.out.tfevents.1680876202.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/lr_train/events.out.tfevents.1680876202.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/ndcg_10_val/events.out.tfevents.1680876202.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/loss_train/events.out.tfevents.1680876202.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-neuralNDCG/ndcg_10_train/events.out.tfevents.1680876202.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/loss_val/events.out.tfevents.1680874740.569b3b3284b9 (deflated 57%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/lr_train/events.out.tfevents.1680874740.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680874740.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/loss_train/events.out.tfevents.1680874740.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680874740.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/loss_val/events.out.tfevents.1680875792.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/lr_train/events.out.tfevents.1680875792.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/ndcg_10_val/events.out.tfevents.1680875792.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/loss_train/events.out.tfevents.1680875792.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-neuralNDCG/ndcg_10_train/events.out.tfevents.1680875792.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/loss_val/events.out.tfevents.1680876129.569b3b3284b9 (deflated 51%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/lr_train/events.out.tfevents.1680876129.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680876129.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/loss_train/events.out.tfevents.1680876129.569b3b3284b9 (deflated 51%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680876129.569b3b3284b9 (deflated 54%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/loss_val/events.out.tfevents.1680875520.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/lr_train/events.out.tfevents.1680875520.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/ndcg_10_val/events.out.tfevents.1680875520.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/loss_train/events.out.tfevents.1680875520.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-listMLE/ndcg_10_train/events.out.tfevents.1680875520.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_val/events.out.tfevents.1680874562.569b3b3284b9 (deflated 25%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_val/events.out.tfevents.1680874583.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/lr_train/events.out.tfevents.1680874562.569b3b3284b9 (deflated 28%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/lr_train/events.out.tfevents.1680874583.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_val/events.out.tfevents.1680874562.569b3b3284b9 (deflated 22%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_val/events.out.tfevents.1680874583.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_train/events.out.tfevents.1680874562.569b3b3284b9 (deflated 25%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/loss_train/events.out.tfevents.1680874583.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_train/events.out.tfevents.1680874562.569b3b3284b9 (deflated 23%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-listMLE/ndcg_10_train/events.out.tfevents.1680874583.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/loss_val/events.out.tfevents.1680876544.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/lr_train/events.out.tfevents.1680876544.569b3b3284b9 (deflated 61%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680876544.569b3b3284b9 (deflated 61%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/loss_train/events.out.tfevents.1680876544.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680876544.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/loss_val/events.out.tfevents.1680875669.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/lr_train/events.out.tfevents.1680875669.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680875669.569b3b3284b9 (deflated 61%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/loss_train/events.out.tfevents.1680875669.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold3-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680875669.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/loss_val/events.out.tfevents.1680875335.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/lr_train/events.out.tfevents.1680875335.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/ndcg_10_val/events.out.tfevents.1680875335.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/loss_train/events.out.tfevents.1680875335.569b3b3284b9 (deflated 55%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-neuralNDCG/ndcg_10_train/events.out.tfevents.1680875335.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/loss_val/events.out.tfevents.1680875081.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/lr_train/events.out.tfevents.1680875081.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/ndcg_10_val/events.out.tfevents.1680875081.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/loss_train/events.out.tfevents.1680875081.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-listMLE/ndcg_10_train/events.out.tfevents.1680875081.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/loss_val/events.out.tfevents.1680874897.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/lr_train/events.out.tfevents.1680874897.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/ndcg_10_val/events.out.tfevents.1680874897.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/loss_train/events.out.tfevents.1680874897.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold1-neuralNDCG/ndcg_10_train/events.out.tfevents.1680874897.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/loss_val/events.out.tfevents.1680875234.569b3b3284b9 (deflated 54%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/lr_train/events.out.tfevents.1680875234.569b3b3284b9 (deflated 60%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/ndcg_10_val/events.out.tfevents.1680875234.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/loss_train/events.out.tfevents.1680875234.569b3b3284b9 (deflated 53%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold2-approxNDCGLoss/ndcg_10_train/events.out.tfevents.1680875234.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/loss_val/events.out.tfevents.1680876665.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/lr_train/events.out.tfevents.1680876665.569b3b3284b9 (deflated 62%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/ndcg_10_val/events.out.tfevents.1680876665.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/loss_train/events.out.tfevents.1680876665.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-neuralNDCG/ndcg_10_train/events.out.tfevents.1680876665.569b3b3284b9 (deflated 58%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/loss_val/events.out.tfevents.1680875970.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/lr_train/events.out.tfevents.1680875970.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/ndcg_10_val/events.out.tfevents.1680875970.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/loss_train/events.out.tfevents.1680875970.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold4-listMLE/ndcg_10_train/events.out.tfevents.1680875970.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/loss_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/loss_val/events.out.tfevents.1680876389.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/lr_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/lr_train/events.out.tfevents.1680876389.569b3b3284b9 (deflated 63%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/ndcg_10_val/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/ndcg_10_val/events.out.tfevents.1680876389.569b3b3284b9 (deflated 59%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/loss_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/loss_train/events.out.tfevents.1680876389.569b3b3284b9 (deflated 56%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/ndcg_10_train/ (stored 0%)\n",
            "updating: output/tb_evals/single/MQ2008-Fold5-listMLE/ndcg_10_train/events.out.tfevents.1680876389.569b3b3284b9 (deflated 58%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/training.log (deflated 82%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/model.pkl (deflated 11%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/training.log (deflated 79%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/training.log (deflated 80%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold1-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/model.pkl (deflated 11%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/model.pkl (deflated 11%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/training.log (deflated 80%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-approxNDCGLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/training.log (deflated 80%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-neuralNDCG/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold3-ordinal/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-lambdaLoss/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold1-pointwise_rmse/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/training.log (deflated 81%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold4-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/training.log (deflated 80%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/model.pkl (deflated 10%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/experiment_result.json (deflated 56%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold5-listMLE/evals/tensorboard/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/predictions/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/models/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/models/partial/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/used_config.json (deflated 47%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/training.log (deflated 67%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/evals/ (stored 0%)\n",
            "updating: output/results/MQ2008-Fold2-lambdaLoss/evals/tensorboard/ (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0e579904-6d77-47bc-a515-4361eb733065\", \"output.zip\", 14055509)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}